/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
/home/yu34/.local/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
2025-02-04 23:04:02.494892: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.494892: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.494947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.494948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.495403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.495402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.495447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.495449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.496154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.496154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.496385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.496385: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.503737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-04 23:04:02.503825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-04 23:04:02.503832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-04 23:04:02.503832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-04 23:04:02.506403: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.506398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-02-04 23:04:02.506429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.506431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-02-04 23:04:02.507400: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.507400: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-04 23:04:02.514185: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-02-04 23:04:02.514423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 6.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 6.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:5.Process (global: 5, local 5), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:5.Process (global: 5, local 5), total 6.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 6.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 6.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
INFO:root:Running in distributed mode with multiple processes. Device: cuda:4.Process (global: 4, local 4), total 6.
2025-02-04,23:04:18 | INFO | Running in distributed mode with multiple processes. Device: cuda:4.Process (global: 4, local 4), total 6.
INFO:root:Loaded GeoLB-ViT-B-16-SigLIP model config.
2025-02-04,23:04:18 | INFO | Loaded GeoLB-ViT-B-16-SigLIP model config.
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:21 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:21 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:21 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:22 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:22 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
INFO:root:Model:
2025-02-04,23:04:22 | INFO | Model:
INFO:root:CustomTextCLIP(
  (visual): TimmModel(
    (trunk): GeoLB_VisionTransformer(
      (patch_embed): DOFA_PatchEmbed(
        (weight_generator): TransformerWeightGenerator(
          (transformer_encoder): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=2048, bias=True)
                (dropout): Dropout(p=False, inplace=False)
                (linear2): Linear(in_features=2048, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=False, inplace=False)
                (dropout2): Dropout(p=False, inplace=False)
              )
            )
          )
          (fc_weight): Linear(in_features=128, out_features=196608, bias=True)
          (fc_bias): Linear(in_features=128, out_features=768, bias=True)
        )
        (fclayer): FCResLayer(
          (nonlin1): ReLU(inplace=True)
          (nonlin2): ReLU(inplace=True)
          (w1): Linear(in_features=128, out_features=128, bias=True)
          (w2): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn_pool): AttentionPoolLatent(
        (q): Linear(in_features=768, out_features=768, bias=True)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
    )
    (head): Sequential()
  )
  (text): TextTransformer(
    (token_embedding): Embedding(32000, 768)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_final): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (text_projection): Linear(in_features=768, out_features=768, bias=True)
  )
  (translators): ModuleDict(
    (dinov2_vitl14_reg): Translator(
      (interpolation): Interpolation()
      (norm1): LayerNorm((768, 16, 16), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(768, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (rearrange): Rearrange('b c h w -> b (h w) c')
    )
    (timm_vit_huge_patch14_clip_224_laion2b_ft_in12k): Translator(
      (interpolation): Interpolation()
      (norm1): LayerNorm((768, 16, 16), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(768, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (rearrange): Rearrange('b c h w -> b (h w) c')
    )
  )
)
2025-02-04,23:04:22 | INFO | CustomTextCLIP(
  (visual): TimmModel(
    (trunk): GeoLB_VisionTransformer(
      (patch_embed): DOFA_PatchEmbed(
        (weight_generator): TransformerWeightGenerator(
          (transformer_encoder): TransformerEncoder(
            (layers): ModuleList(
              (0): TransformerEncoderLayer(
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (linear1): Linear(in_features=128, out_features=2048, bias=True)
                (dropout): Dropout(p=False, inplace=False)
                (linear2): Linear(in_features=2048, out_features=128, bias=True)
                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                (dropout1): Dropout(p=False, inplace=False)
                (dropout2): Dropout(p=False, inplace=False)
              )
            )
          )
          (fc_weight): Linear(in_features=128, out_features=196608, bias=True)
          (fc_bias): Linear(in_features=128, out_features=768, bias=True)
        )
        (fclayer): FCResLayer(
          (nonlin1): ReLU(inplace=True)
          (nonlin2): ReLU(inplace=True)
          (w1): Linear(in_features=128, out_features=128, bias=True)
          (w2): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn_pool): AttentionPoolLatent(
        (q): Linear(in_features=768, out_features=768, bias=True)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.0, inplace=False)
          (norm): Identity()
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
    )
    (head): Sequential()
  )
  (text): TextTransformer(
    (token_embedding): Embedding(32000, 768)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_final): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (text_projection): Linear(in_features=768, out_features=768, bias=True)
  )
  (translators): ModuleDict(
    (dinov2_vitl14_reg): Translator(
      (interpolation): Interpolation()
      (norm1): LayerNorm((768, 16, 16), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(768, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (rearrange): Rearrange('b c h w -> b (h w) c')
    )
    (timm_vit_huge_patch14_clip_224_laion2b_ft_in12k): Translator(
      (interpolation): Interpolation()
      (norm1): LayerNorm((768, 16, 16), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(768, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (rearrange): Rearrange('b c h w -> b (h w) c')
    )
  )
)
INFO:root:Params:
2025-02-04,23:04:22 | INFO | Params:
INFO:root:  DOFA: True
2025-02-04,23:04:22 | INFO |   DOFA: True
INFO:root:  accum_freq: 1
2025-02-04,23:04:22 | INFO |   accum_freq: 1
INFO:root:  aug_cfg: {}
2025-02-04,23:04:22 | INFO |   aug_cfg: {}
INFO:root:  batch_size: 133
2025-02-04,23:04:22 | INFO |   batch_size: 133
INFO:root:  beta1: 0.9
2025-02-04,23:04:22 | INFO |   beta1: 0.9
INFO:root:  beta2: 0.98
2025-02-04,23:04:22 | INFO |   beta2: 0.98
INFO:root:  cache_dir: None
2025-02-04,23:04:22 | INFO |   cache_dir: None
INFO:root:  checkpoint_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/checkpoints
2025-02-04,23:04:22 | INFO |   checkpoint_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/checkpoints
INFO:root:  coca_caption_loss_weight: 2.0
2025-02-04,23:04:22 | INFO |   coca_caption_loss_weight: 2.0
INFO:root:  coca_contrastive_loss_weight: 1.0
2025-02-04,23:04:22 | INFO |   coca_contrastive_loss_weight: 1.0
INFO:root:  copy_codebase: False
2025-02-04,23:04:22 | INFO |   copy_codebase: False
INFO:root:  csv_caption_key: title
2025-02-04,23:04:22 | INFO |   csv_caption_key: title
INFO:root:  csv_img_key: filepath
2025-02-04,23:04:22 | INFO |   csv_img_key: filepath
INFO:root:  csv_separator: 	
2025-02-04,23:04:22 | INFO |   csv_separator: 	
INFO:root:  datamix: None
2025-02-04,23:04:22 | INFO |   datamix: None
INFO:root:  dataset_resampled: False
2025-02-04,23:04:22 | INFO |   dataset_resampled: False
INFO:root:  dataset_type: geolb
2025-02-04,23:04:22 | INFO |   dataset_type: geolb
INFO:root:  ddp_static_graph: False
2025-02-04,23:04:22 | INFO |   ddp_static_graph: False
INFO:root:  debug: False
2025-02-04,23:04:22 | INFO |   debug: False
INFO:root:  delete_previous_checkpoint: False
2025-02-04,23:04:22 | INFO |   delete_previous_checkpoint: False
INFO:root:  device: cuda:0
2025-02-04,23:04:22 | INFO |   device: cuda:0
INFO:root:  dist_backend: None
2025-02-04,23:04:22 | INFO |   dist_backend: None
INFO:root:  dist_url: None
2025-02-04,23:04:22 | INFO |   dist_url: None
INFO:root:  distill: False
2025-02-04,23:04:22 | INFO |   distill: False
INFO:root:  distill_model: None
2025-02-04,23:04:22 | INFO |   distill_model: None
INFO:root:  distill_pretrained: None
2025-02-04,23:04:22 | INFO |   distill_pretrained: None
INFO:root:  distributed: True
2025-02-04,23:04:22 | INFO |   distributed: True
INFO:root:  epochs: 50
2025-02-04,23:04:22 | INFO |   epochs: 50
INFO:root:  epochs_cooldown: None
2025-02-04,23:04:22 | INFO |   epochs_cooldown: None
INFO:root:  eps: 1e-06
2025-02-04,23:04:22 | INFO |   eps: 1e-06
INFO:root:  force_custom_text: False
2025-02-04,23:04:22 | INFO |   force_custom_text: False
INFO:root:  force_image_size: None
2025-02-04,23:04:22 | INFO |   force_image_size: None
INFO:root:  force_patch_dropout: None
2025-02-04,23:04:22 | INFO |   force_patch_dropout: None
INFO:root:  force_quick_gelu: False
2025-02-04,23:04:22 | INFO |   force_quick_gelu: False
INFO:root:  gather_with_grad: False
2025-02-04,23:04:22 | INFO |   gather_with_grad: False
INFO:root:  grad_checkpointing: False
2025-02-04,23:04:22 | INFO |   grad_checkpointing: False
INFO:root:  grad_clip_norm: None
2025-02-04,23:04:22 | INFO |   grad_clip_norm: None
INFO:root:  horovod: False
2025-02-04,23:04:22 | INFO |   horovod: False
INFO:root:  image_interpolation: None
2025-02-04,23:04:22 | INFO |   image_interpolation: None
INFO:root:  image_mean: None
2025-02-04,23:04:22 | INFO |   image_mean: None
INFO:root:  image_resize_mode: None
2025-02-04,23:04:22 | INFO |   image_resize_mode: None
INFO:root:  image_std: None
2025-02-04,23:04:22 | INFO |   image_std: None
INFO:root:  imagenet_v2: None
2025-02-04,23:04:22 | INFO |   imagenet_v2: None
INFO:root:  imagenet_val: None
2025-02-04,23:04:22 | INFO |   imagenet_val: None
INFO:root:  local_loss: False
2025-02-04,23:04:22 | INFO |   local_loss: False
INFO:root:  local_rank: 0
2025-02-04,23:04:22 | INFO |   local_rank: 0
INFO:root:  lock_image: False
2025-02-04,23:04:22 | INFO |   lock_image: False
INFO:root:  lock_image_freeze_bn_stats: False
2025-02-04,23:04:22 | INFO |   lock_image_freeze_bn_stats: False
INFO:root:  lock_image_unlocked_groups: 0
2025-02-04,23:04:22 | INFO |   lock_image_unlocked_groups: 0
INFO:root:  lock_text: True
2025-02-04,23:04:22 | INFO |   lock_text: True
INFO:root:  lock_text_freeze_layer_norm: False
2025-02-04,23:04:22 | INFO |   lock_text_freeze_layer_norm: False
INFO:root:  lock_text_unlocked_layers: 0
2025-02-04,23:04:22 | INFO |   lock_text_unlocked_layers: 0
INFO:root:  log_every_n_steps: 50
2025-02-04,23:04:22 | INFO |   log_every_n_steps: 50
INFO:root:  log_level: 20
2025-02-04,23:04:22 | INFO |   log_level: 20
INFO:root:  log_local: False
2025-02-04,23:04:22 | INFO |   log_local: False
INFO:root:  log_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/out.log
2025-02-04,23:04:22 | INFO |   log_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/out.log
INFO:root:  logs: ./logs/
2025-02-04,23:04:22 | INFO |   logs: ./logs/
INFO:root:  loss_dist_impl: None
2025-02-04,23:04:22 | INFO |   loss_dist_impl: None
INFO:root:  lr: 0.0001
2025-02-04,23:04:22 | INFO |   lr: 0.0001
INFO:root:  lr_cooldown_end: 0.0
2025-02-04,23:04:22 | INFO |   lr_cooldown_end: 0.0
INFO:root:  lr_cooldown_power: 1.0
2025-02-04,23:04:22 | INFO |   lr_cooldown_power: 1.0
INFO:root:  lr_scheduler: cosine
2025-02-04,23:04:22 | INFO |   lr_scheduler: cosine
INFO:root:  model: GeoLB-ViT-B-16-SigLIP
2025-02-04,23:04:22 | INFO |   model: GeoLB-ViT-B-16-SigLIP
INFO:root:  momentum: None
2025-02-04,23:04:22 | INFO |   momentum: None
INFO:root:  name: 2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp
2025-02-04,23:04:22 | INFO |   name: 2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp
INFO:root:  no_set_device_rank: False
2025-02-04,23:04:22 | INFO |   no_set_device_rank: False
INFO:root:  opt: adamw
2025-02-04,23:04:22 | INFO |   opt: adamw
INFO:root:  precision: amp
2025-02-04,23:04:22 | INFO |   precision: amp
INFO:root:  pretrained: webli
2025-02-04,23:04:22 | INFO |   pretrained: webli
INFO:root:  pretrained_image: False
2025-02-04,23:04:22 | INFO |   pretrained_image: False
INFO:root:  rank: 0
2025-02-04,23:04:22 | INFO |   rank: 0
INFO:root:  remote_sync: None
2025-02-04,23:04:22 | INFO |   remote_sync: None
INFO:root:  remote_sync_frequency: 300
2025-02-04,23:04:22 | INFO |   remote_sync_frequency: 300
INFO:root:  remote_sync_protocol: s3
2025-02-04,23:04:22 | INFO |   remote_sync_protocol: s3
INFO:root:  report_to: tensorboard
2025-02-04,23:04:22 | INFO |   report_to: tensorboard
INFO:root:  resume: None
2025-02-04,23:04:22 | INFO |   resume: None
INFO:root:  save_frequency: 5
2025-02-04,23:04:22 | INFO |   save_frequency: 5
INFO:root:  save_most_recent: False
2025-02-04,23:04:22 | INFO |   save_most_recent: False
INFO:root:  seed: 0
2025-02-04,23:04:22 | INFO |   seed: 0
INFO:root:  siglip: True
2025-02-04,23:04:22 | INFO |   siglip: True
INFO:root:  skip_scheduler: False
2025-02-04,23:04:22 | INFO |   skip_scheduler: False
INFO:root:  tensorboard: True
2025-02-04,23:04:22 | INFO |   tensorboard: True
INFO:root:  tensorboard_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/tensorboard
2025-02-04,23:04:22 | INFO |   tensorboard_path: ./logs/2025_02_04-23_04_14-model_GeoLB-ViT-B-16-SigLIP-lr_0.0001-b_133-j_4-p_amp/tensorboard
INFO:root:  torchcompile: False
2025-02-04,23:04:22 | INFO |   torchcompile: False
INFO:root:  torchscript: False
2025-02-04,23:04:22 | INFO |   torchscript: False
INFO:root:  trace: False
2025-02-04,23:04:22 | INFO |   trace: False
INFO:root:  train_data: /home/yu34/GeoLangBind/data/
2025-02-04,23:04:22 | INFO |   train_data: /home/yu34/GeoLangBind/data/
INFO:root:  train_data_upsampling_factors: None
2025-02-04,23:04:22 | INFO |   train_data_upsampling_factors: None
INFO:root:  train_num_samples: None
2025-02-04,23:04:22 | INFO |   train_num_samples: None
INFO:root:  use_bn_sync: True
2025-02-04,23:04:22 | INFO |   use_bn_sync: True
INFO:root:  use_bnb_linear: None
2025-02-04,23:04:22 | INFO |   use_bnb_linear: None
INFO:root:  val_data: None
2025-02-04,23:04:22 | INFO |   val_data: None
INFO:root:  val_frequency: 1
2025-02-04,23:04:22 | INFO |   val_frequency: 1
INFO:root:  val_num_samples: None
2025-02-04,23:04:22 | INFO |   val_num_samples: None
INFO:root:  wandb: False
2025-02-04,23:04:22 | INFO |   wandb: False
INFO:root:  wandb_notes: 
2025-02-04,23:04:22 | INFO |   wandb_notes: 
INFO:root:  wandb_project_name: open-clip
2025-02-04,23:04:22 | INFO |   wandb_project_name: open-clip
INFO:root:  warmup: 1000
2025-02-04,23:04:22 | INFO |   warmup: 1000
INFO:root:  wd: 1e-07
2025-02-04,23:04:22 | INFO |   wd: 1e-07
INFO:root:  workers: 4
2025-02-04,23:04:22 | INFO |   workers: 4
INFO:root:  world_size: 6
2025-02-04,23:04:22 | INFO |   world_size: 6
INFO:root:  zeroshot_frequency: 2
2025-02-04,23:04:22 | INFO |   zeroshot_frequency: 2
INFO:root:Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
2025-02-04,23:04:23 | INFO | Loading pretrained GeoLB-ViT-B-16-SigLIP weights (webli).
/home/yu34/.cache/huggingface/hub/models--XShadow--GeoLB-ViT-B-16-SigLIP/snapshots/ea10497fb492824632191ff579767d2d26c16a99/open_clip_model.safetensors
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:root:Created AdamW (adamw) optimizer: lr: 0.0001, betas: (0.9, 0.98), eps: 1e-06, weight_decay: 1e-07, amsgrad: False, foreach: None, maximize: False, capturable: False, differentiable: False, fused: None
2025-02-04,23:04:24 | INFO | Created AdamW (adamw) optimizer: lr: 0.0001, betas: (0.9, 0.98), eps: 1e-06, weight_decay: 1e-07, amsgrad: False, foreach: None, maximize: False, capturable: False, differentiable: False, fused: None
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
loading file spiece.model from cache at None
loading file tokenizer.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/yu34/.cache/huggingface/hub/models--timm--ViT-B-16-SigLIP/snapshots/41f575766f40e752fdd1383e9565b7f02388c1c4/tokenizer_config.json
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
2025-02-04,23:04:25 | INFO | Loading pretrained weights from Hugging Face hub (('timm/ViT-B-16-SigLIP', 'open_clip_pytorch_model.bin'))
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:25 | INFO | [timm/ViT-B-16-SigLIP] Safe alternative available for 'open_clip_pytorch_model.bin' (as 'open_clip_model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:33 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:33 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:34 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:34 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:34 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
2025-02-04,23:04:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/vit_huge_patch14_clip_224.laion2b_ft_in12k)
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
/home/yu34/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
  warnings.warn(
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:35 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
INFO:timm.models._hub:[timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-02-04,23:04:35 | INFO | [timm/vit_huge_patch14_clip_224.laion2b_ft_in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/yu34/.cache/torch/hub/facebookresearch_dinov2_main
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
2025-02-04,23:04:40 | WARNING | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.5.1+cu118 with CUDA 1108 (you have 2.1.1+cu118)
    Python  3.10.16 (you have 3.10.12)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
/home/yu34/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:dinov2:using MLP layer as FFN
2025-02-04,23:04:40 | INFO | using MLP layer as FFN
INFO:root:Start epoch 0
2025-02-04,23:04:46 | INFO | Start epoch 0
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/home/yu34/.local/lib/python3.10/site-packages/torch/autograd/function.py:539: UserWarning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1856.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
INFO:root:Train Epoch: 0 [    798/1859739.0 (0%)] Data (t): 0.987 Batch (t): 10.758, 74.1757/s, 12.3626/s/gpu LR: 0.000000 Logit Scale: 117.331 Contrastive_loss: 55.355 (55.355) Dstill: 2.0350 (2.0350) Loss: 57.390 (57.390)
2025-02-04,23:04:57 | INFO | Train Epoch: 0 [    798/1859739.0 (0%)] Data (t): 0.987 Batch (t): 10.758, 74.1757/s, 12.3626/s/gpu LR: 0.000000 Logit Scale: 117.331 Contrastive_loss: 55.355 (55.355) Dstill: 2.0350 (2.0350) Loss: 57.390 (57.390)
[rank1]:[2025-02-04 23:04:57,731] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank0]:[2025-02-04 23:04:57,731] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank2]:[2025-02-04 23:04:57,732] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank3]:[2025-02-04 23:04:57,733] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank5]:[2025-02-04 23:04:57,733] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
[rank4]:[2025-02-04 23:04:57,733] torch.nn.parallel.distributed: [INFO] Reducer buckets have been rebuilt in this iteration.
INFO:root:Train Epoch: 0 [  40698/1859739.0 (2%)] Data (t): 0.549 Batch (t): 1.541, 511.947/s, 85.3245/s/gpu LR: 0.000005 Logit Scale: 99.997 Contrastive_loss: 6.3029 (30.829) Dstill: 2.2716 (2.1533) Loss: 8.5745 (32.982)
2025-02-04,23:06:14 | INFO | Train Epoch: 0 [  40698/1859739.0 (2%)] Data (t): 0.549 Batch (t): 1.541, 511.947/s, 85.3245/s/gpu LR: 0.000005 Logit Scale: 99.997 Contrastive_loss: 6.3029 (30.829) Dstill: 2.2716 (2.1533) Loss: 8.5745 (32.982)
INFO:root:Train Epoch: 0 [  80598/1859739.0 (4%)] Data (t): 0.523 Batch (t): 1.517, 515.977/s, 85.9962/s/gpu LR: 0.000010 Logit Scale: 99.994 Contrastive_loss: 5.6915 (22.450) Dstill: 1.8062 (2.0376) Loss: 7.4977 (24.487)
2025-02-04,23:07:29 | INFO | Train Epoch: 0 [  80598/1859739.0 (4%)] Data (t): 0.523 Batch (t): 1.517, 515.977/s, 85.9962/s/gpu LR: 0.000010 Logit Scale: 99.994 Contrastive_loss: 5.6915 (22.450) Dstill: 1.8062 (2.0376) Loss: 7.4977 (24.487)
INFO:root:Train Epoch: 0 [ 120498/1859739.0 (6%)] Data (t): 0.567 Batch (t): 1.538, 586.334/s, 97.7223/s/gpu LR: 0.000015 Logit Scale: 99.992 Contrastive_loss: 5.3253 (18.169) Dstill: 1.7136 (1.9566) Loss: 7.0389 (20.125)
2025-02-04,23:08:46 | INFO | Train Epoch: 0 [ 120498/1859739.0 (6%)] Data (t): 0.567 Batch (t): 1.538, 586.334/s, 97.7223/s/gpu LR: 0.000015 Logit Scale: 99.992 Contrastive_loss: 5.3253 (18.169) Dstill: 1.7136 (1.9566) Loss: 7.0389 (20.125)
INFO:root:Train Epoch: 0 [ 160398/1859739.0 (9%)] Data (t): 0.549 Batch (t): 1.525, 479.447/s, 79.9078/s/gpu LR: 0.000020 Logit Scale: 99.991 Contrastive_loss: 4.8995 (15.515) Dstill: 1.6762 (1.9005) Loss: 6.5757 (17.415)
2025-02-04,23:10:03 | INFO | Train Epoch: 0 [ 160398/1859739.0 (9%)] Data (t): 0.549 Batch (t): 1.525, 479.447/s, 79.9078/s/gpu LR: 0.000020 Logit Scale: 99.991 Contrastive_loss: 4.8995 (15.515) Dstill: 1.6762 (1.9005) Loss: 6.5757 (17.415)
INFO:root:Train Epoch: 0 [ 200298/1859739.0 (11%)] Data (t): 0.541 Batch (t): 1.506, 553.618/s, 92.2696/s/gpu LR: 0.000025 Logit Scale: 99.990 Contrastive_loss: 4.6564 (13.705) Dstill: 2.0950 (1.9329) Loss: 6.7515 (15.638)
2025-02-04,23:11:18 | INFO | Train Epoch: 0 [ 200298/1859739.0 (11%)] Data (t): 0.541 Batch (t): 1.506, 553.618/s, 92.2696/s/gpu LR: 0.000025 Logit Scale: 99.990 Contrastive_loss: 4.6564 (13.705) Dstill: 2.0950 (1.9329) Loss: 6.7515 (15.638)
INFO:root:Train Epoch: 0 [ 240198/1859739.0 (13%)] Data (t): 0.546 Batch (t): 1.511, 533.139/s, 88.8564/s/gpu LR: 0.000030 Logit Scale: 99.990 Contrastive_loss: 4.7796 (12.430) Dstill: 1.5485 (1.8780) Loss: 6.3280 (14.308)
2025-02-04,23:12:34 | INFO | Train Epoch: 0 [ 240198/1859739.0 (13%)] Data (t): 0.546 Batch (t): 1.511, 533.139/s, 88.8564/s/gpu LR: 0.000030 Logit Scale: 99.990 Contrastive_loss: 4.7796 (12.430) Dstill: 1.5485 (1.8780) Loss: 6.3280 (14.308)
INFO:root:Train Epoch: 0 [ 280098/1859739.0 (15%)] Data (t): 0.548 Batch (t): 1.514, 489.480/s, 81.5801/s/gpu LR: 0.000035 Logit Scale: 99.987 Contrastive_loss: 4.3765 (11.423) Dstill: 1.8234 (1.8712) Loss: 6.1999 (13.295)
2025-02-04,23:13:49 | INFO | Train Epoch: 0 [ 280098/1859739.0 (15%)] Data (t): 0.548 Batch (t): 1.514, 489.480/s, 81.5801/s/gpu LR: 0.000035 Logit Scale: 99.987 Contrastive_loss: 4.3765 (11.423) Dstill: 1.8234 (1.8712) Loss: 6.1999 (13.295)
INFO:root:Train Epoch: 0 [ 319998/1859739.0 (17%)] Data (t): 0.599 Batch (t): 1.543, 535.563/s, 89.2605/s/gpu LR: 0.000040 Logit Scale: 99.988 Contrastive_loss: 4.3301 (10.635) Dstill: 2.0780 (1.8942) Loss: 6.4081 (12.529)
2025-02-04,23:15:06 | INFO | Train Epoch: 0 [ 319998/1859739.0 (17%)] Data (t): 0.599 Batch (t): 1.543, 535.563/s, 89.2605/s/gpu LR: 0.000040 Logit Scale: 99.988 Contrastive_loss: 4.3301 (10.635) Dstill: 2.0780 (1.8942) Loss: 6.4081 (12.529)
INFO:root:Train Epoch: 0 [ 359898/1859739.0 (19%)] Data (t): 0.529 Batch (t): 1.505, 547.477/s, 91.2461/s/gpu LR: 0.000045 Logit Scale: 99.987 Contrastive_loss: 4.1584 (9.9875) Dstill: 2.0300 (1.9078) Loss: 6.1885 (11.895)
2025-02-04,23:16:22 | INFO | Train Epoch: 0 [ 359898/1859739.0 (19%)] Data (t): 0.529 Batch (t): 1.505, 547.477/s, 91.2461/s/gpu LR: 0.000045 Logit Scale: 99.987 Contrastive_loss: 4.1584 (9.9875) Dstill: 2.0300 (1.9078) Loss: 6.1885 (11.895)
INFO:root:Train Epoch: 0 [ 399798/1859739.0 (21%)] Data (t): 0.570 Batch (t): 1.542, 500.098/s, 83.3497/s/gpu LR: 0.000050 Logit Scale: 99.982 Contrastive_loss: 4.3970 (9.4793) Dstill: 2.0668 (1.9222) Loss: 6.4638 (11.402)
2025-02-04,23:17:39 | INFO | Train Epoch: 0 [ 399798/1859739.0 (21%)] Data (t): 0.570 Batch (t): 1.542, 500.098/s, 83.3497/s/gpu LR: 0.000050 Logit Scale: 99.982 Contrastive_loss: 4.3970 (9.4793) Dstill: 2.0668 (1.9222) Loss: 6.4638 (11.402)
INFO:root:Train Epoch: 0 [ 439698/1859739.0 (24%)] Data (t): 0.577 Batch (t): 1.563, 495.654/s, 82.6090/s/gpu LR: 0.000055 Logit Scale: 99.977 Contrastive_loss: 4.1756 (9.0373) Dstill: 2.2519 (1.9497) Loss: 6.4275 (10.987)
2025-02-04,23:18:57 | INFO | Train Epoch: 0 [ 439698/1859739.0 (24%)] Data (t): 0.577 Batch (t): 1.563, 495.654/s, 82.6090/s/gpu LR: 0.000055 Logit Scale: 99.977 Contrastive_loss: 4.1756 (9.0373) Dstill: 2.2519 (1.9497) Loss: 6.4275 (10.987)
INFO:root:Train Epoch: 0 [ 479598/1859739.0 (26%)] Data (t): 0.564 Batch (t): 1.533, 529.028/s, 88.1713/s/gpu LR: 0.000060 Logit Scale: 99.972 Contrastive_loss: 4.3495 (8.6767) Dstill: 2.0639 (1.9585) Loss: 6.4134 (10.635)
2025-02-04,23:20:14 | INFO | Train Epoch: 0 [ 479598/1859739.0 (26%)] Data (t): 0.564 Batch (t): 1.533, 529.028/s, 88.1713/s/gpu LR: 0.000060 Logit Scale: 99.972 Contrastive_loss: 4.3495 (8.6767) Dstill: 2.0639 (1.9585) Loss: 6.4134 (10.635)
INFO:root:Train Epoch: 0 [ 519498/1859739.0 (28%)] Data (t): 0.566 Batch (t): 1.521, 506.559/s, 84.4265/s/gpu LR: 0.000065 Logit Scale: 99.969 Contrastive_loss: 4.1441 (8.3530) Dstill: 2.2458 (1.9790) Loss: 6.3899 (10.332)
2025-02-04,23:21:30 | INFO | Train Epoch: 0 [ 519498/1859739.0 (28%)] Data (t): 0.566 Batch (t): 1.521, 506.559/s, 84.4265/s/gpu LR: 0.000065 Logit Scale: 99.969 Contrastive_loss: 4.1441 (8.3530) Dstill: 2.2458 (1.9790) Loss: 6.3899 (10.332)
INFO:root:Train Epoch: 0 [ 559398/1859739.0 (30%)] Data (t): 0.534 Batch (t): 1.516, 540.700/s, 90.1166/s/gpu LR: 0.000070 Logit Scale: 99.966 Contrastive_loss: 4.1614 (8.0735) Dstill: 2.1161 (1.9881) Loss: 6.2775 (10.062)
2025-02-04,23:22:45 | INFO | Train Epoch: 0 [ 559398/1859739.0 (30%)] Data (t): 0.534 Batch (t): 1.516, 540.700/s, 90.1166/s/gpu LR: 0.000070 Logit Scale: 99.966 Contrastive_loss: 4.1614 (8.0735) Dstill: 2.1161 (1.9881) Loss: 6.2775 (10.062)
INFO:root:Train Epoch: 0 [ 599298/1859739.0 (32%)] Data (t): 0.602 Batch (t): 1.578, 503.906/s, 83.9843/s/gpu LR: 0.000075 Logit Scale: 99.962 Contrastive_loss: 4.3783 (7.8426) Dstill: 2.5273 (2.0218) Loss: 6.9057 (9.8644)
2025-02-04,23:24:04 | INFO | Train Epoch: 0 [ 599298/1859739.0 (32%)] Data (t): 0.602 Batch (t): 1.578, 503.906/s, 83.9843/s/gpu LR: 0.000075 Logit Scale: 99.962 Contrastive_loss: 4.3783 (7.8426) Dstill: 2.5273 (2.0218) Loss: 6.9057 (9.8644)
INFO:root:Train Epoch: 0 [ 639198/1859739.0 (34%)] Data (t): 0.595 Batch (t): 1.569, 477.370/s, 79.5616/s/gpu LR: 0.000080 Logit Scale: 99.958 Contrastive_loss: 4.2114 (7.6290) Dstill: 2.6716 (2.0601) Loss: 6.8830 (9.6890)
2025-02-04,23:25:23 | INFO | Train Epoch: 0 [ 639198/1859739.0 (34%)] Data (t): 0.595 Batch (t): 1.569, 477.370/s, 79.5616/s/gpu LR: 0.000080 Logit Scale: 99.958 Contrastive_loss: 4.2114 (7.6290) Dstill: 2.6716 (2.0601) Loss: 6.8830 (9.6890)
INFO:root:Train Epoch: 0 [ 679098/1859739.0 (37%)] Data (t): 0.572 Batch (t): 1.551, 477.911/s, 79.6518/s/gpu LR: 0.000085 Logit Scale: 99.952 Contrastive_loss: 3.9788 (7.4262) Dstill: 2.4423 (2.0813) Loss: 6.4211 (9.5075)
2025-02-04,23:26:40 | INFO | Train Epoch: 0 [ 679098/1859739.0 (37%)] Data (t): 0.572 Batch (t): 1.551, 477.911/s, 79.6518/s/gpu LR: 0.000085 Logit Scale: 99.952 Contrastive_loss: 3.9788 (7.4262) Dstill: 2.4423 (2.0813) Loss: 6.4211 (9.5075)
INFO:root:Train Epoch: 0 [ 718998/1859739.0 (39%)] Data (t): 0.591 Batch (t): 1.572, 469.924/s, 78.3207/s/gpu LR: 0.000090 Logit Scale: 99.937 Contrastive_loss: 4.0054 (7.2462) Dstill: 2.7552 (2.1168) Loss: 6.7606 (9.3629)
2025-02-04,23:27:59 | INFO | Train Epoch: 0 [ 718998/1859739.0 (39%)] Data (t): 0.591 Batch (t): 1.572, 469.924/s, 78.3207/s/gpu LR: 0.000090 Logit Scale: 99.937 Contrastive_loss: 4.0054 (7.2462) Dstill: 2.7552 (2.1168) Loss: 6.7606 (9.3629)
INFO:root:Train Epoch: 0 [ 758898/1859739.0 (41%)] Data (t): 0.590 Batch (t): 1.571, 529.905/s, 88.3175/s/gpu LR: 0.000095 Logit Scale: 99.927 Contrastive_loss: 4.1357 (7.0906) Dstill: 2.7996 (2.1509) Loss: 6.9353 (9.2415)
2025-02-04,23:29:17 | INFO | Train Epoch: 0 [ 758898/1859739.0 (41%)] Data (t): 0.590 Batch (t): 1.571, 529.905/s, 88.3175/s/gpu LR: 0.000095 Logit Scale: 99.927 Contrastive_loss: 4.1357 (7.0906) Dstill: 2.7996 (2.1509) Loss: 6.9353 (9.2415)
INFO:root:Train Epoch: 0 [ 798798/1859739.0 (43%)] Data (t): 0.575 Batch (t): 1.553, 502.327/s, 83.7212/s/gpu LR: 0.000100 Logit Scale: 99.927 Contrastive_loss: 3.5635 (6.9227) Dstill: 2.9091 (2.1870) Loss: 6.4726 (9.1097)
2025-02-04,23:30:35 | INFO | Train Epoch: 0 [ 798798/1859739.0 (43%)] Data (t): 0.575 Batch (t): 1.553, 502.327/s, 83.7212/s/gpu LR: 0.000100 Logit Scale: 99.927 Contrastive_loss: 3.5635 (6.9227) Dstill: 2.9091 (2.1870) Loss: 6.4726 (9.1097)
INFO:root:Train Epoch: 0 [ 838698/1859739.0 (45%)] Data (t): 0.576 Batch (t): 1.535, 507.133/s, 84.5222/s/gpu LR: 0.000100 Logit Scale: 99.918 Contrastive_loss: 4.0388 (6.7916) Dstill: 3.4633 (2.2450) Loss: 7.5022 (9.0366)
2025-02-04,23:31:52 | INFO | Train Epoch: 0 [ 838698/1859739.0 (45%)] Data (t): 0.576 Batch (t): 1.535, 507.133/s, 84.5222/s/gpu LR: 0.000100 Logit Scale: 99.918 Contrastive_loss: 4.0388 (6.7916) Dstill: 3.4633 (2.2450) Loss: 7.5022 (9.0366)
INFO:root:Train Epoch: 0 [ 878598/1859739.0 (47%)] Data (t): 0.596 Batch (t): 1.597, 499.020/s, 83.1701/s/gpu LR: 0.000100 Logit Scale: 99.910 Contrastive_loss: 3.6469 (6.6549) Dstill: 2.8238 (2.2702) Loss: 6.4707 (8.9251)
2025-02-04,23:33:12 | INFO | Train Epoch: 0 [ 878598/1859739.0 (47%)] Data (t): 0.596 Batch (t): 1.597, 499.020/s, 83.1701/s/gpu LR: 0.000100 Logit Scale: 99.910 Contrastive_loss: 3.6469 (6.6549) Dstill: 2.8238 (2.2702) Loss: 6.4707 (8.9251)
INFO:root:Train Epoch: 0 [ 918498/1859739.0 (49%)] Data (t): 0.599 Batch (t): 1.551, 573.091/s, 95.5152/s/gpu LR: 0.000100 Logit Scale: 99.906 Contrastive_loss: 3.7209 (6.5326) Dstill: 2.6035 (2.2841) Loss: 6.3244 (8.8167)
2025-02-04,23:34:29 | INFO | Train Epoch: 0 [ 918498/1859739.0 (49%)] Data (t): 0.599 Batch (t): 1.551, 573.091/s, 95.5152/s/gpu LR: 0.000100 Logit Scale: 99.906 Contrastive_loss: 3.7209 (6.5326) Dstill: 2.6035 (2.2841) Loss: 6.3244 (8.8167)
INFO:root:Train Epoch: 0 [ 958398/1859739.0 (52%)] Data (t): 0.598 Batch (t): 1.539, 514.063/s, 85.6771/s/gpu LR: 0.000100 Logit Scale: 99.899 Contrastive_loss: 3.7166 (6.4200) Dstill: 2.8361 (2.3062) Loss: 6.5527 (8.7261)
2025-02-04,23:35:46 | INFO | Train Epoch: 0 [ 958398/1859739.0 (52%)] Data (t): 0.598 Batch (t): 1.539, 514.063/s, 85.6771/s/gpu LR: 0.000100 Logit Scale: 99.899 Contrastive_loss: 3.7166 (6.4200) Dstill: 2.8361 (2.3062) Loss: 6.5527 (8.7261)
INFO:root:Train Epoch: 0 [ 998298/1859739.0 (54%)] Data (t): 0.602 Batch (t): 1.569, 482.963/s, 80.4938/s/gpu LR: 0.000100 Logit Scale: 99.894 Contrastive_loss: 4.1003 (6.3308) Dstill: 3.1203 (2.3375) Loss: 7.2207 (8.6682)
2025-02-04,23:37:05 | INFO | Train Epoch: 0 [ 998298/1859739.0 (54%)] Data (t): 0.602 Batch (t): 1.569, 482.963/s, 80.4938/s/gpu LR: 0.000100 Logit Scale: 99.894 Contrastive_loss: 4.1003 (6.3308) Dstill: 3.1203 (2.3375) Loss: 7.2207 (8.6682)
INFO:root:Train Epoch: 0 [1038198/1859739.0 (56%)] Data (t): 0.579 Batch (t): 1.569, 506.442/s, 84.4070/s/gpu LR: 0.000100 Logit Scale: 99.892 Contrastive_loss: 3.7082 (6.2336) Dstill: 2.7418 (2.3525) Loss: 6.4500 (8.5861)
2025-02-04,23:38:23 | INFO | Train Epoch: 0 [1038198/1859739.0 (56%)] Data (t): 0.579 Batch (t): 1.569, 506.442/s, 84.4070/s/gpu LR: 0.000100 Logit Scale: 99.892 Contrastive_loss: 3.7082 (6.2336) Dstill: 2.7418 (2.3525) Loss: 6.4500 (8.5861)
INFO:root:Train Epoch: 0 [1078098/1859739.0 (58%)] Data (t): 0.603 Batch (t): 1.552, 539.195/s, 89.8658/s/gpu LR: 0.000100 Logit Scale: 99.893 Contrastive_loss: 3.7182 (6.1438) Dstill: 2.9353 (2.3733) Loss: 6.6535 (8.5171)
2025-02-04,23:39:41 | INFO | Train Epoch: 0 [1078098/1859739.0 (58%)] Data (t): 0.603 Batch (t): 1.552, 539.195/s, 89.8658/s/gpu LR: 0.000100 Logit Scale: 99.893 Contrastive_loss: 3.7182 (6.1438) Dstill: 2.9353 (2.3733) Loss: 6.6535 (8.5171)
INFO:root:Train Epoch: 0 [1117998/1859739.0 (60%)] Data (t): 0.604 Batch (t): 1.549, 500.920/s, 83.4867/s/gpu LR: 0.000100 Logit Scale: 99.892 Contrastive_loss: 3.8758 (6.0656) Dstill: 2.8244 (2.3888) Loss: 6.7003 (8.4544)
2025-02-04,23:40:58 | INFO | Train Epoch: 0 [1117998/1859739.0 (60%)] Data (t): 0.604 Batch (t): 1.549, 500.920/s, 83.4867/s/gpu LR: 0.000100 Logit Scale: 99.892 Contrastive_loss: 3.8758 (6.0656) Dstill: 2.8244 (2.3888) Loss: 6.7003 (8.4544)
INFO:root:Train Epoch: 0 [1157898/1859739.0 (62%)] Data (t): 0.616 Batch (t): 1.597, 515.937/s, 85.9895/s/gpu LR: 0.000100 Logit Scale: 99.877 Contrastive_loss: 3.8069 (5.9903) Dstill: 2.7253 (2.4000) Loss: 6.5322 (8.3903)
2025-02-04,23:42:18 | INFO | Train Epoch: 0 [1157898/1859739.0 (62%)] Data (t): 0.616 Batch (t): 1.597, 515.937/s, 85.9895/s/gpu LR: 0.000100 Logit Scale: 99.877 Contrastive_loss: 3.8069 (5.9903) Dstill: 2.7253 (2.4000) Loss: 6.5322 (8.3903)
INFO:root:Train Epoch: 0 [1197798/1859739.0 (64%)] Data (t): 0.608 Batch (t): 1.553, 494.748/s, 82.4579/s/gpu LR: 0.000100 Logit Scale: 99.869 Contrastive_loss: 4.2231 (5.9333) Dstill: 2.7465 (2.4112) Loss: 6.9695 (8.3445)
2025-02-04,23:43:36 | INFO | Train Epoch: 0 [1197798/1859739.0 (64%)] Data (t): 0.608 Batch (t): 1.553, 494.748/s, 82.4579/s/gpu LR: 0.000100 Logit Scale: 99.869 Contrastive_loss: 4.2231 (5.9333) Dstill: 2.7465 (2.4112) Loss: 6.9695 (8.3445)
INFO:root:Train Epoch: 0 [1237698/1859739.0 (67%)] Data (t): 0.608 Batch (t): 1.546, 529.231/s, 88.2052/s/gpu LR: 0.000100 Logit Scale: 99.866 Contrastive_loss: 3.8191 (5.8672) Dstill: 2.7102 (2.4206) Loss: 6.5293 (8.2878)
2025-02-04,23:44:53 | INFO | Train Epoch: 0 [1237698/1859739.0 (67%)] Data (t): 0.608 Batch (t): 1.546, 529.231/s, 88.2052/s/gpu LR: 0.000100 Logit Scale: 99.866 Contrastive_loss: 3.8191 (5.8672) Dstill: 2.7102 (2.4206) Loss: 6.5293 (8.2878)
INFO:root:Train Epoch: 0 [1277598/1859739.0 (69%)] Data (t): 0.625 Batch (t): 1.569, 504.870/s, 84.1450/s/gpu LR: 0.000100 Logit Scale: 99.861 Contrastive_loss: 3.9050 (5.8078) Dstill: 2.8830 (2.4346) Loss: 6.7880 (8.2423)
2025-02-04,23:46:11 | INFO | Train Epoch: 0 [1277598/1859739.0 (69%)] Data (t): 0.625 Batch (t): 1.569, 504.870/s, 84.1450/s/gpu LR: 0.000100 Logit Scale: 99.861 Contrastive_loss: 3.9050 (5.8078) Dstill: 2.8830 (2.4346) Loss: 6.7880 (8.2423)
INFO:root:Train Epoch: 0 [1317498/1859739.0 (71%)] Data (t): 0.602 Batch (t): 1.566, 540.992/s, 90.1654/s/gpu LR: 0.000100 Logit Scale: 99.855 Contrastive_loss: 3.6920 (5.7455) Dstill: 3.0232 (2.4519) Loss: 6.7152 (8.1974)
2025-02-04,23:47:30 | INFO | Train Epoch: 0 [1317498/1859739.0 (71%)] Data (t): 0.602 Batch (t): 1.566, 540.992/s, 90.1654/s/gpu LR: 0.000100 Logit Scale: 99.855 Contrastive_loss: 3.6920 (5.7455) Dstill: 3.0232 (2.4519) Loss: 6.7152 (8.1974)
INFO:root:Train Epoch: 0 [1357398/1859739.0 (73%)] Data (t): 0.616 Batch (t): 1.546, 528.878/s, 88.1463/s/gpu LR: 0.000100 Logit Scale: 99.859 Contrastive_loss: 3.6467 (5.6856) Dstill: 2.8477 (2.4632) Loss: 6.4943 (8.1488)
2025-02-04,23:48:47 | INFO | Train Epoch: 0 [1357398/1859739.0 (73%)] Data (t): 0.616 Batch (t): 1.546, 528.878/s, 88.1463/s/gpu LR: 0.000100 Logit Scale: 99.859 Contrastive_loss: 3.6467 (5.6856) Dstill: 2.8477 (2.4632) Loss: 6.4943 (8.1488)
INFO:root:Train Epoch: 0 [1397298/1859739.0 (75%)] Data (t): 0.594 Batch (t): 1.572, 498.199/s, 83.0331/s/gpu LR: 0.000100 Logit Scale: 99.860 Contrastive_loss: 3.7833 (5.6327) Dstill: 2.7236 (2.4704) Loss: 6.5069 (8.1031)
2025-02-04,23:50:06 | INFO | Train Epoch: 0 [1397298/1859739.0 (75%)] Data (t): 0.594 Batch (t): 1.572, 498.199/s, 83.0331/s/gpu LR: 0.000100 Logit Scale: 99.860 Contrastive_loss: 3.7833 (5.6327) Dstill: 2.7236 (2.4704) Loss: 6.5069 (8.1031)
INFO:root:Train Epoch: 0 [1437198/1859739.0 (77%)] Data (t): 0.602 Batch (t): 1.574, 489.688/s, 81.6147/s/gpu LR: 0.000100 Logit Scale: 99.848 Contrastive_loss: 3.6936 (5.5803) Dstill: 2.7905 (2.4791) Loss: 6.4841 (8.0594)
2025-02-04,23:51:24 | INFO | Train Epoch: 0 [1437198/1859739.0 (77%)] Data (t): 0.602 Batch (t): 1.574, 489.688/s, 81.6147/s/gpu LR: 0.000100 Logit Scale: 99.848 Contrastive_loss: 3.6936 (5.5803) Dstill: 2.7905 (2.4791) Loss: 6.4841 (8.0594)
INFO:root:Train Epoch: 0 [1477098/1859739.0 (79%)] Data (t): 0.604 Batch (t): 1.584, 507.467/s, 84.5778/s/gpu LR: 0.000100 Logit Scale: 99.847 Contrastive_loss: 3.4465 (5.5242) Dstill: 2.8076 (2.4877) Loss: 6.2541 (8.0119)
2025-02-04,23:52:43 | INFO | Train Epoch: 0 [1477098/1859739.0 (79%)] Data (t): 0.604 Batch (t): 1.584, 507.467/s, 84.5778/s/gpu LR: 0.000100 Logit Scale: 99.847 Contrastive_loss: 3.4465 (5.5242) Dstill: 2.8076 (2.4877) Loss: 6.2541 (8.0119)
INFO:root:Train Epoch: 0 [1516998/1859739.0 (82%)] Data (t): 0.601 Batch (t): 1.563, 554.954/s, 92.4924/s/gpu LR: 0.000100 Logit Scale: 99.844 Contrastive_loss: 3.6455 (5.4760) Dstill: 2.8099 (2.4960) Loss: 6.4554 (7.9720)
2025-02-04,23:54:02 | INFO | Train Epoch: 0 [1516998/1859739.0 (82%)] Data (t): 0.601 Batch (t): 1.563, 554.954/s, 92.4924/s/gpu LR: 0.000100 Logit Scale: 99.844 Contrastive_loss: 3.6455 (5.4760) Dstill: 2.8099 (2.4960) Loss: 6.4554 (7.9720)
INFO:root:Train Epoch: 0 [1556898/1859739.0 (84%)] Data (t): 0.612 Batch (t): 1.586, 545.770/s, 90.9617/s/gpu LR: 0.000100 Logit Scale: 99.841 Contrastive_loss: 3.7169 (5.4320) Dstill: 2.6466 (2.4998) Loss: 6.3636 (7.9318)
2025-02-04,23:55:21 | INFO | Train Epoch: 0 [1556898/1859739.0 (84%)] Data (t): 0.612 Batch (t): 1.586, 545.770/s, 90.9617/s/gpu LR: 0.000100 Logit Scale: 99.841 Contrastive_loss: 3.7169 (5.4320) Dstill: 2.6466 (2.4998) Loss: 6.3636 (7.9318)
INFO:root:Train Epoch: 0 [1596798/1859739.0 (86%)] Data (t): 0.595 Batch (t): 1.590, 505.864/s, 84.3106/s/gpu LR: 0.000100 Logit Scale: 99.839 Contrastive_loss: 3.6641 (5.3889) Dstill: 2.7613 (2.5061) Loss: 6.4254 (7.8950)
2025-02-04,23:56:40 | INFO | Train Epoch: 0 [1596798/1859739.0 (86%)] Data (t): 0.595 Batch (t): 1.590, 505.864/s, 84.3106/s/gpu LR: 0.000100 Logit Scale: 99.839 Contrastive_loss: 3.6641 (5.3889) Dstill: 2.7613 (2.5061) Loss: 6.4254 (7.8950)
INFO:root:Train Epoch: 0 [1636698/1859739.0 (88%)] Data (t): 0.597 Batch (t): 1.566, 480.738/s, 80.1230/s/gpu LR: 0.000100 Logit Scale: 99.839 Contrastive_loss: 3.7781 (5.3505) Dstill: 2.7746 (2.5125) Loss: 6.5526 (7.8631)
2025-02-04,23:57:59 | INFO | Train Epoch: 0 [1636698/1859739.0 (88%)] Data (t): 0.597 Batch (t): 1.566, 480.738/s, 80.1230/s/gpu LR: 0.000100 Logit Scale: 99.839 Contrastive_loss: 3.7781 (5.3505) Dstill: 2.7746 (2.5125) Loss: 6.5526 (7.8631)
INFO:root:Train Epoch: 0 [1676598/1859739.0 (90%)] Data (t): 0.592 Batch (t): 1.591, 510.148/s, 85.0247/s/gpu LR: 0.000100 Logit Scale: 99.830 Contrastive_loss: 3.8157 (5.3148) Dstill: 2.5983 (2.5145) Loss: 6.4140 (7.8294)
2025-02-04,23:59:18 | INFO | Train Epoch: 0 [1676598/1859739.0 (90%)] Data (t): 0.592 Batch (t): 1.591, 510.148/s, 85.0247/s/gpu LR: 0.000100 Logit Scale: 99.830 Contrastive_loss: 3.8157 (5.3148) Dstill: 2.5983 (2.5145) Loss: 6.4140 (7.8294)
INFO:root:Train Epoch: 0 [1716498/1859739.0 (92%)] Data (t): 0.550 Batch (t): 1.599, 506.500/s, 84.4167/s/gpu LR: 0.000100 Logit Scale: 99.829 Contrastive_loss: 3.7033 (5.2782) Dstill: 2.7762 (2.5205) Loss: 6.4795 (7.7987)
2025-02-05,00:00:38 | INFO | Train Epoch: 0 [1716498/1859739.0 (92%)] Data (t): 0.550 Batch (t): 1.599, 506.500/s, 84.4167/s/gpu LR: 0.000100 Logit Scale: 99.829 Contrastive_loss: 3.7033 (5.2782) Dstill: 2.7762 (2.5205) Loss: 6.4795 (7.7987)
INFO:root:Train Epoch: 0 [1756398/1859739.0 (94%)] Data (t): 0.558 Batch (t): 1.580, 528.167/s, 88.0278/s/gpu LR: 0.000100 Logit Scale: 99.830 Contrastive_loss: 3.8396 (5.2462) Dstill: 2.4742 (2.5194) Loss: 6.3138 (7.7657)
2025-02-05,00:01:57 | INFO | Train Epoch: 0 [1756398/1859739.0 (94%)] Data (t): 0.558 Batch (t): 1.580, 528.167/s, 88.0278/s/gpu LR: 0.000100 Logit Scale: 99.830 Contrastive_loss: 3.8396 (5.2462) Dstill: 2.4742 (2.5194) Loss: 6.3138 (7.7657)
INFO:root:Train Epoch: 0 [1796298/1859739.0 (97%)] Data (t): 0.529 Batch (t): 1.568, 528.076/s, 88.0126/s/gpu LR: 0.000100 Logit Scale: 99.827 Contrastive_loss: 3.9998 (5.2192) Dstill: 2.5761 (2.5207) Loss: 6.5759 (7.7398)
2025-02-05,00:03:16 | INFO | Train Epoch: 0 [1796298/1859739.0 (97%)] Data (t): 0.529 Batch (t): 1.568, 528.076/s, 88.0126/s/gpu LR: 0.000100 Logit Scale: 99.827 Contrastive_loss: 3.9998 (5.2192) Dstill: 2.5761 (2.5207) Loss: 6.5759 (7.7398)
INFO:root:Train Epoch: 0 [1836198/1859739.0 (99%)] Data (t): 0.548 Batch (t): 1.593, 489.236/s, 81.5393/s/gpu LR: 0.000100 Logit Scale: 99.822 Contrastive_loss: 3.7875 (5.1887) Dstill: 2.7479 (2.5255) Loss: 6.5354 (7.7142)
2025-02-05,00:04:35 | INFO | Train Epoch: 0 [1836198/1859739.0 (99%)] Data (t): 0.548 Batch (t): 1.593, 489.236/s, 81.5393/s/gpu LR: 0.000100 Logit Scale: 99.822 Contrastive_loss: 3.7875 (5.1887) Dstill: 2.7479 (2.5255) Loss: 6.5354 (7.7142)
INFO:root:Train Epoch: 0 [1860138/1859739.0 (100%)] Data (t): 0.560 Batch (t): 1.567, 496.928/s, 82.8213/s/gpu LR: 0.000100 Logit Scale: 99.827 Contrastive_loss: 3.6297 (5.1562) Dstill: 3.2782 (2.5412) Loss: 6.9079 (7.6974)
2025-02-05,00:05:22 | INFO | Train Epoch: 0 [1860138/1859739.0 (100%)] Data (t): 0.560 Batch (t): 1.567, 496.928/s, 82.8213/s/gpu LR: 0.000100 Logit Scale: 99.827 Contrastive_loss: 3.6297 (5.1562) Dstill: 3.2782 (2.5412) Loss: 6.9079 (7.6974)
INFO:root:Train Epoch: 0 [  798/59975.0 (1%)] Data (t): 0.987 Batch (t): 2.623, 304.177/s, 50.6961/s/gpu LR: 0.000000 Logit Scale: 99.828 Contrastive_loss: 8.3288 (8.3288) Dstill: 10.601 (10.601) Loss: 18.930 (18.930)
2025-02-05,00:05:25 | INFO | Train Epoch: 0 [  798/59975.0 (1%)] Data (t): 0.987 Batch (t): 2.623, 304.177/s, 50.6961/s/gpu LR: 0.000000 Logit Scale: 99.828 Contrastive_loss: 8.3288 (8.3288) Dstill: 10.601 (10.601) Loss: 18.930 (18.930)
INFO:root:Train Epoch: 0 [40698/59975.0 (67%)] Data (t): 0.463 Batch (t): 1.366, 595.420/s, 99.2367/s/gpu LR: 0.000005 Logit Scale: 99.822 Contrastive_loss: 7.4843 (7.9065) Dstill: 7.3089 (8.9549) Loss: 14.793 (16.861)
2025-02-05,00:06:33 | INFO | Train Epoch: 0 [40698/59975.0 (67%)] Data (t): 0.463 Batch (t): 1.366, 595.420/s, 99.2367/s/gpu LR: 0.000005 Logit Scale: 99.822 Contrastive_loss: 7.4843 (7.9065) Dstill: 7.3089 (8.9549) Loss: 14.793 (16.861)
INFO:root:Train Epoch: 0 [60648/59975.0 (100%)] Data (t): 0.462 Batch (t): 1.420, 559.201/s, 93.2001/s/gpu LR: 0.000008 Logit Scale: 99.821 Contrastive_loss: 7.0862 (7.6331) Dstill: 6.8227 (8.2442) Loss: 13.909 (15.877)
2025-02-05,00:07:09 | INFO | Train Epoch: 0 [60648/59975.0 (100%)] Data (t): 0.462 Batch (t): 1.420, 559.201/s, 93.2001/s/gpu LR: 0.000008 Logit Scale: 99.821 Contrastive_loss: 7.0862 (7.6331) Dstill: 6.8227 (8.2442) Loss: 13.909 (15.877)
INFO:root:Train Epoch: 0 [  798/47810.0 (2%)] Data (t): 3.048 Batch (t): 4.611, 173.071/s, 28.8452/s/gpu LR: 0.000000 Logit Scale: 99.821 Contrastive_loss: 9.4712 (9.4712) Dstill: 11.290 (11.290) Loss: 20.761 (20.761)
2025-02-05,00:07:13 | INFO | Train Epoch: 0 [  798/47810.0 (2%)] Data (t): 3.048 Batch (t): 4.611, 173.071/s, 28.8452/s/gpu LR: 0.000000 Logit Scale: 99.821 Contrastive_loss: 9.4712 (9.4712) Dstill: 11.290 (11.290) Loss: 20.761 (20.761)
INFO:root:Train Epoch: 0 [40698/47810.0 (85%)] Data (t): 2.255 Batch (t): 3.206, 249.649/s, 41.6081/s/gpu LR: 0.000005 Logit Scale: 99.817 Contrastive_loss: 7.4727 (8.4719) Dstill: 8.3873 (9.8384) Loss: 15.860 (18.310)
2025-02-05,00:09:54 | INFO | Train Epoch: 0 [40698/47810.0 (85%)] Data (t): 2.255 Batch (t): 3.206, 249.649/s, 41.6081/s/gpu LR: 0.000005 Logit Scale: 99.817 Contrastive_loss: 7.4727 (8.4719) Dstill: 8.3873 (9.8384) Loss: 15.860 (18.310)
INFO:root:Train Epoch: 0 [47880/47810.0 (100%)] Data (t): 2.231 Batch (t): 3.180, 255.008/s, 42.5014/s/gpu LR: 0.000006 Logit Scale: 99.817 Contrastive_loss: 7.1262 (8.0233) Dstill: 8.3091 (9.3286) Loss: 15.435 (17.352)
2025-02-05,00:10:22 | INFO | Train Epoch: 0 [47880/47810.0 (100%)] Data (t): 2.231 Batch (t): 3.180, 255.008/s, 42.5014/s/gpu LR: 0.000006 Logit Scale: 99.817 Contrastive_loss: 7.1262 (8.0233) Dstill: 8.3091 (9.3286) Loss: 15.435 (17.352)
INFO:root:Train Epoch: 0 [ 798/7500.0 (10%)] Data (t): 36.896 Batch (t): 47.694, 16.7317/s, 2.78862/s/gpu LR: 0.000000 Logit Scale: 99.817 Contrastive_loss: 14.651 (14.651) Dstill: 20.821 (20.821) Loss: 35.472 (35.472)
2025-02-05,00:11:10 | INFO | Train Epoch: 0 [ 798/7500.0 (10%)] Data (t): 36.896 Batch (t): 47.694, 16.7317/s, 2.78862/s/gpu LR: 0.000000 Logit Scale: 99.817 Contrastive_loss: 14.651 (14.651) Dstill: 20.821 (20.821) Loss: 35.472 (35.472)
INFO:root:Train Epoch: 0 [7980/7500.0 (100%)] Data (t): 35.174 Batch (t): 42.128, 18.8618/s, 3.14363/s/gpu LR: 0.000001 Logit Scale: 99.817 Contrastive_loss: 8.2740 (11.462) Dstill: 14.930 (17.875) Loss: 23.204 (29.338)
2025-02-05,00:17:29 | INFO | Train Epoch: 0 [7980/7500.0 (100%)] Data (t): 35.174 Batch (t): 42.128, 18.8618/s, 3.14363/s/gpu LR: 0.000001 Logit Scale: 99.817 Contrastive_loss: 8.2740 (11.462) Dstill: 14.930 (17.875) Loss: 23.204 (29.338)
INFO:root:Start epoch 1
2025-02-05,00:17:30 | INFO | Start epoch 1
INFO:root:Train Epoch: 1 [    798/1859739.0 (0%)] Data (t): 0.679 Batch (t): 1.690, 472.251/s, 78.7085/s/gpu LR: 0.000100 Logit Scale: 99.817 Contrastive_loss: 6.9160 (6.9160) Dstill: 4.6167 (4.6167) Loss: 11.533 (11.533)
2025-02-05,00:17:32 | INFO | Train Epoch: 1 [    798/1859739.0 (0%)] Data (t): 0.679 Batch (t): 1.690, 472.251/s, 78.7085/s/gpu LR: 0.000100 Logit Scale: 99.817 Contrastive_loss: 6.9160 (6.9160) Dstill: 4.6167 (4.6167) Loss: 11.533 (11.533)
INFO:root:Train Epoch: 1 [  40698/1859739.0 (2%)] Data (t): 0.379 Batch (t): 1.335, 605.896/s, 100.983/s/gpu LR: 0.000100 Logit Scale: 99.775 Contrastive_loss: 3.9714 (5.4437) Dstill: 7.6618 (6.1393) Loss: 11.633 (11.583)
2025-02-05,00:18:39 | INFO | Train Epoch: 1 [  40698/1859739.0 (2%)] Data (t): 0.379 Batch (t): 1.335, 605.896/s, 100.983/s/gpu LR: 0.000100 Logit Scale: 99.775 Contrastive_loss: 3.9714 (5.4437) Dstill: 7.6618 (6.1393) Loss: 11.633 (11.583)
INFO:root:Train Epoch: 1 [  80598/1859739.0 (4%)] Data (t): 0.374 Batch (t): 1.330, 618.923/s, 103.154/s/gpu LR: 0.000100 Logit Scale: 99.776 Contrastive_loss: 3.6798 (4.8557) Dstill: 5.7377 (6.0054) Loss: 9.4174 (10.861)
2025-02-05,00:19:45 | INFO | Train Epoch: 1 [  80598/1859739.0 (4%)] Data (t): 0.374 Batch (t): 1.330, 618.923/s, 103.154/s/gpu LR: 0.000100 Logit Scale: 99.776 Contrastive_loss: 3.6798 (4.8557) Dstill: 5.7377 (6.0054) Loss: 9.4174 (10.861)
INFO:root:Train Epoch: 1 [ 120498/1859739.0 (6%)] Data (t): 0.395 Batch (t): 1.315, 654.783/s, 109.130/s/gpu LR: 0.000100 Logit Scale: 99.782 Contrastive_loss: 3.6822 (4.5624) Dstill: 4.5937 (5.6525) Loss: 8.2760 (10.215)
2025-02-05,00:20:51 | INFO | Train Epoch: 1 [ 120498/1859739.0 (6%)] Data (t): 0.395 Batch (t): 1.315, 654.783/s, 109.130/s/gpu LR: 0.000100 Logit Scale: 99.782 Contrastive_loss: 3.6822 (4.5624) Dstill: 4.5937 (5.6525) Loss: 8.2760 (10.215)
INFO:root:Train Epoch: 1 [ 160398/1859739.0 (9%)] Data (t): 0.432 Batch (t): 1.369, 514.919/s, 85.8198/s/gpu LR: 0.000100 Logit Scale: 99.776 Contrastive_loss: 3.5579 (4.3615) Dstill: 3.7890 (5.2798) Loss: 7.3469 (9.6413)
2025-02-05,00:22:00 | INFO | Train Epoch: 1 [ 160398/1859739.0 (9%)] Data (t): 0.432 Batch (t): 1.369, 514.919/s, 85.8198/s/gpu LR: 0.000100 Logit Scale: 99.776 Contrastive_loss: 3.5579 (4.3615) Dstill: 3.7890 (5.2798) Loss: 7.3469 (9.6413)
INFO:root:Train Epoch: 1 [ 200298/1859739.0 (11%)] Data (t): 0.503 Batch (t): 1.445, 544.449/s, 90.7415/s/gpu LR: 0.000100 Logit Scale: 99.774 Contrastive_loss: 3.7086 (4.2527) Dstill: 3.0738 (4.9121) Loss: 6.7824 (9.1648)
2025-02-05,00:23:12 | INFO | Train Epoch: 1 [ 200298/1859739.0 (11%)] Data (t): 0.503 Batch (t): 1.445, 544.449/s, 90.7415/s/gpu LR: 0.000100 Logit Scale: 99.774 Contrastive_loss: 3.7086 (4.2527) Dstill: 3.0738 (4.9121) Loss: 6.7824 (9.1648)
INFO:root:Train Epoch: 1 [ 240198/1859739.0 (13%)] Data (t): 0.503 Batch (t): 1.450, 584.494/s, 97.4156/s/gpu LR: 0.000100 Logit Scale: 99.774 Contrastive_loss: 3.8171 (4.1904) Dstill: 3.0326 (4.6436) Loss: 6.8497 (8.8340)
2025-02-05,00:24:24 | INFO | Train Epoch: 1 [ 240198/1859739.0 (13%)] Data (t): 0.503 Batch (t): 1.450, 584.494/s, 97.4156/s/gpu LR: 0.000100 Logit Scale: 99.774 Contrastive_loss: 3.8171 (4.1904) Dstill: 3.0326 (4.6436) Loss: 6.8497 (8.8340)
INFO:root:Train Epoch: 1 [ 280098/1859739.0 (15%)] Data (t): 0.505 Batch (t): 1.463, 543.665/s, 90.6108/s/gpu LR: 0.000100 Logit Scale: 99.769 Contrastive_loss: 3.4072 (4.0925) Dstill: 3.0006 (4.4382) Loss: 6.4078 (8.5308)
2025-02-05,00:25:37 | INFO | Train Epoch: 1 [ 280098/1859739.0 (15%)] Data (t): 0.505 Batch (t): 1.463, 543.665/s, 90.6108/s/gpu LR: 0.000100 Logit Scale: 99.769 Contrastive_loss: 3.4072 (4.0925) Dstill: 3.0006 (4.4382) Loss: 6.4078 (8.5308)
INFO:root:Train Epoch: 1 [ 319998/1859739.0 (17%)] Data (t): 0.576 Batch (t): 1.509, 550.427/s, 91.7378/s/gpu LR: 0.000100 Logit Scale: 99.764 Contrastive_loss: 3.4974 (4.0264) Dstill: 3.1117 (4.2908) Loss: 6.6091 (8.3173)
2025-02-05,00:26:53 | INFO | Train Epoch: 1 [ 319998/1859739.0 (17%)] Data (t): 0.576 Batch (t): 1.509, 550.427/s, 91.7378/s/gpu LR: 0.000100 Logit Scale: 99.764 Contrastive_loss: 3.4974 (4.0264) Dstill: 3.1117 (4.2908) Loss: 6.6091 (8.3173)
INFO:root:Train Epoch: 1 [ 359898/1859739.0 (19%)] Data (t): 0.523 Batch (t): 1.491, 557.115/s, 92.8525/s/gpu LR: 0.000100 Logit Scale: 99.769 Contrastive_loss: 3.5493 (3.9787) Dstill: 2.8613 (4.1479) Loss: 6.4106 (8.1266)
2025-02-05,00:28:07 | INFO | Train Epoch: 1 [ 359898/1859739.0 (19%)] Data (t): 0.523 Batch (t): 1.491, 557.115/s, 92.8525/s/gpu LR: 0.000100 Logit Scale: 99.769 Contrastive_loss: 3.5493 (3.9787) Dstill: 2.8613 (4.1479) Loss: 6.4106 (8.1266)
INFO:root:Train Epoch: 1 [ 399798/1859739.0 (21%)] Data (t): 0.545 Batch (t): 1.505, 535.399/s, 89.2332/s/gpu LR: 0.000100 Logit Scale: 99.763 Contrastive_loss: 3.6301 (3.9470) Dstill: 2.9233 (4.0366) Loss: 6.5534 (7.9836)
2025-02-05,00:29:23 | INFO | Train Epoch: 1 [ 399798/1859739.0 (21%)] Data (t): 0.545 Batch (t): 1.505, 535.399/s, 89.2332/s/gpu LR: 0.000100 Logit Scale: 99.763 Contrastive_loss: 3.6301 (3.9470) Dstill: 2.9233 (4.0366) Loss: 6.5534 (7.9836)
INFO:root:Train Epoch: 1 [ 439698/1859739.0 (24%)] Data (t): 0.578 Batch (t): 1.542, 508.486/s, 84.7476/s/gpu LR: 0.000100 Logit Scale: 99.756 Contrastive_loss: 3.4445 (3.9051) Dstill: 3.0447 (3.9539) Loss: 6.4893 (7.8590)
2025-02-05,00:30:40 | INFO | Train Epoch: 1 [ 439698/1859739.0 (24%)] Data (t): 0.578 Batch (t): 1.542, 508.486/s, 84.7476/s/gpu LR: 0.000100 Logit Scale: 99.756 Contrastive_loss: 3.4445 (3.9051) Dstill: 3.0447 (3.9539) Loss: 6.4893 (7.8590)
INFO:root:Train Epoch: 1 [ 479598/1859739.0 (26%)] Data (t): 0.552 Batch (t): 1.513, 563.529/s, 93.9215/s/gpu LR: 0.000100 Logit Scale: 99.749 Contrastive_loss: 3.5150 (3.8751) Dstill: 2.8903 (3.8721) Loss: 6.4053 (7.7472)
2025-02-05,00:31:55 | INFO | Train Epoch: 1 [ 479598/1859739.0 (26%)] Data (t): 0.552 Batch (t): 1.513, 563.529/s, 93.9215/s/gpu LR: 0.000100 Logit Scale: 99.749 Contrastive_loss: 3.5150 (3.8751) Dstill: 2.8903 (3.8721) Loss: 6.4053 (7.7472)
INFO:root:Train Epoch: 1 [ 519498/1859739.0 (28%)] Data (t): 0.552 Batch (t): 1.507, 484.041/s, 80.6735/s/gpu LR: 0.000100 Logit Scale: 99.757 Contrastive_loss: 3.5351 (3.8508) Dstill: 2.7635 (3.7929) Loss: 6.2986 (7.6438)
2025-02-05,00:33:11 | INFO | Train Epoch: 1 [ 519498/1859739.0 (28%)] Data (t): 0.552 Batch (t): 1.507, 484.041/s, 80.6735/s/gpu LR: 0.000100 Logit Scale: 99.757 Contrastive_loss: 3.5351 (3.8508) Dstill: 2.7635 (3.7929) Loss: 6.2986 (7.6438)
INFO:root:Train Epoch: 1 [ 559398/1859739.0 (30%)] Data (t): 0.533 Batch (t): 1.495, 501.162/s, 83.5269/s/gpu LR: 0.000100 Logit Scale: 99.748 Contrastive_loss: 3.6919 (3.8402) Dstill: 2.9696 (3.7380) Loss: 6.6615 (7.5783)
2025-02-05,00:34:26 | INFO | Train Epoch: 1 [ 559398/1859739.0 (30%)] Data (t): 0.533 Batch (t): 1.495, 501.162/s, 83.5269/s/gpu LR: 0.000100 Logit Scale: 99.748 Contrastive_loss: 3.6919 (3.8402) Dstill: 2.9696 (3.7380) Loss: 6.6615 (7.5783)
INFO:root:Train Epoch: 1 [ 599298/1859739.0 (32%)] Data (t): 0.617 Batch (t): 1.581, 494.694/s, 82.4491/s/gpu LR: 0.000100 Logit Scale: 99.753 Contrastive_loss: 3.6608 (3.8290) Dstill: 2.9548 (3.6891) Loss: 6.6156 (7.5181)
2025-02-05,00:35:45 | INFO | Train Epoch: 1 [ 599298/1859739.0 (32%)] Data (t): 0.617 Batch (t): 1.581, 494.694/s, 82.4491/s/gpu LR: 0.000100 Logit Scale: 99.753 Contrastive_loss: 3.6608 (3.8290) Dstill: 2.9548 (3.6891) Loss: 6.6156 (7.5181)
INFO:root:Train Epoch: 1 [ 639198/1859739.0 (34%)] Data (t): 0.598 Batch (t): 1.556, 470.596/s, 78.4327/s/gpu LR: 0.000100 Logit Scale: 99.751 Contrastive_loss: 3.5969 (3.8154) Dstill: 2.7547 (3.6341) Loss: 6.3516 (7.4495)
2025-02-05,00:37:02 | INFO | Train Epoch: 1 [ 639198/1859739.0 (34%)] Data (t): 0.598 Batch (t): 1.556, 470.596/s, 78.4327/s/gpu LR: 0.000100 Logit Scale: 99.751 Contrastive_loss: 3.5969 (3.8154) Dstill: 2.7547 (3.6341) Loss: 6.3516 (7.4495)
INFO:root:Train Epoch: 1 [ 679098/1859739.0 (37%)] Data (t): 0.581 Batch (t): 1.537, 513.145/s, 85.5242/s/gpu LR: 0.000100 Logit Scale: 99.751 Contrastive_loss: 3.3927 (3.7919) Dstill: 3.0727 (3.6029) Loss: 6.4654 (7.3948)
2025-02-05,00:38:19 | INFO | Train Epoch: 1 [ 679098/1859739.0 (37%)] Data (t): 0.581 Batch (t): 1.537, 513.145/s, 85.5242/s/gpu LR: 0.000100 Logit Scale: 99.751 Contrastive_loss: 3.3927 (3.7919) Dstill: 3.0727 (3.6029) Loss: 6.4654 (7.3948)
INFO:root:Train Epoch: 1 [ 718998/1859739.0 (39%)] Data (t): 0.602 Batch (t): 1.579, 464.503/s, 77.4172/s/gpu LR: 0.000100 Logit Scale: 99.754 Contrastive_loss: 3.5601 (3.7797) Dstill: 3.0455 (3.5736) Loss: 6.6055 (7.3533)
2025-02-05,00:39:38 | INFO | Train Epoch: 1 [ 718998/1859739.0 (39%)] Data (t): 0.602 Batch (t): 1.579, 464.503/s, 77.4172/s/gpu LR: 0.000100 Logit Scale: 99.754 Contrastive_loss: 3.5601 (3.7797) Dstill: 3.0455 (3.5736) Loss: 6.6055 (7.3533)
INFO:root:Train Epoch: 1 [ 758898/1859739.0 (41%)] Data (t): 0.560 Batch (t): 1.571, 485.047/s, 80.8412/s/gpu LR: 0.000100 Logit Scale: 99.752 Contrastive_loss: 3.6495 (3.7732) Dstill: 2.7662 (3.5332) Loss: 6.4157 (7.3064)
2025-02-05,00:40:57 | INFO | Train Epoch: 1 [ 758898/1859739.0 (41%)] Data (t): 0.560 Batch (t): 1.571, 485.047/s, 80.8412/s/gpu LR: 0.000100 Logit Scale: 99.752 Contrastive_loss: 3.6495 (3.7732) Dstill: 2.7662 (3.5332) Loss: 6.4157 (7.3064)
INFO:root:Train Epoch: 1 [ 798798/1859739.0 (43%)] Data (t): 0.591 Batch (t): 1.553, 517.036/s, 86.1727/s/gpu LR: 0.000100 Logit Scale: 99.760 Contrastive_loss: 3.2038 (3.7461) Dstill: 2.7220 (3.4946) Loss: 5.9258 (7.2407)
2025-02-05,00:42:14 | INFO | Train Epoch: 1 [ 798798/1859739.0 (43%)] Data (t): 0.591 Batch (t): 1.553, 517.036/s, 86.1727/s/gpu LR: 0.000100 Logit Scale: 99.760 Contrastive_loss: 3.2038 (3.7461) Dstill: 2.7220 (3.4946) Loss: 5.9258 (7.2407)
INFO:root:Train Epoch: 1 [ 838698/1859739.0 (45%)] Data (t): 0.581 Batch (t): 1.549, 530.505/s, 88.4175/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.6490 (3.7417) Dstill: 2.8380 (3.4647) Loss: 6.4870 (7.2064)
2025-02-05,00:43:32 | INFO | Train Epoch: 1 [ 838698/1859739.0 (45%)] Data (t): 0.581 Batch (t): 1.549, 530.505/s, 88.4175/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.6490 (3.7417) Dstill: 2.8380 (3.4647) Loss: 6.4870 (7.2064)
INFO:root:Train Epoch: 1 [ 878598/1859739.0 (47%)] Data (t): 0.585 Batch (t): 1.569, 476.009/s, 79.3349/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.2230 (3.7191) Dstill: 2.7040 (3.4317) Loss: 5.9270 (7.1508)
2025-02-05,00:44:50 | INFO | Train Epoch: 1 [ 878598/1859739.0 (47%)] Data (t): 0.585 Batch (t): 1.569, 476.009/s, 79.3349/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.2230 (3.7191) Dstill: 2.7040 (3.4317) Loss: 5.9270 (7.1508)
INFO:root:Train Epoch: 1 [ 918498/1859739.0 (49%)] Data (t): 0.583 Batch (t): 1.550, 522.565/s, 87.0942/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.4997 (3.7100) Dstill: 2.6289 (3.3982) Loss: 6.1286 (7.1082)
2025-02-05,00:46:08 | INFO | Train Epoch: 1 [ 918498/1859739.0 (49%)] Data (t): 0.583 Batch (t): 1.550, 522.565/s, 87.0942/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.4997 (3.7100) Dstill: 2.6289 (3.3982) Loss: 6.1286 (7.1082)
INFO:root:Train Epoch: 1 [ 958398/1859739.0 (52%)] Data (t): 0.592 Batch (t): 1.546, 503.261/s, 83.8768/s/gpu LR: 0.000100 Logit Scale: 99.757 Contrastive_loss: 3.4687 (3.7003) Dstill: 3.2251 (3.3913) Loss: 6.6938 (7.0916)
2025-02-05,00:47:25 | INFO | Train Epoch: 1 [ 958398/1859739.0 (52%)] Data (t): 0.592 Batch (t): 1.546, 503.261/s, 83.8768/s/gpu LR: 0.000100 Logit Scale: 99.757 Contrastive_loss: 3.4687 (3.7003) Dstill: 3.2251 (3.3913) Loss: 6.6938 (7.0916)
INFO:root:Train Epoch: 1 [ 998298/1859739.0 (54%)] Data (t): 0.604 Batch (t): 1.560, 485.655/s, 80.9425/s/gpu LR: 0.000100 Logit Scale: 99.754 Contrastive_loss: 3.7717 (3.7031) Dstill: 2.7301 (3.3659) Loss: 6.5018 (7.0689)
2025-02-05,00:48:43 | INFO | Train Epoch: 1 [ 998298/1859739.0 (54%)] Data (t): 0.604 Batch (t): 1.560, 485.655/s, 80.9425/s/gpu LR: 0.000100 Logit Scale: 99.754 Contrastive_loss: 3.7717 (3.7031) Dstill: 2.7301 (3.3659) Loss: 6.5018 (7.0689)
INFO:root:Train Epoch: 1 [1038198/1859739.0 (56%)] Data (t): 0.578 Batch (t): 1.554, 500.243/s, 83.3738/s/gpu LR: 0.000100 Logit Scale: 99.759 Contrastive_loss: 3.1820 (3.6838) Dstill: 2.8515 (3.3468) Loss: 6.0335 (7.0306)
2025-02-05,00:50:01 | INFO | Train Epoch: 1 [1038198/1859739.0 (56%)] Data (t): 0.578 Batch (t): 1.554, 500.243/s, 83.3738/s/gpu LR: 0.000100 Logit Scale: 99.759 Contrastive_loss: 3.1820 (3.6838) Dstill: 2.8515 (3.3468) Loss: 6.0335 (7.0306)
INFO:root:Train Epoch: 1 [1078098/1859739.0 (58%)] Data (t): 0.605 Batch (t): 1.544, 510.024/s, 85.0040/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3299 (3.6711) Dstill: 2.7554 (3.3257) Loss: 6.0853 (6.9968)
2025-02-05,00:51:18 | INFO | Train Epoch: 1 [1078098/1859739.0 (58%)] Data (t): 0.605 Batch (t): 1.544, 510.024/s, 85.0040/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3299 (3.6711) Dstill: 2.7554 (3.3257) Loss: 6.0853 (6.9968)
INFO:root:Train Epoch: 1 [1117998/1859739.0 (60%)] Data (t): 0.611 Batch (t): 1.559, 513.391/s, 85.5651/s/gpu LR: 0.000100 Logit Scale: 99.771 Contrastive_loss: 3.5852 (3.6682) Dstill: 2.7165 (3.3047) Loss: 6.3017 (6.9728)
2025-02-05,00:52:36 | INFO | Train Epoch: 1 [1117998/1859739.0 (60%)] Data (t): 0.611 Batch (t): 1.559, 513.391/s, 85.5651/s/gpu LR: 0.000100 Logit Scale: 99.771 Contrastive_loss: 3.5852 (3.6682) Dstill: 2.7165 (3.3047) Loss: 6.3017 (6.9728)
INFO:root:Train Epoch: 1 [1157898/1859739.0 (62%)] Data (t): 0.606 Batch (t): 1.582, 512.003/s, 85.3338/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3879 (3.6588) Dstill: 2.7037 (3.2846) Loss: 6.0916 (6.9435)
2025-02-05,00:53:55 | INFO | Train Epoch: 1 [1157898/1859739.0 (62%)] Data (t): 0.606 Batch (t): 1.582, 512.003/s, 85.3338/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3879 (3.6588) Dstill: 2.7037 (3.2846) Loss: 6.0916 (6.9435)
INFO:root:Train Epoch: 1 [1197798/1859739.0 (64%)] Data (t): 0.600 Batch (t): 1.548, 525.038/s, 87.5063/s/gpu LR: 0.000100 Logit Scale: 99.759 Contrastive_loss: 3.8890 (3.6662) Dstill: 2.9022 (3.2723) Loss: 6.7912 (6.9386)
2025-02-05,00:55:12 | INFO | Train Epoch: 1 [1197798/1859739.0 (64%)] Data (t): 0.600 Batch (t): 1.548, 525.038/s, 87.5063/s/gpu LR: 0.000100 Logit Scale: 99.759 Contrastive_loss: 3.8890 (3.6662) Dstill: 2.9022 (3.2723) Loss: 6.7912 (6.9386)
INFO:root:Train Epoch: 1 [1237698/1859739.0 (67%)] Data (t): 0.610 Batch (t): 1.550, 541.114/s, 90.1856/s/gpu LR: 0.000100 Logit Scale: 99.766 Contrastive_loss: 3.5486 (3.6626) Dstill: 2.7010 (3.2545) Loss: 6.2496 (6.9170)
2025-02-05,00:56:30 | INFO | Train Epoch: 1 [1237698/1859739.0 (67%)] Data (t): 0.610 Batch (t): 1.550, 541.114/s, 90.1856/s/gpu LR: 0.000100 Logit Scale: 99.766 Contrastive_loss: 3.5486 (3.6626) Dstill: 2.7010 (3.2545) Loss: 6.2496 (6.9170)
INFO:root:Train Epoch: 1 [1277598/1859739.0 (69%)] Data (t): 0.622 Batch (t): 1.561, 505.143/s, 84.1905/s/gpu LR: 0.000100 Logit Scale: 99.771 Contrastive_loss: 3.7238 (3.6644) Dstill: 2.7250 (3.2384) Loss: 6.4488 (6.9028)
2025-02-05,00:57:48 | INFO | Train Epoch: 1 [1277598/1859739.0 (69%)] Data (t): 0.622 Batch (t): 1.561, 505.143/s, 84.1905/s/gpu LR: 0.000100 Logit Scale: 99.771 Contrastive_loss: 3.7238 (3.6644) Dstill: 2.7250 (3.2384) Loss: 6.4488 (6.9028)
INFO:root:Train Epoch: 1 [1317498/1859739.0 (71%)] Data (t): 0.605 Batch (t): 1.572, 569.296/s, 94.8826/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3603 (3.6555) Dstill: 2.7467 (3.2240) Loss: 6.1070 (6.8794)
2025-02-05,00:59:07 | INFO | Train Epoch: 1 [1317498/1859739.0 (71%)] Data (t): 0.605 Batch (t): 1.572, 569.296/s, 94.8826/s/gpu LR: 0.000100 Logit Scale: 99.767 Contrastive_loss: 3.3603 (3.6555) Dstill: 2.7467 (3.2240) Loss: 6.1070 (6.8794)
INFO:root:Train Epoch: 1 [1357398/1859739.0 (73%)] Data (t): 0.631 Batch (t): 1.565, 493.005/s, 82.1674/s/gpu LR: 0.000100 Logit Scale: 99.780 Contrastive_loss: 3.4326 (3.6491) Dstill: 2.7460 (3.2103) Loss: 6.1787 (6.8594)
2025-02-05,01:00:25 | INFO | Train Epoch: 1 [1357398/1859739.0 (73%)] Data (t): 0.631 Batch (t): 1.565, 493.005/s, 82.1674/s/gpu LR: 0.000100 Logit Scale: 99.780 Contrastive_loss: 3.4326 (3.6491) Dstill: 2.7460 (3.2103) Loss: 6.1787 (6.8594)
INFO:root:Train Epoch: 1 [1397298/1859739.0 (75%)] Data (t): 0.600 Batch (t): 1.551, 513.487/s, 85.5812/s/gpu LR: 0.000100 Logit Scale: 99.786 Contrastive_loss: 3.6263 (3.6485) Dstill: 2.8362 (3.1999) Loss: 6.4626 (6.8484)
2025-02-05,01:01:42 | INFO | Train Epoch: 1 [1397298/1859739.0 (75%)] Data (t): 0.600 Batch (t): 1.551, 513.487/s, 85.5812/s/gpu LR: 0.000100 Logit Scale: 99.786 Contrastive_loss: 3.6263 (3.6485) Dstill: 2.8362 (3.1999) Loss: 6.4626 (6.8484)
INFO:root:Train Epoch: 1 [1437198/1859739.0 (77%)] Data (t): 0.626 Batch (t): 1.582, 491.045/s, 81.8409/s/gpu LR: 0.000100 Logit Scale: 99.780 Contrastive_loss: 3.4814 (3.6440) Dstill: 2.6336 (3.1846) Loss: 6.1149 (6.8286)
2025-02-05,01:03:01 | INFO | Train Epoch: 1 [1437198/1859739.0 (77%)] Data (t): 0.626 Batch (t): 1.582, 491.045/s, 81.8409/s/gpu LR: 0.000100 Logit Scale: 99.780 Contrastive_loss: 3.4814 (3.6440) Dstill: 2.6336 (3.1846) Loss: 6.1149 (6.8286)
INFO:root:Train Epoch: 1 [1477098/1859739.0 (79%)] Data (t): 0.638 Batch (t): 1.573, 510.497/s, 85.0829/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.1437 (3.6308) Dstill: 2.5860 (3.1688) Loss: 5.7297 (6.7996)
2025-02-05,01:04:20 | INFO | Train Epoch: 1 [1477098/1859739.0 (79%)] Data (t): 0.638 Batch (t): 1.573, 510.497/s, 85.0829/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.1437 (3.6308) Dstill: 2.5860 (3.1688) Loss: 5.7297 (6.7996)
INFO:root:Train Epoch: 1 [1516998/1859739.0 (82%)] Data (t): 0.627 Batch (t): 1.561, 512.093/s, 85.3489/s/gpu LR: 0.000100 Logit Scale: 99.795 Contrastive_loss: 3.3878 (3.6246) Dstill: 2.7213 (3.1574) Loss: 6.1092 (6.7819)
2025-02-05,01:05:38 | INFO | Train Epoch: 1 [1516998/1859739.0 (82%)] Data (t): 0.627 Batch (t): 1.561, 512.093/s, 85.3489/s/gpu LR: 0.000100 Logit Scale: 99.795 Contrastive_loss: 3.3878 (3.6246) Dstill: 2.7213 (3.1574) Loss: 6.1092 (6.7819)
INFO:root:Train Epoch: 1 [1556898/1859739.0 (84%)] Data (t): 0.626 Batch (t): 1.595, 489.092/s, 81.5153/s/gpu LR: 0.000100 Logit Scale: 99.799 Contrastive_loss: 3.3406 (3.6175) Dstill: 2.7257 (3.1466) Loss: 6.0663 (6.7641)
2025-02-05,01:06:58 | INFO | Train Epoch: 1 [1556898/1859739.0 (84%)] Data (t): 0.626 Batch (t): 1.595, 489.092/s, 81.5153/s/gpu LR: 0.000100 Logit Scale: 99.799 Contrastive_loss: 3.3406 (3.6175) Dstill: 2.7257 (3.1466) Loss: 6.0663 (6.7641)
INFO:root:Train Epoch: 1 [1596798/1859739.0 (86%)] Data (t): 0.620 Batch (t): 1.595, 531.788/s, 88.6313/s/gpu LR: 0.000100 Logit Scale: 99.807 Contrastive_loss: 3.3875 (3.6119) Dstill: 2.5288 (3.1315) Loss: 5.9163 (6.7434)
2025-02-05,01:08:18 | INFO | Train Epoch: 1 [1596798/1859739.0 (86%)] Data (t): 0.620 Batch (t): 1.595, 531.788/s, 88.6313/s/gpu LR: 0.000100 Logit Scale: 99.807 Contrastive_loss: 3.3875 (3.6119) Dstill: 2.5288 (3.1315) Loss: 5.9163 (6.7434)
INFO:root:Train Epoch: 1 [1636698/1859739.0 (88%)] Data (t): 0.631 Batch (t): 1.569, 484.387/s, 80.7312/s/gpu LR: 0.000100 Logit Scale: 99.811 Contrastive_loss: 3.5518 (3.6104) Dstill: 2.6093 (3.1191) Loss: 6.1612 (6.7295)
2025-02-05,01:09:36 | INFO | Train Epoch: 1 [1636698/1859739.0 (88%)] Data (t): 0.631 Batch (t): 1.569, 484.387/s, 80.7312/s/gpu LR: 0.000100 Logit Scale: 99.811 Contrastive_loss: 3.5518 (3.6104) Dstill: 2.6093 (3.1191) Loss: 6.1612 (6.7295)
INFO:root:Train Epoch: 1 [1676598/1859739.0 (90%)] Data (t): 0.574 Batch (t): 1.549, 544.804/s, 90.8007/s/gpu LR: 0.000100 Logit Scale: 99.810 Contrastive_loss: 3.6537 (3.6114) Dstill: 2.6272 (3.1076) Loss: 6.2809 (6.7191)
2025-02-05,01:10:54 | INFO | Train Epoch: 1 [1676598/1859739.0 (90%)] Data (t): 0.574 Batch (t): 1.549, 544.804/s, 90.8007/s/gpu LR: 0.000100 Logit Scale: 99.810 Contrastive_loss: 3.6537 (3.6114) Dstill: 2.6272 (3.1076) Loss: 6.2809 (6.7191)
INFO:root:Train Epoch: 1 [1716498/1859739.0 (92%)] Data (t): 0.512 Batch (t): 1.502, 612.739/s, 102.123/s/gpu LR: 0.000100 Logit Scale: 99.815 Contrastive_loss: 3.4931 (3.6087) Dstill: 2.6179 (3.0965) Loss: 6.1111 (6.7053)
2025-02-05,01:12:09 | INFO | Train Epoch: 1 [1716498/1859739.0 (92%)] Data (t): 0.512 Batch (t): 1.502, 612.739/s, 102.123/s/gpu LR: 0.000100 Logit Scale: 99.815 Contrastive_loss: 3.4931 (3.6087) Dstill: 2.6179 (3.0965) Loss: 6.1111 (6.7053)
INFO:root:Train Epoch: 1 [1756398/1859739.0 (94%)] Data (t): 0.471 Batch (t): 1.434, 558.343/s, 93.0571/s/gpu LR: 0.000100 Logit Scale: 99.825 Contrastive_loss: 3.6376 (3.6094) Dstill: 2.6085 (3.0857) Loss: 6.2461 (6.6951)
2025-02-05,01:13:20 | INFO | Train Epoch: 1 [1756398/1859739.0 (94%)] Data (t): 0.471 Batch (t): 1.434, 558.343/s, 93.0571/s/gpu LR: 0.000100 Logit Scale: 99.825 Contrastive_loss: 3.6376 (3.6094) Dstill: 2.6085 (3.0857) Loss: 6.2461 (6.6951)
INFO:root:Train Epoch: 1 [1796298/1859739.0 (97%)] Data (t): 0.472 Batch (t): 1.461, 537.625/s, 89.6041/s/gpu LR: 0.000100 Logit Scale: 99.833 Contrastive_loss: 3.7348 (3.6121) Dstill: 2.6984 (3.0772) Loss: 6.4332 (6.6894)
2025-02-05,01:14:33 | INFO | Train Epoch: 1 [1796298/1859739.0 (97%)] Data (t): 0.472 Batch (t): 1.461, 537.625/s, 89.6041/s/gpu LR: 0.000100 Logit Scale: 99.833 Contrastive_loss: 3.7348 (3.6121) Dstill: 2.6984 (3.0772) Loss: 6.4332 (6.6894)
INFO:root:Train Epoch: 1 [1836198/1859739.0 (99%)] Data (t): 0.519 Batch (t): 1.543, 515.429/s, 85.9049/s/gpu LR: 0.000100 Logit Scale: 99.831 Contrastive_loss: 3.5714 (3.6113) Dstill: 2.8207 (3.0718) Loss: 6.3922 (6.6830)
2025-02-05,01:15:51 | INFO | Train Epoch: 1 [1836198/1859739.0 (99%)] Data (t): 0.519 Batch (t): 1.543, 515.429/s, 85.9049/s/gpu LR: 0.000100 Logit Scale: 99.831 Contrastive_loss: 3.5714 (3.6113) Dstill: 2.8207 (3.0718) Loss: 6.3922 (6.6830)
INFO:root:Train Epoch: 1 [1860138/1859739.0 (100%)] Data (t): 0.515 Batch (t): 1.545, 552.964/s, 92.1607/s/gpu LR: 0.000100 Logit Scale: 99.833 Contrastive_loss: 3.4410 (3.6077) Dstill: 3.1912 (3.0743) Loss: 6.6322 (6.6820)
2025-02-05,01:16:37 | INFO | Train Epoch: 1 [1860138/1859739.0 (100%)] Data (t): 0.515 Batch (t): 1.545, 552.964/s, 92.1607/s/gpu LR: 0.000100 Logit Scale: 99.833 Contrastive_loss: 3.4410 (3.6077) Dstill: 3.1912 (3.0743) Loss: 6.6322 (6.6820)
INFO:root:Train Epoch: 1 [  798/59975.0 (1%)] Data (t): 0.622 Batch (t): 1.522, 524.216/s, 87.3694/s/gpu LR: 0.000008 Logit Scale: 99.835 Contrastive_loss: 8.4280 (8.4280) Dstill: 9.9752 (9.9752) Loss: 18.403 (18.403)
2025-02-05,01:16:38 | INFO | Train Epoch: 1 [  798/59975.0 (1%)] Data (t): 0.622 Batch (t): 1.522, 524.216/s, 87.3694/s/gpu LR: 0.000008 Logit Scale: 99.835 Contrastive_loss: 8.4280 (8.4280) Dstill: 9.9752 (9.9752) Loss: 18.403 (18.403)
INFO:root:Train Epoch: 1 [40698/59975.0 (67%)] Data (t): 0.440 Batch (t): 1.361, 585.722/s, 97.6204/s/gpu LR: 0.000013 Logit Scale: 99.827 Contrastive_loss: 7.1274 (7.7777) Dstill: 6.6927 (8.3339) Loss: 13.820 (16.112)
2025-02-05,01:17:47 | INFO | Train Epoch: 1 [40698/59975.0 (67%)] Data (t): 0.440 Batch (t): 1.361, 585.722/s, 97.6204/s/gpu LR: 0.000013 Logit Scale: 99.827 Contrastive_loss: 7.1274 (7.7777) Dstill: 6.6927 (8.3339) Loss: 13.820 (16.112)
INFO:root:Train Epoch: 1 [60648/59975.0 (100%)] Data (t): 0.473 Batch (t): 1.384, 591.699/s, 98.6165/s/gpu LR: 0.000015 Logit Scale: 99.826 Contrastive_loss: 6.8288 (7.4614) Dstill: 6.3380 (7.6686) Loss: 13.167 (15.130)
2025-02-05,01:18:21 | INFO | Train Epoch: 1 [60648/59975.0 (100%)] Data (t): 0.473 Batch (t): 1.384, 591.699/s, 98.6165/s/gpu LR: 0.000015 Logit Scale: 99.826 Contrastive_loss: 6.8288 (7.4614) Dstill: 6.3380 (7.6686) Loss: 13.167 (15.130)
INFO:root:Train Epoch: 1 [  798/47810.0 (2%)] Data (t): 2.511 Batch (t): 3.390, 235.397/s, 39.2328/s/gpu LR: 0.000006 Logit Scale: 99.826 Contrastive_loss: 9.0501 (9.0501) Dstill: 11.267 (11.267) Loss: 20.317 (20.317)
2025-02-05,01:18:25 | INFO | Train Epoch: 1 [  798/47810.0 (2%)] Data (t): 2.511 Batch (t): 3.390, 235.397/s, 39.2328/s/gpu LR: 0.000006 Logit Scale: 99.826 Contrastive_loss: 9.0501 (9.0501) Dstill: 11.267 (11.267) Loss: 20.317 (20.317)
INFO:root:Train Epoch: 1 [40698/47810.0 (85%)] Data (t): 2.244 Batch (t): 3.212, 253.536/s, 42.2561/s/gpu LR: 0.000011 Logit Scale: 99.820 Contrastive_loss: 7.2263 (8.1382) Dstill: 7.5753 (9.4214) Loss: 14.802 (17.560)
2025-02-05,01:21:05 | INFO | Train Epoch: 1 [40698/47810.0 (85%)] Data (t): 2.244 Batch (t): 3.212, 253.536/s, 42.2561/s/gpu LR: 0.000011 Logit Scale: 99.820 Contrastive_loss: 7.2263 (8.1382) Dstill: 7.5753 (9.4214) Loss: 14.802 (17.560)
INFO:root:Train Epoch: 1 [47880/47810.0 (100%)] Data (t): 2.119 Batch (t): 3.071, 255.818/s, 42.6363/s/gpu LR: 0.000012 Logit Scale: 99.820 Contrastive_loss: 6.7956 (7.6907) Dstill: 7.4009 (8.7479) Loss: 14.196 (16.439)
2025-02-05,01:21:33 | INFO | Train Epoch: 1 [47880/47810.0 (100%)] Data (t): 2.119 Batch (t): 3.071, 255.818/s, 42.6363/s/gpu LR: 0.000012 Logit Scale: 99.820 Contrastive_loss: 6.7956 (7.6907) Dstill: 7.4009 (8.7479) Loss: 14.196 (16.439)
INFO:root:Train Epoch: 1 [ 798/7500.0 (10%)] Data (t): 26.688 Batch (t): 32.339, 24.6763/s, 4.11272/s/gpu LR: 0.000001 Logit Scale: 99.820 Contrastive_loss: 168.93 (168.93) Dstill: 16.476 (16.476) Loss: 185.41 (185.41)
2025-02-05,01:22:05 | INFO | Train Epoch: 1 [ 798/7500.0 (10%)] Data (t): 26.688 Batch (t): 32.339, 24.6763/s, 4.11272/s/gpu LR: 0.000001 Logit Scale: 99.820 Contrastive_loss: 168.93 (168.93) Dstill: 16.476 (16.476) Loss: 185.41 (185.41)
INFO:root:Train Epoch: 1 [7980/7500.0 (100%)] Data (t): 27.646 Batch (t): 32.960, 18.6478/s, 3.10796/s/gpu LR: 0.000002 Logit Scale: 99.819 Contrastive_loss: 9.3871 (89.159) Dstill: 16.478 (16.477) Loss: 25.865 (105.64)
2025-02-05,01:27:02 | INFO | Train Epoch: 1 [7980/7500.0 (100%)] Data (t): 27.646 Batch (t): 32.960, 18.6478/s, 3.10796/s/gpu LR: 0.000002 Logit Scale: 99.819 Contrastive_loss: 9.3871 (89.159) Dstill: 16.478 (16.477) Loss: 25.865 (105.64)
INFO:root:Start epoch 2
2025-02-05,01:27:03 | INFO | Start epoch 2
INFO:root:Train Epoch: 2 [    798/1859739.0 (0%)] Data (t): 0.508 Batch (t): 1.489, 535.994/s, 89.3324/s/gpu LR: 0.000100 Logit Scale: 99.819 Contrastive_loss: 5.3841 (5.3841) Dstill: 5.4827 (5.4827) Loss: 10.867 (10.867)
2025-02-05,01:27:04 | INFO | Train Epoch: 2 [    798/1859739.0 (0%)] Data (t): 0.508 Batch (t): 1.489, 535.994/s, 89.3324/s/gpu LR: 0.000100 Logit Scale: 99.819 Contrastive_loss: 5.3841 (5.3841) Dstill: 5.4827 (5.4827) Loss: 10.867 (10.867)
INFO:root:Train Epoch: 2 [  40698/1859739.0 (2%)] Data (t): 0.408 Batch (t): 1.333, 626.312/s, 104.385/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.8220 (4.6031) Dstill: 7.7950 (6.6389) Loss: 11.617 (11.242)
2025-02-05,01:28:11 | INFO | Train Epoch: 2 [  40698/1859739.0 (2%)] Data (t): 0.408 Batch (t): 1.333, 626.312/s, 104.385/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.8220 (4.6031) Dstill: 7.7950 (6.6389) Loss: 11.617 (11.242)
INFO:root:Train Epoch: 2 [  80598/1859739.0 (4%)] Data (t): 0.420 Batch (t): 1.334, 583.189/s, 97.1982/s/gpu LR: 0.000100 Logit Scale: 99.738 Contrastive_loss: 3.5135 (4.2399) Dstill: 6.6332 (6.6370) Loss: 10.147 (10.877)
2025-02-05,01:29:18 | INFO | Train Epoch: 2 [  80598/1859739.0 (4%)] Data (t): 0.420 Batch (t): 1.334, 583.189/s, 97.1982/s/gpu LR: 0.000100 Logit Scale: 99.738 Contrastive_loss: 3.5135 (4.2399) Dstill: 6.6332 (6.6370) Loss: 10.147 (10.877)
INFO:root:Train Epoch: 2 [ 120498/1859739.0 (6%)] Data (t): 0.427 Batch (t): 1.330, 638.584/s, 106.431/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.4621 (4.0454) Dstill: 5.7970 (6.4270) Loss: 9.2590 (10.472)
2025-02-05,01:30:24 | INFO | Train Epoch: 2 [ 120498/1859739.0 (6%)] Data (t): 0.427 Batch (t): 1.330, 638.584/s, 106.431/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.4621 (4.0454) Dstill: 5.7970 (6.4270) Loss: 9.2590 (10.472)
INFO:root:Train Epoch: 2 [ 160398/1859739.0 (9%)] Data (t): 0.453 Batch (t): 1.390, 526.555/s, 87.7591/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.1235 (3.8610) Dstill: 5.0284 (6.1472) Loss: 8.1519 (10.008)
2025-02-05,01:31:34 | INFO | Train Epoch: 2 [ 160398/1859739.0 (9%)] Data (t): 0.453 Batch (t): 1.390, 526.555/s, 87.7591/s/gpu LR: 0.000100 Logit Scale: 99.739 Contrastive_loss: 3.1235 (3.8610) Dstill: 5.0284 (6.1472) Loss: 8.1519 (10.008)
INFO:root:Train Epoch: 2 [ 200298/1859739.0 (11%)] Data (t): 0.507 Batch (t): 1.438, 539.802/s, 89.9670/s/gpu LR: 0.000100 Logit Scale: 99.740 Contrastive_loss: 3.5842 (3.8149) Dstill: 4.3887 (5.8542) Loss: 7.9729 (9.6691)
2025-02-05,01:32:45 | INFO | Train Epoch: 2 [ 200298/1859739.0 (11%)] Data (t): 0.507 Batch (t): 1.438, 539.802/s, 89.9670/s/gpu LR: 0.000100 Logit Scale: 99.740 Contrastive_loss: 3.5842 (3.8149) Dstill: 4.3887 (5.8542) Loss: 7.9729 (9.6691)
INFO:root:Train Epoch: 2 [ 240198/1859739.0 (13%)] Data (t): 0.530 Batch (t): 1.456, 580.985/s, 96.8308/s/gpu LR: 0.000100 Logit Scale: 99.742 Contrastive_loss: 3.5101 (3.7714) Dstill: 4.2834 (5.6298) Loss: 7.7935 (9.4011)
2025-02-05,01:33:58 | INFO | Train Epoch: 2 [ 240198/1859739.0 (13%)] Data (t): 0.530 Batch (t): 1.456, 580.985/s, 96.8308/s/gpu LR: 0.000100 Logit Scale: 99.742 Contrastive_loss: 3.5101 (3.7714) Dstill: 4.2834 (5.6298) Loss: 7.7935 (9.4011)
INFO:root:Train Epoch: 2 [ 280098/1859739.0 (15%)] Data (t): 0.538 Batch (t): 1.475, 515.162/s, 85.8603/s/gpu LR: 0.000100 Logit Scale: 99.745 Contrastive_loss: 3.1604 (3.6950) Dstill: 3.9533 (5.4202) Loss: 7.1137 (9.1152)
2025-02-05,01:35:12 | INFO | Train Epoch: 2 [ 280098/1859739.0 (15%)] Data (t): 0.538 Batch (t): 1.475, 515.162/s, 85.8603/s/gpu LR: 0.000100 Logit Scale: 99.745 Contrastive_loss: 3.1604 (3.6950) Dstill: 3.9533 (5.4202) Loss: 7.1137 (9.1152)
INFO:root:Train Epoch: 2 [ 319998/1859739.0 (17%)] Data (t): 0.592 Batch (t): 1.505, 524.919/s, 87.4865/s/gpu LR: 0.000100 Logit Scale: 99.746 Contrastive_loss: 3.2707 (3.6478) Dstill: 4.0631 (5.2694) Loss: 7.3337 (8.9173)
2025-02-05,01:36:27 | INFO | Train Epoch: 2 [ 319998/1859739.0 (17%)] Data (t): 0.592 Batch (t): 1.505, 524.919/s, 87.4865/s/gpu LR: 0.000100 Logit Scale: 99.746 Contrastive_loss: 3.2707 (3.6478) Dstill: 4.0631 (5.2694) Loss: 7.3337 (8.9173)
INFO:root:Train Epoch: 2 [ 359898/1859739.0 (19%)] Data (t): 0.560 Batch (t): 1.491, 550.857/s, 91.8095/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.3790 (3.6210) Dstill: 3.6459 (5.1071) Loss: 7.0249 (8.7280)
2025-02-05,01:37:42 | INFO | Train Epoch: 2 [ 359898/1859739.0 (19%)] Data (t): 0.560 Batch (t): 1.491, 550.857/s, 91.8095/s/gpu LR: 0.000100 Logit Scale: 99.755 Contrastive_loss: 3.3790 (3.6210) Dstill: 3.6459 (5.1071) Loss: 7.0249 (8.7280)
INFO:root:Train Epoch: 2 [ 399798/1859739.0 (21%)] Data (t): 0.593 Batch (t): 1.517, 534.514/s, 89.0856/s/gpu LR: 0.000100 Logit Scale: 99.760 Contrastive_loss: 3.4230 (3.6030) Dstill: 3.3171 (4.9443) Loss: 6.7401 (8.5473)
2025-02-05,01:38:58 | INFO | Train Epoch: 2 [ 399798/1859739.0 (21%)] Data (t): 0.593 Batch (t): 1.517, 534.514/s, 89.0856/s/gpu LR: 0.000100 Logit Scale: 99.760 Contrastive_loss: 3.4230 (3.6030) Dstill: 3.3171 (4.9443) Loss: 6.7401 (8.5473)
INFO:root:Train Epoch: 2 [ 439698/1859739.0 (24%)] Data (t): 0.579 Batch (t): 1.537, 542.035/s, 90.3391/s/gpu LR: 0.000100 Logit Scale: 99.762 Contrastive_loss: 3.3384 (3.5809) Dstill: 3.1530 (4.7951) Loss: 6.4914 (8.3760)
2025-02-05,01:40:14 | INFO | Train Epoch: 2 [ 439698/1859739.0 (24%)] Data (t): 0.579 Batch (t): 1.537, 542.035/s, 90.3391/s/gpu LR: 0.000100 Logit Scale: 99.762 Contrastive_loss: 3.3384 (3.5809) Dstill: 3.1530 (4.7951) Loss: 6.4914 (8.3760)
INFO:root:Train Epoch: 2 [ 479598/1859739.0 (26%)] Data (t): 0.565 Batch (t): 1.507, 565.508/s, 94.2513/s/gpu LR: 0.000100 Logit Scale: 99.764 Contrastive_loss: 3.2883 (3.5584) Dstill: 2.9339 (4.6519) Loss: 6.2222 (8.2103)
2025-02-05,01:41:30 | INFO | Train Epoch: 2 [ 479598/1859739.0 (26%)] Data (t): 0.565 Batch (t): 1.507, 565.508/s, 94.2513/s/gpu LR: 0.000100 Logit Scale: 99.764 Contrastive_loss: 3.2883 (3.5584) Dstill: 2.9339 (4.6519) Loss: 6.2222 (8.2103)
INFO:root:Train Epoch: 2 [ 519498/1859739.0 (28%)] Data (t): 0.545 Batch (t): 1.489, 490.394/s, 81.7324/s/gpu LR: 0.000100 Logit Scale: 99.778 Contrastive_loss: 3.3405 (3.5428) Dstill: 2.8734 (4.5249) Loss: 6.2139 (8.0677)
2025-02-05,01:42:44 | INFO | Train Epoch: 2 [ 519498/1859739.0 (28%)] Data (t): 0.545 Batch (t): 1.489, 490.394/s, 81.7324/s/gpu LR: 0.000100 Logit Scale: 99.778 Contrastive_loss: 3.3405 (3.5428) Dstill: 2.8734 (4.5249) Loss: 6.2139 (8.0677)
INFO:root:Train Epoch: 2 [ 559398/1859739.0 (30%)] Data (t): 0.551 Batch (t): 1.491, 487.435/s, 81.2391/s/gpu LR: 0.000100 Logit Scale: 99.778 Contrastive_loss: 3.4818 (3.5388) Dstill: 2.8720 (4.4147) Loss: 6.3538 (7.9534)
2025-02-05,01:43:59 | INFO | Train Epoch: 2 [ 559398/1859739.0 (30%)] Data (t): 0.551 Batch (t): 1.491, 487.435/s, 81.2391/s/gpu LR: 0.000100 Logit Scale: 99.778 Contrastive_loss: 3.4818 (3.5388) Dstill: 2.8720 (4.4147) Loss: 6.3538 (7.9534)
INFO:root:Train Epoch: 2 [ 599298/1859739.0 (32%)] Data (t): 0.623 Batch (t): 1.567, 501.443/s, 83.5738/s/gpu LR: 0.000100 Logit Scale: 99.784 Contrastive_loss: 3.4596 (3.5338) Dstill: 2.9384 (4.3224) Loss: 6.3980 (7.8562)
2025-02-05,01:45:17 | INFO | Train Epoch: 2 [ 599298/1859739.0 (32%)] Data (t): 0.623 Batch (t): 1.567, 501.443/s, 83.5738/s/gpu LR: 0.000100 Logit Scale: 99.784 Contrastive_loss: 3.4596 (3.5338) Dstill: 2.9384 (4.3224) Loss: 6.3980 (7.8562)
INFO:root:Train Epoch: 2 [ 639198/1859739.0 (34%)] Data (t): 0.605 Batch (t): 1.538, 475.525/s, 79.2542/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.5182 (3.5329) Dstill: 2.9033 (4.2389) Loss: 6.4215 (7.7718)
2025-02-05,01:46:34 | INFO | Train Epoch: 2 [ 639198/1859739.0 (34%)] Data (t): 0.605 Batch (t): 1.538, 475.525/s, 79.2542/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.5182 (3.5329) Dstill: 2.9033 (4.2389) Loss: 6.4215 (7.7718)
INFO:root:Train Epoch: 2 [ 679098/1859739.0 (37%)] Data (t): 0.601 Batch (t): 1.536, 476.889/s, 79.4815/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.3149 (3.5208) Dstill: 2.9080 (4.1650) Loss: 6.2229 (7.6858)
2025-02-05,01:47:51 | INFO | Train Epoch: 2 [ 679098/1859739.0 (37%)] Data (t): 0.601 Batch (t): 1.536, 476.889/s, 79.4815/s/gpu LR: 0.000100 Logit Scale: 99.788 Contrastive_loss: 3.3149 (3.5208) Dstill: 2.9080 (4.1650) Loss: 6.2229 (7.6858)
INFO:root:Train Epoch: 2 [ 718998/1859739.0 (39%)] Data (t): 0.597 Batch (t): 1.556, 445.248/s, 74.2079/s/gpu LR: 0.000100 Logit Scale: 99.800 Contrastive_loss: 3.5121 (3.5203) Dstill: 2.8675 (4.0967) Loss: 6.3796 (7.6170)
2025-02-05,01:49:09 | INFO | Train Epoch: 2 [ 718998/1859739.0 (39%)] Data (t): 0.597 Batch (t): 1.556, 445.248/s, 74.2079/s/gpu LR: 0.000100 Logit Scale: 99.800 Contrastive_loss: 3.5121 (3.5203) Dstill: 2.8675 (4.0967) Loss: 6.3796 (7.6170)
INFO:root:Train Epoch: 2 [ 758898/1859739.0 (41%)] Data (t): 0.576 Batch (t): 1.556, 514.718/s, 85.7864/s/gpu LR: 0.000100 Logit Scale: 99.800 Contrastive_loss: 3.5313 (3.5209) Dstill: 2.9702 (4.0404) Loss: 6.5015 (7.5613)
2025-02-05,01:50:26 | INFO | Train Epoch: 2 [ 758898/1859739.0 (41%)] Data (t): 0.576 Batch (t): 1.556, 514.718/s, 85.7864/s/gpu LR: 0.000100 Logit Scale: 99.800 Contrastive_loss: 3.5313 (3.5209) Dstill: 2.9702 (4.0404) Loss: 6.5015 (7.5613)
INFO:root:Train Epoch: 2 [ 798798/1859739.0 (43%)] Data (t): 0.614 Batch (t): 1.583, 510.324/s, 85.0540/s/gpu LR: 0.000100 Logit Scale: 99.809 Contrastive_loss: 3.0217 (3.4971) Dstill: 2.8410 (3.9833) Loss: 5.8628 (7.4804)
2025-02-05,01:51:46 | INFO | Train Epoch: 2 [ 798798/1859739.0 (43%)] Data (t): 0.614 Batch (t): 1.583, 510.324/s, 85.0540/s/gpu LR: 0.000100 Logit Scale: 99.809 Contrastive_loss: 3.0217 (3.4971) Dstill: 2.8410 (3.9833) Loss: 5.8628 (7.4804)
INFO:root:Train Epoch: 2 [ 838698/1859739.0 (45%)] Data (t): 0.618 Batch (t): 1.569, 510.737/s, 85.1228/s/gpu LR: 0.000100 Logit Scale: 99.811 Contrastive_loss: 3.5543 (3.4997) Dstill: 2.8300 (3.9308) Loss: 6.3843 (7.4305)
2025-02-05,01:53:04 | INFO | Train Epoch: 2 [ 838698/1859739.0 (45%)] Data (t): 0.618 Batch (t): 1.569, 510.737/s, 85.1228/s/gpu LR: 0.000100 Logit Scale: 99.811 Contrastive_loss: 3.5543 (3.4997) Dstill: 2.8300 (3.9308) Loss: 6.3843 (7.4305)
INFO:root:Train Epoch: 2 [ 878598/1859739.0 (47%)] Data (t): 0.587 Batch (t): 1.555, 577.108/s, 96.1846/s/gpu LR: 0.000100 Logit Scale: 99.821 Contrastive_loss: 3.0165 (3.4787) Dstill: 2.9946 (3.8901) Loss: 6.0111 (7.3688)
2025-02-05,01:54:22 | INFO | Train Epoch: 2 [ 878598/1859739.0 (47%)] Data (t): 0.587 Batch (t): 1.555, 577.108/s, 96.1846/s/gpu LR: 0.000100 Logit Scale: 99.821 Contrastive_loss: 3.0165 (3.4787) Dstill: 2.9946 (3.8901) Loss: 6.0111 (7.3688)
INFO:root:Train Epoch: 2 [ 918498/1859739.0 (49%)] Data (t): 0.566 Batch (t): 1.507, 517.689/s, 86.2816/s/gpu LR: 0.000100 Logit Scale: 99.825 Contrastive_loss: 3.2771 (3.4703) Dstill: 2.6514 (3.8385) Loss: 5.9286 (7.3088)
2025-02-05,01:55:37 | INFO | Train Epoch: 2 [ 918498/1859739.0 (49%)] Data (t): 0.566 Batch (t): 1.507, 517.689/s, 86.2816/s/gpu LR: 0.000100 Logit Scale: 99.825 Contrastive_loss: 3.2771 (3.4703) Dstill: 2.6514 (3.8385) Loss: 5.9286 (7.3088)
INFO:root:Train Epoch: 2 [ 958398/1859739.0 (52%)] Data (t): 0.561 Batch (t): 1.562, 488.565/s, 81.4275/s/gpu LR: 0.000100 Logit Scale: 99.832 Contrastive_loss: 3.4250 (3.4685) Dstill: 2.9228 (3.8019) Loss: 6.3478 (7.2704)
2025-02-05,01:56:55 | INFO | Train Epoch: 2 [ 958398/1859739.0 (52%)] Data (t): 0.561 Batch (t): 1.562, 488.565/s, 81.4275/s/gpu LR: 0.000100 Logit Scale: 99.832 Contrastive_loss: 3.4250 (3.4685) Dstill: 2.9228 (3.8019) Loss: 6.3478 (7.2704)
INFO:root:Train Epoch: 2 [ 998298/1859739.0 (54%)] Data (t): 0.592 Batch (t): 1.557, 561.244/s, 93.5406/s/gpu LR: 0.000100 Logit Scale: 99.836 Contrastive_loss: 3.5891 (3.4731) Dstill: 2.8176 (3.7640) Loss: 6.4066 (7.2372)
2025-02-05,01:58:13 | INFO | Train Epoch: 2 [ 998298/1859739.0 (54%)] Data (t): 0.592 Batch (t): 1.557, 561.244/s, 93.5406/s/gpu LR: 0.000100 Logit Scale: 99.836 Contrastive_loss: 3.5891 (3.4731) Dstill: 2.8176 (3.7640) Loss: 6.4066 (7.2372)
INFO:root:Train Epoch: 2 [1038198/1859739.0 (56%)] Data (t): 0.568 Batch (t): 1.488, 603.719/s, 100.620/s/gpu LR: 0.000100 Logit Scale: 99.849 Contrastive_loss: 3.1688 (3.4619) Dstill: 2.8080 (3.7286) Loss: 5.9768 (7.1905)
2025-02-05,01:59:27 | INFO | Train Epoch: 2 [1038198/1859739.0 (56%)] Data (t): 0.568 Batch (t): 1.488, 603.719/s, 100.620/s/gpu LR: 0.000100 Logit Scale: 99.849 Contrastive_loss: 3.1688 (3.4619) Dstill: 2.8080 (3.7286) Loss: 5.9768 (7.1905)
INFO:root:Train Epoch: 2 [1078098/1859739.0 (58%)] Data (t): 0.412 Batch (t): 1.317, 622.670/s, 103.778/s/gpu LR: 0.000100 Logit Scale: 99.861 Contrastive_loss: 3.1439 (3.4505) Dstill: 2.9535 (3.7009) Loss: 6.0975 (7.1514)
2025-02-05,02:00:33 | INFO | Train Epoch: 2 [1078098/1859739.0 (58%)] Data (t): 0.412 Batch (t): 1.317, 622.670/s, 103.778/s/gpu LR: 0.000100 Logit Scale: 99.861 Contrastive_loss: 3.1439 (3.4505) Dstill: 2.9535 (3.7009) Loss: 6.0975 (7.1514)
INFO:root:Train Epoch: 2 [1117998/1859739.0 (60%)] Data (t): 0.410 Batch (t): 1.305, 608.332/s, 101.389/s/gpu LR: 0.000100 Logit Scale: 99.869 Contrastive_loss: 3.4681 (3.4511) Dstill: 2.7817 (3.6692) Loss: 6.2497 (7.1203)
2025-02-05,02:01:38 | INFO | Train Epoch: 2 [1117998/1859739.0 (60%)] Data (t): 0.410 Batch (t): 1.305, 608.332/s, 101.389/s/gpu LR: 0.000100 Logit Scale: 99.869 Contrastive_loss: 3.4681 (3.4511) Dstill: 2.7817 (3.6692) Loss: 6.2497 (7.1203)
INFO:root:Train Epoch: 2 [1157898/1859739.0 (62%)] Data (t): 0.404 Batch (t): 1.310, 637.796/s, 106.299/s/gpu LR: 0.000100 Logit Scale: 99.874 Contrastive_loss: 3.2727 (3.4452) Dstill: 2.6026 (3.6337) Loss: 5.8753 (7.0788)
2025-02-05,02:02:44 | INFO | Train Epoch: 2 [1157898/1859739.0 (62%)] Data (t): 0.404 Batch (t): 1.310, 637.796/s, 106.299/s/gpu LR: 0.000100 Logit Scale: 99.874 Contrastive_loss: 3.2727 (3.4452) Dstill: 2.6026 (3.6337) Loss: 5.8753 (7.0788)
INFO:root:Train Epoch: 2 [1197798/1859739.0 (64%)] Data (t): 0.407 Batch (t): 1.304, 660.863/s, 110.144/s/gpu LR: 0.000100 Logit Scale: 99.871 Contrastive_loss: 3.7735 (3.4558) Dstill: 2.7665 (3.6057) Loss: 6.5400 (7.0615)
2025-02-05,02:03:49 | INFO | Train Epoch: 2 [1197798/1859739.0 (64%)] Data (t): 0.407 Batch (t): 1.304, 660.863/s, 110.144/s/gpu LR: 0.000100 Logit Scale: 99.871 Contrastive_loss: 3.7735 (3.4558) Dstill: 2.7665 (3.6057) Loss: 6.5400 (7.0615)
INFO:root:Train Epoch: 2 [1237698/1859739.0 (67%)] Data (t): 0.377 Batch (t): 1.279, 693.608/s, 115.601/s/gpu LR: 0.000100 Logit Scale: 99.879 Contrastive_loss: 3.3185 (3.4515) Dstill: 2.8174 (3.5811) Loss: 6.1359 (7.0325)
2025-02-05,02:04:53 | INFO | Train Epoch: 2 [1237698/1859739.0 (67%)] Data (t): 0.377 Batch (t): 1.279, 693.608/s, 115.601/s/gpu LR: 0.000100 Logit Scale: 99.879 Contrastive_loss: 3.3185 (3.4515) Dstill: 2.8174 (3.5811) Loss: 6.1359 (7.0325)
INFO:root:Train Epoch: 2 [1277598/1859739.0 (69%)] Data (t): 0.361 Batch (t): 1.292, 631.843/s, 105.307/s/gpu LR: 0.000100 Logit Scale: 99.886 Contrastive_loss: 3.5206 (3.4536) Dstill: 2.8083 (3.5577) Loss: 6.3289 (7.0112)
2025-02-05,02:05:58 | INFO | Train Epoch: 2 [1277598/1859739.0 (69%)] Data (t): 0.361 Batch (t): 1.292, 631.843/s, 105.307/s/gpu LR: 0.000100 Logit Scale: 99.886 Contrastive_loss: 3.5206 (3.4536) Dstill: 2.8083 (3.5577) Loss: 6.3289 (7.0112)
INFO:root:Train Epoch: 2 [1317498/1859739.0 (71%)] Data (t): 0.375 Batch (t): 1.303, 672.150/s, 112.025/s/gpu LR: 0.000100 Logit Scale: 99.887 Contrastive_loss: 3.1970 (3.4460) Dstill: 2.7432 (3.5337) Loss: 5.9402 (6.9797)
2025-02-05,02:07:03 | INFO | Train Epoch: 2 [1317498/1859739.0 (71%)] Data (t): 0.375 Batch (t): 1.303, 672.150/s, 112.025/s/gpu LR: 0.000100 Logit Scale: 99.887 Contrastive_loss: 3.1970 (3.4460) Dstill: 2.7432 (3.5337) Loss: 5.9402 (6.9797)
INFO:root:Train Epoch: 2 [1357398/1859739.0 (73%)] Data (t): 0.366 Batch (t): 1.284, 515.927/s, 85.9878/s/gpu LR: 0.000100 Logit Scale: 99.903 Contrastive_loss: 3.3047 (3.4420) Dstill: 2.7801 (3.5122) Loss: 6.0848 (6.9541)
2025-02-05,02:08:07 | INFO | Train Epoch: 2 [1357398/1859739.0 (73%)] Data (t): 0.366 Batch (t): 1.284, 515.927/s, 85.9878/s/gpu LR: 0.000100 Logit Scale: 99.903 Contrastive_loss: 3.3047 (3.4420) Dstill: 2.7801 (3.5122) Loss: 6.0848 (6.9541)
INFO:root:Train Epoch: 2 [1397298/1859739.0 (75%)] Data (t): 0.550 Batch (t): 1.615, 483.449/s, 80.5749/s/gpu LR: 0.000100 Logit Scale: 99.916 Contrastive_loss: 3.4314 (3.4417) Dstill: 2.6613 (3.4885) Loss: 6.0927 (6.9302)
2025-02-05,02:09:28 | INFO | Train Epoch: 2 [1397298/1859739.0 (75%)] Data (t): 0.550 Batch (t): 1.615, 483.449/s, 80.5749/s/gpu LR: 0.000100 Logit Scale: 99.916 Contrastive_loss: 3.4314 (3.4417) Dstill: 2.6613 (3.4885) Loss: 6.0927 (6.9302)
INFO:root:Train Epoch: 2 [1437198/1859739.0 (77%)] Data (t): 0.563 Batch (t): 1.555, 477.486/s, 79.5810/s/gpu LR: 0.000100 Logit Scale: 99.914 Contrastive_loss: 3.3714 (3.4398) Dstill: 2.7062 (3.4674) Loss: 6.0776 (6.9072)
2025-02-05,02:10:45 | INFO | Train Epoch: 2 [1437198/1859739.0 (77%)] Data (t): 0.563 Batch (t): 1.555, 477.486/s, 79.5810/s/gpu LR: 0.000100 Logit Scale: 99.914 Contrastive_loss: 3.3714 (3.4398) Dstill: 2.7062 (3.4674) Loss: 6.0776 (6.9072)
INFO:root:Train Epoch: 2 [1477098/1859739.0 (79%)] Data (t): 0.539 Batch (t): 1.616, 495.731/s, 82.6218/s/gpu LR: 0.000100 Logit Scale: 99.927 Contrastive_loss: 3.0222 (3.4288) Dstill: 2.6333 (3.4454) Loss: 5.6555 (6.8742)
2025-02-05,02:12:06 | INFO | Train Epoch: 2 [1477098/1859739.0 (79%)] Data (t): 0.539 Batch (t): 1.616, 495.731/s, 82.6218/s/gpu LR: 0.000100 Logit Scale: 99.927 Contrastive_loss: 3.0222 (3.4288) Dstill: 2.6333 (3.4454) Loss: 5.6555 (6.8742)
INFO:root:Train Epoch: 2 [1516998/1859739.0 (82%)] Data (t): 0.559 Batch (t): 1.635, 486.505/s, 81.0841/s/gpu LR: 0.000099 Logit Scale: 99.938 Contrastive_loss: 3.2980 (3.4254) Dstill: 2.7573 (3.4278) Loss: 6.0554 (6.8532)
2025-02-05,02:13:28 | INFO | Train Epoch: 2 [1516998/1859739.0 (82%)] Data (t): 0.559 Batch (t): 1.635, 486.505/s, 81.0841/s/gpu LR: 0.000099 Logit Scale: 99.938 Contrastive_loss: 3.2980 (3.4254) Dstill: 2.7573 (3.4278) Loss: 6.0554 (6.8532)
INFO:root:Train Epoch: 2 [1556898/1859739.0 (84%)] Data (t): 0.535 Batch (t): 1.613, 497.976/s, 82.9960/s/gpu LR: 0.000099 Logit Scale: 99.946 Contrastive_loss: 3.2626 (3.4214) Dstill: 2.7592 (3.4111) Loss: 6.0218 (6.8325)
2025-02-05,02:14:49 | INFO | Train Epoch: 2 [1556898/1859739.0 (84%)] Data (t): 0.535 Batch (t): 1.613, 497.976/s, 82.9960/s/gpu LR: 0.000099 Logit Scale: 99.946 Contrastive_loss: 3.2626 (3.4214) Dstill: 2.7592 (3.4111) Loss: 6.0218 (6.8325)
INFO:root:Train Epoch: 2 [1596798/1859739.0 (86%)] Data (t): 0.540 Batch (t): 1.615, 567.893/s, 94.6488/s/gpu LR: 0.000099 Logit Scale: 99.956 Contrastive_loss: 3.2940 (3.4183) Dstill: 2.5626 (3.3904) Loss: 5.8567 (6.8087)
2025-02-05,02:16:09 | INFO | Train Epoch: 2 [1596798/1859739.0 (86%)] Data (t): 0.540 Batch (t): 1.615, 567.893/s, 94.6488/s/gpu LR: 0.000099 Logit Scale: 99.956 Contrastive_loss: 3.2940 (3.4183) Dstill: 2.5626 (3.3904) Loss: 5.8567 (6.8087)
INFO:root:Train Epoch: 2 [1636698/1859739.0 (88%)] Data (t): 0.549 Batch (t): 1.604, 496.834/s, 82.8057/s/gpu LR: 0.000099 Logit Scale: 99.966 Contrastive_loss: 3.4114 (3.4181) Dstill: 2.7072 (3.3741) Loss: 6.1186 (6.7922)
2025-02-05,02:17:30 | INFO | Train Epoch: 2 [1636698/1859739.0 (88%)] Data (t): 0.549 Batch (t): 1.604, 496.834/s, 82.8057/s/gpu LR: 0.000099 Logit Scale: 99.966 Contrastive_loss: 3.4114 (3.4181) Dstill: 2.7072 (3.3741) Loss: 6.1186 (6.7922)
INFO:root:Train Epoch: 2 [1676598/1859739.0 (90%)] Data (t): 0.489 Batch (t): 1.569, 492.048/s, 82.0081/s/gpu LR: 0.000099 Logit Scale: 99.969 Contrastive_loss: 3.5838 (3.4220) Dstill: 2.7881 (3.3605) Loss: 6.3719 (6.7825)
2025-02-05,02:18:48 | INFO | Train Epoch: 2 [1676598/1859739.0 (90%)] Data (t): 0.489 Batch (t): 1.569, 492.048/s, 82.0081/s/gpu LR: 0.000099 Logit Scale: 99.969 Contrastive_loss: 3.5838 (3.4220) Dstill: 2.7881 (3.3605) Loss: 6.3719 (6.7825)
INFO:root:Train Epoch: 2 [1716498/1859739.0 (92%)] Data (t): 0.462 Batch (t): 1.515, 547.590/s, 91.2650/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.3732 (3.4208) Dstill: 2.5513 (3.3421) Loss: 5.9245 (6.7630)
2025-02-05,02:20:04 | INFO | Train Epoch: 2 [1716498/1859739.0 (92%)] Data (t): 0.462 Batch (t): 1.515, 547.590/s, 91.2650/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.3732 (3.4208) Dstill: 2.5513 (3.3421) Loss: 5.9245 (6.7630)
INFO:root:Train Epoch: 2 [1756398/1859739.0 (94%)] Data (t): 0.445 Batch (t): 1.442, 566.813/s, 94.4688/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.5210 (3.4231) Dstill: 2.6656 (3.3271) Loss: 6.1866 (6.7501)
2025-02-05,02:21:16 | INFO | Train Epoch: 2 [1756398/1859739.0 (94%)] Data (t): 0.445 Batch (t): 1.442, 566.813/s, 94.4688/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.5210 (3.4231) Dstill: 2.6656 (3.3271) Loss: 6.1866 (6.7501)
INFO:root:Train Epoch: 2 [1796298/1859739.0 (97%)] Data (t): 0.423 Batch (t): 1.468, 518.586/s, 86.4309/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.6627 (3.4283) Dstill: 2.9073 (3.3179) Loss: 6.5701 (6.7462)
2025-02-05,02:22:29 | INFO | Train Epoch: 2 [1796298/1859739.0 (97%)] Data (t): 0.423 Batch (t): 1.468, 518.586/s, 86.4309/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.6627 (3.4283) Dstill: 2.9073 (3.3179) Loss: 6.5701 (6.7462)
INFO:root:Train Epoch: 2 [1836198/1859739.0 (99%)] Data (t): 0.484 Batch (t): 1.540, 509.350/s, 84.8916/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.5369 (3.4306) Dstill: 3.1452 (3.3143) Loss: 6.6820 (6.7449)
2025-02-05,02:23:46 | INFO | Train Epoch: 2 [1836198/1859739.0 (99%)] Data (t): 0.484 Batch (t): 1.540, 509.350/s, 84.8916/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.5369 (3.4306) Dstill: 3.1452 (3.3143) Loss: 6.6820 (6.7449)
INFO:root:Train Epoch: 2 [1860138/1859739.0 (100%)] Data (t): 0.480 Batch (t): 1.528, 537.403/s, 89.5672/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.2865 (3.4276) Dstill: 3.2712 (3.3134) Loss: 6.5577 (6.7410)
2025-02-05,02:24:32 | INFO | Train Epoch: 2 [1860138/1859739.0 (100%)] Data (t): 0.480 Batch (t): 1.528, 537.403/s, 89.5672/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.2865 (3.4276) Dstill: 3.2712 (3.3134) Loss: 6.5577 (6.7410)
INFO:root:Train Epoch: 2 [  798/59975.0 (1%)] Data (t): 0.402 Batch (t): 1.338, 596.373/s, 99.3955/s/gpu LR: 0.000015 Logit Scale: 100.000 Contrastive_loss: 8.7939 (8.7939) Dstill: 9.8165 (9.8165) Loss: 18.610 (18.610)
2025-02-05,02:24:34 | INFO | Train Epoch: 2 [  798/59975.0 (1%)] Data (t): 0.402 Batch (t): 1.338, 596.373/s, 99.3955/s/gpu LR: 0.000015 Logit Scale: 100.000 Contrastive_loss: 8.7939 (8.7939) Dstill: 9.8165 (9.8165) Loss: 18.610 (18.610)
INFO:root:Train Epoch: 2 [40698/59975.0 (67%)] Data (t): 0.430 Batch (t): 1.366, 585.513/s, 97.5854/s/gpu LR: 0.000020 Logit Scale: 99.993 Contrastive_loss: 7.0303 (7.9121) Dstill: 6.5584 (8.1874) Loss: 13.589 (16.100)
2025-02-05,02:25:42 | INFO | Train Epoch: 2 [40698/59975.0 (67%)] Data (t): 0.430 Batch (t): 1.366, 585.513/s, 97.5854/s/gpu LR: 0.000020 Logit Scale: 99.993 Contrastive_loss: 7.0303 (7.9121) Dstill: 6.5584 (8.1874) Loss: 13.589 (16.100)
INFO:root:Train Epoch: 2 [60648/59975.0 (100%)] Data (t): 0.426 Batch (t): 1.381, 597.006/s, 99.5009/s/gpu LR: 0.000023 Logit Scale: 99.992 Contrastive_loss: 6.7828 (7.5357) Dstill: 6.2401 (7.5383) Loss: 13.023 (15.074)
2025-02-05,02:26:16 | INFO | Train Epoch: 2 [60648/59975.0 (100%)] Data (t): 0.426 Batch (t): 1.381, 597.006/s, 99.5009/s/gpu LR: 0.000023 Logit Scale: 99.992 Contrastive_loss: 6.7828 (7.5357) Dstill: 6.2401 (7.5383) Loss: 13.023 (15.074)
INFO:root:Train Epoch: 2 [  798/47810.0 (2%)] Data (t): 2.011 Batch (t): 2.913, 273.907/s, 45.6511/s/gpu LR: 0.000012 Logit Scale: 99.992 Contrastive_loss: 8.1916 (8.1916) Dstill: 11.287 (11.287) Loss: 19.479 (19.479)
2025-02-05,02:26:19 | INFO | Train Epoch: 2 [  798/47810.0 (2%)] Data (t): 2.011 Batch (t): 2.913, 273.907/s, 45.6511/s/gpu LR: 0.000012 Logit Scale: 99.992 Contrastive_loss: 8.1916 (8.1916) Dstill: 11.287 (11.287) Loss: 19.479 (19.479)
INFO:root:Train Epoch: 2 [40698/47810.0 (85%)] Data (t): 1.916 Batch (t): 2.904, 254.621/s, 42.4368/s/gpu LR: 0.000017 Logit Scale: 99.989 Contrastive_loss: 7.0723 (7.6320) Dstill: 7.2038 (9.2456) Loss: 14.276 (16.878)
2025-02-05,02:28:45 | INFO | Train Epoch: 2 [40698/47810.0 (85%)] Data (t): 1.916 Batch (t): 2.904, 254.621/s, 42.4368/s/gpu LR: 0.000017 Logit Scale: 99.989 Contrastive_loss: 7.0723 (7.6320) Dstill: 7.2038 (9.2456) Loss: 14.276 (16.878)
INFO:root:Train Epoch: 2 [47880/47810.0 (100%)] Data (t): 1.958 Batch (t): 2.966, 266.611/s, 44.4352/s/gpu LR: 0.000018 Logit Scale: 99.988 Contrastive_loss: 6.6831 (7.3157) Dstill: 7.1057 (8.5323) Loss: 13.789 (15.848)
2025-02-05,02:29:11 | INFO | Train Epoch: 2 [47880/47810.0 (100%)] Data (t): 1.958 Batch (t): 2.966, 266.611/s, 44.4352/s/gpu LR: 0.000018 Logit Scale: 99.988 Contrastive_loss: 6.6831 (7.3157) Dstill: 7.1057 (8.5323) Loss: 13.789 (15.848)
INFO:root:Train Epoch: 2 [ 798/7500.0 (10%)] Data (t): 34.711 Batch (t): 41.065, 19.4325/s, 3.23874/s/gpu LR: 0.000002 Logit Scale: 99.988 Contrastive_loss: 52.599 (52.599) Dstill: 25.307 (25.307) Loss: 77.906 (77.906)
2025-02-05,02:29:52 | INFO | Train Epoch: 2 [ 798/7500.0 (10%)] Data (t): 34.711 Batch (t): 41.065, 19.4325/s, 3.23874/s/gpu LR: 0.000002 Logit Scale: 99.988 Contrastive_loss: 52.599 (52.599) Dstill: 25.307 (25.307) Loss: 77.906 (77.906)
INFO:root:Train Epoch: 2 [7980/7500.0 (100%)] Data (t): 34.180 Batch (t): 41.497, 18.9876/s, 3.16460/s/gpu LR: 0.000003 Logit Scale: 99.985 Contrastive_loss: 46.883 (49.741) Dstill: 21.462 (23.384) Loss: 68.345 (73.126)
2025-02-05,02:36:06 | INFO | Train Epoch: 2 [7980/7500.0 (100%)] Data (t): 34.180 Batch (t): 41.497, 18.9876/s, 3.16460/s/gpu LR: 0.000003 Logit Scale: 99.985 Contrastive_loss: 46.883 (49.741) Dstill: 21.462 (23.384) Loss: 68.345 (73.126)
INFO:root:Start epoch 3
2025-02-05,02:36:07 | INFO | Start epoch 3
INFO:root:Train Epoch: 3 [    798/1859739.0 (0%)] Data (t): 0.931 Batch (t): 1.884, 423.462/s, 70.5769/s/gpu LR: 0.000099 Logit Scale: 99.985 Contrastive_loss: 5.7324 (5.7324) Dstill: 5.8513 (5.8513) Loss: 11.584 (11.584)
2025-02-05,02:36:08 | INFO | Train Epoch: 3 [    798/1859739.0 (0%)] Data (t): 0.931 Batch (t): 1.884, 423.462/s, 70.5769/s/gpu LR: 0.000099 Logit Scale: 99.985 Contrastive_loss: 5.7324 (5.7324) Dstill: 5.8513 (5.8513) Loss: 11.584 (11.584)
INFO:root:Train Epoch: 3 [  40698/1859739.0 (2%)] Data (t): 0.490 Batch (t): 1.536, 517.545/s, 86.2574/s/gpu LR: 0.000099 Logit Scale: 99.828 Contrastive_loss: 3.4737 (4.6031) Dstill: 5.9483 (5.8998) Loss: 9.4220 (10.503)
2025-02-05,02:37:25 | INFO | Train Epoch: 3 [  40698/1859739.0 (2%)] Data (t): 0.490 Batch (t): 1.536, 517.545/s, 86.2574/s/gpu LR: 0.000099 Logit Scale: 99.828 Contrastive_loss: 3.4737 (4.6031) Dstill: 5.9483 (5.8998) Loss: 9.4220 (10.503)
INFO:root:Train Epoch: 3 [  80598/1859739.0 (4%)] Data (t): 0.491 Batch (t): 1.533, 534.862/s, 89.1437/s/gpu LR: 0.000099 Logit Scale: 99.827 Contrastive_loss: 3.3867 (4.1976) Dstill: 4.8043 (5.5346) Loss: 8.1910 (9.7322)
2025-02-05,02:38:42 | INFO | Train Epoch: 3 [  80598/1859739.0 (4%)] Data (t): 0.491 Batch (t): 1.533, 534.862/s, 89.1437/s/gpu LR: 0.000099 Logit Scale: 99.827 Contrastive_loss: 3.3867 (4.1976) Dstill: 4.8043 (5.5346) Loss: 8.1910 (9.7322)
INFO:root:Train Epoch: 3 [ 120498/1859739.0 (6%)] Data (t): 0.489 Batch (t): 1.533, 556.509/s, 92.7515/s/gpu LR: 0.000099 Logit Scale: 99.830 Contrastive_loss: 3.1656 (3.9396) Dstill: 3.3295 (4.9834) Loss: 6.4951 (8.9230)
2025-02-05,02:39:59 | INFO | Train Epoch: 3 [ 120498/1859739.0 (6%)] Data (t): 0.489 Batch (t): 1.533, 556.509/s, 92.7515/s/gpu LR: 0.000099 Logit Scale: 99.830 Contrastive_loss: 3.1656 (3.9396) Dstill: 3.3295 (4.9834) Loss: 6.4951 (8.9230)
INFO:root:Train Epoch: 3 [ 160398/1859739.0 (9%)] Data (t): 0.527 Batch (t): 1.527, 511.825/s, 85.3042/s/gpu LR: 0.000099 Logit Scale: 99.832 Contrastive_loss: 3.0838 (3.7684) Dstill: 3.0753 (4.6017) Loss: 6.1591 (8.3702)
2025-02-05,02:41:15 | INFO | Train Epoch: 3 [ 160398/1859739.0 (9%)] Data (t): 0.527 Batch (t): 1.527, 511.825/s, 85.3042/s/gpu LR: 0.000099 Logit Scale: 99.832 Contrastive_loss: 3.0838 (3.7684) Dstill: 3.0753 (4.6017) Loss: 6.1591 (8.3702)
INFO:root:Train Epoch: 3 [ 200298/1859739.0 (11%)] Data (t): 0.581 Batch (t): 1.525, 540.046/s, 90.0076/s/gpu LR: 0.000099 Logit Scale: 99.833 Contrastive_loss: 3.5033 (3.7242) Dstill: 2.8674 (4.3127) Loss: 6.3707 (8.0369)
2025-02-05,02:42:31 | INFO | Train Epoch: 3 [ 200298/1859739.0 (11%)] Data (t): 0.581 Batch (t): 1.525, 540.046/s, 90.0076/s/gpu LR: 0.000099 Logit Scale: 99.833 Contrastive_loss: 3.5033 (3.7242) Dstill: 2.8674 (4.3127) Loss: 6.3707 (8.0369)
INFO:root:Train Epoch: 3 [ 240198/1859739.0 (13%)] Data (t): 0.580 Batch (t): 1.534, 585.739/s, 97.6232/s/gpu LR: 0.000099 Logit Scale: 99.839 Contrastive_loss: 3.4145 (3.6800) Dstill: 2.9501 (4.1180) Loss: 6.3645 (7.7980)
2025-02-05,02:43:48 | INFO | Train Epoch: 3 [ 240198/1859739.0 (13%)] Data (t): 0.580 Batch (t): 1.534, 585.739/s, 97.6232/s/gpu LR: 0.000099 Logit Scale: 99.839 Contrastive_loss: 3.4145 (3.6800) Dstill: 2.9501 (4.1180) Loss: 6.3645 (7.7980)
INFO:root:Train Epoch: 3 [ 280098/1859739.0 (15%)] Data (t): 0.590 Batch (t): 1.546, 511.899/s, 85.3165/s/gpu LR: 0.000099 Logit Scale: 99.844 Contrastive_loss: 3.1222 (3.6103) Dstill: 2.7515 (3.9472) Loss: 5.8737 (7.5575)
2025-02-05,02:45:05 | INFO | Train Epoch: 3 [ 280098/1859739.0 (15%)] Data (t): 0.590 Batch (t): 1.546, 511.899/s, 85.3165/s/gpu LR: 0.000099 Logit Scale: 99.844 Contrastive_loss: 3.1222 (3.6103) Dstill: 2.7515 (3.9472) Loss: 5.8737 (7.5575)
INFO:root:Train Epoch: 3 [ 319998/1859739.0 (17%)] Data (t): 0.622 Batch (t): 1.557, 489.143/s, 81.5239/s/gpu LR: 0.000099 Logit Scale: 99.840 Contrastive_loss: 3.2352 (3.5686) Dstill: 2.8932 (3.8301) Loss: 6.1284 (7.3987)
2025-02-05,02:46:23 | INFO | Train Epoch: 3 [ 319998/1859739.0 (17%)] Data (t): 0.622 Batch (t): 1.557, 489.143/s, 81.5239/s/gpu LR: 0.000099 Logit Scale: 99.840 Contrastive_loss: 3.2352 (3.5686) Dstill: 2.8932 (3.8301) Loss: 6.1284 (7.3987)
INFO:root:Train Epoch: 3 [ 359898/1859739.0 (19%)] Data (t): 0.592 Batch (t): 1.531, 538.450/s, 89.7416/s/gpu LR: 0.000099 Logit Scale: 99.859 Contrastive_loss: 3.3055 (3.5423) Dstill: 2.7853 (3.7256) Loss: 6.0909 (7.2679)
2025-02-05,02:47:40 | INFO | Train Epoch: 3 [ 359898/1859739.0 (19%)] Data (t): 0.592 Batch (t): 1.531, 538.450/s, 89.7416/s/gpu LR: 0.000099 Logit Scale: 99.859 Contrastive_loss: 3.3055 (3.5423) Dstill: 2.7853 (3.7256) Loss: 6.0909 (7.2679)
INFO:root:Train Epoch: 3 [ 399798/1859739.0 (21%)] Data (t): 0.625 Batch (t): 1.554, 501.220/s, 83.5367/s/gpu LR: 0.000099 Logit Scale: 99.860 Contrastive_loss: 3.3323 (3.5232) Dstill: 2.8911 (3.6498) Loss: 6.2234 (7.1730)
2025-02-05,02:48:57 | INFO | Train Epoch: 3 [ 399798/1859739.0 (21%)] Data (t): 0.625 Batch (t): 1.554, 501.220/s, 83.5367/s/gpu LR: 0.000099 Logit Scale: 99.860 Contrastive_loss: 3.3323 (3.5232) Dstill: 2.8911 (3.6498) Loss: 6.2234 (7.1730)
INFO:root:Train Epoch: 3 [ 439698/1859739.0 (24%)] Data (t): 0.600 Batch (t): 1.547, 466.895/s, 77.8159/s/gpu LR: 0.000099 Logit Scale: 99.861 Contrastive_loss: 3.2229 (3.4982) Dstill: 2.8810 (3.5857) Loss: 6.1039 (7.0839)
2025-02-05,02:50:15 | INFO | Train Epoch: 3 [ 439698/1859739.0 (24%)] Data (t): 0.600 Batch (t): 1.547, 466.895/s, 77.8159/s/gpu LR: 0.000099 Logit Scale: 99.861 Contrastive_loss: 3.2229 (3.4982) Dstill: 2.8810 (3.5857) Loss: 6.1039 (7.0839)
INFO:root:Train Epoch: 3 [ 479598/1859739.0 (26%)] Data (t): 0.597 Batch (t): 1.539, 564.114/s, 94.0191/s/gpu LR: 0.000099 Logit Scale: 99.863 Contrastive_loss: 3.2148 (3.4764) Dstill: 2.9350 (3.5356) Loss: 6.1498 (7.0120)
2025-02-05,02:51:32 | INFO | Train Epoch: 3 [ 479598/1859739.0 (26%)] Data (t): 0.597 Batch (t): 1.539, 564.114/s, 94.0191/s/gpu LR: 0.000099 Logit Scale: 99.863 Contrastive_loss: 3.2148 (3.4764) Dstill: 2.9350 (3.5356) Loss: 6.1498 (7.0120)
INFO:root:Train Epoch: 3 [ 519498/1859739.0 (28%)] Data (t): 0.594 Batch (t): 1.536, 481.010/s, 80.1683/s/gpu LR: 0.000099 Logit Scale: 99.882 Contrastive_loss: 3.2726 (3.4618) Dstill: 2.9518 (3.4939) Loss: 6.2243 (6.9558)
2025-02-05,02:52:48 | INFO | Train Epoch: 3 [ 519498/1859739.0 (28%)] Data (t): 0.594 Batch (t): 1.536, 481.010/s, 80.1683/s/gpu LR: 0.000099 Logit Scale: 99.882 Contrastive_loss: 3.2726 (3.4618) Dstill: 2.9518 (3.4939) Loss: 6.2243 (6.9558)
INFO:root:Train Epoch: 3 [ 559398/1859739.0 (30%)] Data (t): 0.571 Batch (t): 1.527, 524.558/s, 87.4263/s/gpu LR: 0.000099 Logit Scale: 99.883 Contrastive_loss: 3.4386 (3.4603) Dstill: 2.7237 (3.4426) Loss: 6.1623 (6.9029)
2025-02-05,02:54:05 | INFO | Train Epoch: 3 [ 559398/1859739.0 (30%)] Data (t): 0.571 Batch (t): 1.527, 524.558/s, 87.4263/s/gpu LR: 0.000099 Logit Scale: 99.883 Contrastive_loss: 3.4386 (3.4603) Dstill: 2.7237 (3.4426) Loss: 6.1623 (6.9029)
INFO:root:Train Epoch: 3 [ 599298/1859739.0 (32%)] Data (t): 0.631 Batch (t): 1.581, 487.406/s, 81.2343/s/gpu LR: 0.000099 Logit Scale: 99.897 Contrastive_loss: 3.3271 (3.4519) Dstill: 2.8100 (3.4030) Loss: 6.1371 (6.8550)
2025-02-05,02:55:24 | INFO | Train Epoch: 3 [ 599298/1859739.0 (32%)] Data (t): 0.631 Batch (t): 1.581, 487.406/s, 81.2343/s/gpu LR: 0.000099 Logit Scale: 99.897 Contrastive_loss: 3.3271 (3.4519) Dstill: 2.8100 (3.4030) Loss: 6.1371 (6.8550)
INFO:root:Train Epoch: 3 [ 639198/1859739.0 (34%)] Data (t): 0.615 Batch (t): 1.553, 466.979/s, 77.8299/s/gpu LR: 0.000099 Logit Scale: 99.906 Contrastive_loss: 3.2993 (3.4430) Dstill: 2.8595 (3.3711) Loss: 6.1588 (6.8140)
2025-02-05,02:56:42 | INFO | Train Epoch: 3 [ 639198/1859739.0 (34%)] Data (t): 0.615 Batch (t): 1.553, 466.979/s, 77.8299/s/gpu LR: 0.000099 Logit Scale: 99.906 Contrastive_loss: 3.2993 (3.4430) Dstill: 2.8595 (3.3711) Loss: 6.1588 (6.8140)
INFO:root:Train Epoch: 3 [ 679098/1859739.0 (37%)] Data (t): 0.589 Batch (t): 1.535, 474.739/s, 79.1232/s/gpu LR: 0.000099 Logit Scale: 99.913 Contrastive_loss: 3.1401 (3.4261) Dstill: 2.6694 (3.3321) Loss: 5.8095 (6.7582)
2025-02-05,02:57:58 | INFO | Train Epoch: 3 [ 679098/1859739.0 (37%)] Data (t): 0.589 Batch (t): 1.535, 474.739/s, 79.1232/s/gpu LR: 0.000099 Logit Scale: 99.913 Contrastive_loss: 3.1401 (3.4261) Dstill: 2.6694 (3.3321) Loss: 5.8095 (6.7582)
INFO:root:Train Epoch: 3 [ 718998/1859739.0 (39%)] Data (t): 0.599 Batch (t): 1.564, 476.327/s, 79.3878/s/gpu LR: 0.000099 Logit Scale: 99.927 Contrastive_loss: 3.3773 (3.4236) Dstill: 2.7914 (3.3036) Loss: 6.1687 (6.7272)
2025-02-05,02:59:16 | INFO | Train Epoch: 3 [ 718998/1859739.0 (39%)] Data (t): 0.599 Batch (t): 1.564, 476.327/s, 79.3878/s/gpu LR: 0.000099 Logit Scale: 99.927 Contrastive_loss: 3.3773 (3.4236) Dstill: 2.7914 (3.3036) Loss: 6.1687 (6.7272)
INFO:root:Train Epoch: 3 [ 758898/1859739.0 (41%)] Data (t): 0.574 Batch (t): 1.548, 519.513/s, 86.5855/s/gpu LR: 0.000099 Logit Scale: 99.932 Contrastive_loss: 3.5188 (3.4283) Dstill: 2.8232 (3.2796) Loss: 6.3420 (6.7079)
2025-02-05,03:00:34 | INFO | Train Epoch: 3 [ 758898/1859739.0 (41%)] Data (t): 0.574 Batch (t): 1.548, 519.513/s, 86.5855/s/gpu LR: 0.000099 Logit Scale: 99.932 Contrastive_loss: 3.5188 (3.4283) Dstill: 2.8232 (3.2796) Loss: 6.3420 (6.7079)
INFO:root:Train Epoch: 3 [ 798798/1859739.0 (43%)] Data (t): 0.596 Batch (t): 1.547, 503.205/s, 83.8675/s/gpu LR: 0.000099 Logit Scale: 99.946 Contrastive_loss: 2.9594 (3.4060) Dstill: 2.6031 (3.2474) Loss: 5.5626 (6.6534)
2025-02-05,03:01:51 | INFO | Train Epoch: 3 [ 798798/1859739.0 (43%)] Data (t): 0.596 Batch (t): 1.547, 503.205/s, 83.8675/s/gpu LR: 0.000099 Logit Scale: 99.946 Contrastive_loss: 2.9594 (3.4060) Dstill: 2.6031 (3.2474) Loss: 5.5626 (6.6534)
INFO:root:Train Epoch: 3 [ 838698/1859739.0 (45%)] Data (t): 0.606 Batch (t): 1.538, 514.672/s, 85.7787/s/gpu LR: 0.000099 Logit Scale: 99.953 Contrastive_loss: 3.4701 (3.4089) Dstill: 2.7605 (3.2253) Loss: 6.2307 (6.6342)
2025-02-05,03:03:08 | INFO | Train Epoch: 3 [ 838698/1859739.0 (45%)] Data (t): 0.606 Batch (t): 1.538, 514.672/s, 85.7787/s/gpu LR: 0.000099 Logit Scale: 99.953 Contrastive_loss: 3.4701 (3.4089) Dstill: 2.7605 (3.2253) Loss: 6.2307 (6.6342)
INFO:root:Train Epoch: 3 [ 878598/1859739.0 (47%)] Data (t): 0.605 Batch (t): 1.580, 512.453/s, 85.4089/s/gpu LR: 0.000099 Logit Scale: 99.960 Contrastive_loss: 3.0463 (3.3932) Dstill: 2.7287 (3.2037) Loss: 5.7751 (6.5968)
2025-02-05,03:04:27 | INFO | Train Epoch: 3 [ 878598/1859739.0 (47%)] Data (t): 0.605 Batch (t): 1.580, 512.453/s, 85.4089/s/gpu LR: 0.000099 Logit Scale: 99.960 Contrastive_loss: 3.0463 (3.3932) Dstill: 2.7287 (3.2037) Loss: 5.7751 (6.5968)
INFO:root:Train Epoch: 3 [ 918498/1859739.0 (49%)] Data (t): 0.603 Batch (t): 1.549, 565.841/s, 94.3068/s/gpu LR: 0.000099 Logit Scale: 99.968 Contrastive_loss: 3.2557 (3.3874) Dstill: 2.6251 (3.1796) Loss: 5.8808 (6.5670)
2025-02-05,03:05:44 | INFO | Train Epoch: 3 [ 918498/1859739.0 (49%)] Data (t): 0.603 Batch (t): 1.549, 565.841/s, 94.3068/s/gpu LR: 0.000099 Logit Scale: 99.968 Contrastive_loss: 3.2557 (3.3874) Dstill: 2.6251 (3.1796) Loss: 5.8808 (6.5670)
INFO:root:Train Epoch: 3 [ 958398/1859739.0 (52%)] Data (t): 0.606 Batch (t): 1.544, 475.969/s, 79.3282/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.3075 (3.3842) Dstill: 2.7251 (3.1614) Loss: 6.0326 (6.5456)
2025-02-05,03:07:02 | INFO | Train Epoch: 3 [ 958398/1859739.0 (52%)] Data (t): 0.606 Batch (t): 1.544, 475.969/s, 79.3282/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.3075 (3.3842) Dstill: 2.7251 (3.1614) Loss: 6.0326 (6.5456)
INFO:root:Train Epoch: 3 [ 998298/1859739.0 (54%)] Data (t): 0.613 Batch (t): 1.560, 477.509/s, 79.5848/s/gpu LR: 0.000099 Logit Scale: 99.993 Contrastive_loss: 3.4795 (3.3879) Dstill: 2.8989 (3.1513) Loss: 6.3784 (6.5392)
2025-02-05,03:08:20 | INFO | Train Epoch: 3 [ 998298/1859739.0 (54%)] Data (t): 0.613 Batch (t): 1.560, 477.509/s, 79.5848/s/gpu LR: 0.000099 Logit Scale: 99.993 Contrastive_loss: 3.4795 (3.3879) Dstill: 2.8989 (3.1513) Loss: 6.3784 (6.5392)
INFO:root:Train Epoch: 3 [1038198/1859739.0 (56%)] Data (t): 0.593 Batch (t): 1.555, 514.281/s, 85.7135/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.0073 (3.3738) Dstill: 2.7744 (3.1373) Loss: 5.7817 (6.5111)
2025-02-05,03:09:37 | INFO | Train Epoch: 3 [1038198/1859739.0 (56%)] Data (t): 0.593 Batch (t): 1.555, 514.281/s, 85.7135/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.0073 (3.3738) Dstill: 2.7744 (3.1373) Loss: 5.7817 (6.5111)
INFO:root:Train Epoch: 3 [1078098/1859739.0 (58%)] Data (t): 0.617 Batch (t): 1.549, 512.895/s, 85.4825/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 2.9943 (3.3602) Dstill: 2.9554 (3.1308) Loss: 5.9497 (6.4911)
2025-02-05,03:10:55 | INFO | Train Epoch: 3 [1078098/1859739.0 (58%)] Data (t): 0.617 Batch (t): 1.549, 512.895/s, 85.4825/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 2.9943 (3.3602) Dstill: 2.9554 (3.1308) Loss: 5.9497 (6.4911)
INFO:root:Train Epoch: 3 [1117998/1859739.0 (60%)] Data (t): 0.610 Batch (t): 1.548, 544.984/s, 90.8307/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.4532 (3.3634) Dstill: 2.6912 (3.1157) Loss: 6.1444 (6.4791)
2025-02-05,03:12:12 | INFO | Train Epoch: 3 [1117998/1859739.0 (60%)] Data (t): 0.610 Batch (t): 1.548, 544.984/s, 90.8307/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.4532 (3.3634) Dstill: 2.6912 (3.1157) Loss: 6.1444 (6.4791)
INFO:root:Train Epoch: 3 [1157898/1859739.0 (62%)] Data (t): 0.617 Batch (t): 1.592, 544.438/s, 90.7397/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1741 (3.3571) Dstill: 2.6569 (3.1004) Loss: 5.8310 (6.4575)
2025-02-05,03:13:32 | INFO | Train Epoch: 3 [1157898/1859739.0 (62%)] Data (t): 0.617 Batch (t): 1.592, 544.438/s, 90.7397/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1741 (3.3571) Dstill: 2.6569 (3.1004) Loss: 5.8310 (6.4575)
INFO:root:Train Epoch: 3 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.542, 535.450/s, 89.2417/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.6797 (3.3675) Dstill: 2.7594 (3.0894) Loss: 6.4391 (6.4569)
2025-02-05,03:14:49 | INFO | Train Epoch: 3 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.542, 535.450/s, 89.2417/s/gpu LR: 0.000099 Logit Scale: 99.984 Contrastive_loss: 3.6797 (3.3675) Dstill: 2.7594 (3.0894) Loss: 6.4391 (6.4569)
INFO:root:Train Epoch: 3 [1237698/1859739.0 (67%)] Data (t): 0.628 Batch (t): 1.554, 541.267/s, 90.2111/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2787 (3.3648) Dstill: 2.6016 (3.0741) Loss: 5.8803 (6.4389)
2025-02-05,03:16:07 | INFO | Train Epoch: 3 [1237698/1859739.0 (67%)] Data (t): 0.628 Batch (t): 1.554, 541.267/s, 90.2111/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2787 (3.3648) Dstill: 2.6016 (3.0741) Loss: 5.8803 (6.4389)
INFO:root:Train Epoch: 3 [1277598/1859739.0 (69%)] Data (t): 0.558 Batch (t): 1.502, 592.236/s, 98.7061/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4473 (3.3673) Dstill: 2.7013 (3.0629) Loss: 6.1487 (6.4301)
2025-02-05,03:17:22 | INFO | Train Epoch: 3 [1277598/1859739.0 (69%)] Data (t): 0.558 Batch (t): 1.502, 592.236/s, 98.7061/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4473 (3.3673) Dstill: 2.7013 (3.0629) Loss: 6.1487 (6.4301)
INFO:root:Train Epoch: 3 [1317498/1859739.0 (71%)] Data (t): 0.490 Batch (t): 1.426, 606.122/s, 101.020/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.0932 (3.3592) Dstill: 2.7752 (3.0544) Loss: 5.8684 (6.4136)
2025-02-05,03:18:33 | INFO | Train Epoch: 3 [1317498/1859739.0 (71%)] Data (t): 0.490 Batch (t): 1.426, 606.122/s, 101.020/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.0932 (3.3592) Dstill: 2.7752 (3.0544) Loss: 5.8684 (6.4136)
INFO:root:Train Epoch: 3 [1357398/1859739.0 (73%)] Data (t): 0.438 Batch (t): 1.349, 576.045/s, 96.0075/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1615 (3.3536) Dstill: 2.7432 (3.0455) Loss: 5.9047 (6.3991)
2025-02-05,03:19:40 | INFO | Train Epoch: 3 [1357398/1859739.0 (73%)] Data (t): 0.438 Batch (t): 1.349, 576.045/s, 96.0075/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1615 (3.3536) Dstill: 2.7432 (3.0455) Loss: 5.9047 (6.3991)
INFO:root:Train Epoch: 3 [1397298/1859739.0 (75%)] Data (t): 0.556 Batch (t): 1.532, 509.623/s, 84.9372/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.3692 (3.3540) Dstill: 2.6224 (3.0337) Loss: 5.9917 (6.3877)
2025-02-05,03:20:57 | INFO | Train Epoch: 3 [1397298/1859739.0 (75%)] Data (t): 0.556 Batch (t): 1.532, 509.623/s, 84.9372/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.3692 (3.3540) Dstill: 2.6224 (3.0337) Loss: 5.9917 (6.3877)
INFO:root:Train Epoch: 3 [1437198/1859739.0 (77%)] Data (t): 0.624 Batch (t): 1.588, 488.538/s, 81.4230/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2403 (3.3509) Dstill: 2.7937 (3.0273) Loss: 6.0340 (6.3782)
2025-02-05,03:22:16 | INFO | Train Epoch: 3 [1437198/1859739.0 (77%)] Data (t): 0.624 Batch (t): 1.588, 488.538/s, 81.4230/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2403 (3.3509) Dstill: 2.7937 (3.0273) Loss: 6.0340 (6.3782)
INFO:root:Train Epoch: 3 [1477098/1859739.0 (79%)] Data (t): 0.618 Batch (t): 1.576, 504.864/s, 84.1440/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.1049 (3.3444) Dstill: 2.6275 (3.0167) Loss: 5.7324 (6.3612)
2025-02-05,03:23:35 | INFO | Train Epoch: 3 [1477098/1859739.0 (79%)] Data (t): 0.618 Batch (t): 1.576, 504.864/s, 84.1440/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.1049 (3.3444) Dstill: 2.6275 (3.0167) Loss: 5.7324 (6.3612)
INFO:root:Train Epoch: 3 [1516998/1859739.0 (82%)] Data (t): 0.604 Batch (t): 1.549, 550.450/s, 91.7417/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1994 (3.3407) Dstill: 2.6444 (3.0072) Loss: 5.8438 (6.3479)
2025-02-05,03:24:53 | INFO | Train Epoch: 3 [1516998/1859739.0 (82%)] Data (t): 0.604 Batch (t): 1.549, 550.450/s, 91.7417/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1994 (3.3407) Dstill: 2.6444 (3.0072) Loss: 5.8438 (6.3479)
INFO:root:Train Epoch: 3 [1556898/1859739.0 (84%)] Data (t): 0.609 Batch (t): 1.586, 535.517/s, 89.2528/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1107 (3.3350) Dstill: 2.7357 (3.0004) Loss: 5.8465 (6.3354)
2025-02-05,03:26:12 | INFO | Train Epoch: 3 [1556898/1859739.0 (84%)] Data (t): 0.609 Batch (t): 1.586, 535.517/s, 89.2528/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1107 (3.3350) Dstill: 2.7357 (3.0004) Loss: 5.8465 (6.3354)
INFO:root:Train Epoch: 3 [1596798/1859739.0 (86%)] Data (t): 0.619 Batch (t): 1.592, 509.298/s, 84.8830/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.2729 (3.3335) Dstill: 2.5392 (2.9892) Loss: 5.8121 (6.3226)
2025-02-05,03:27:32 | INFO | Train Epoch: 3 [1596798/1859739.0 (86%)] Data (t): 0.619 Batch (t): 1.592, 509.298/s, 84.8830/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.2729 (3.3335) Dstill: 2.5392 (2.9892) Loss: 5.8121 (6.3226)
INFO:root:Train Epoch: 3 [1636698/1859739.0 (88%)] Data (t): 0.621 Batch (t): 1.579, 471.178/s, 78.5297/s/gpu LR: 0.000099 Logit Scale: 99.994 Contrastive_loss: 3.4356 (3.3359) Dstill: 2.6431 (2.9809) Loss: 6.0786 (6.3168)
2025-02-05,03:28:50 | INFO | Train Epoch: 3 [1636698/1859739.0 (88%)] Data (t): 0.621 Batch (t): 1.579, 471.178/s, 78.5297/s/gpu LR: 0.000099 Logit Scale: 99.994 Contrastive_loss: 3.4356 (3.3359) Dstill: 2.6431 (2.9809) Loss: 6.0786 (6.3168)
INFO:root:Train Epoch: 3 [1676598/1859739.0 (90%)] Data (t): 0.577 Batch (t): 1.586, 511.662/s, 85.2770/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.5415 (3.3407) Dstill: 2.9715 (2.9807) Loss: 6.5130 (6.3214)
2025-02-05,03:30:10 | INFO | Train Epoch: 3 [1676598/1859739.0 (90%)] Data (t): 0.577 Batch (t): 1.586, 511.662/s, 85.2770/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.5415 (3.3407) Dstill: 2.9715 (2.9807) Loss: 6.5130 (6.3214)
INFO:root:Train Epoch: 3 [1716498/1859739.0 (92%)] Data (t): 0.561 Batch (t): 1.597, 494.829/s, 82.4715/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3292 (3.3404) Dstill: 2.5990 (2.9720) Loss: 5.9282 (6.3124)
2025-02-05,03:31:30 | INFO | Train Epoch: 3 [1716498/1859739.0 (92%)] Data (t): 0.561 Batch (t): 1.597, 494.829/s, 82.4715/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3292 (3.3404) Dstill: 2.5990 (2.9720) Loss: 5.9282 (6.3124)
INFO:root:Train Epoch: 3 [1756398/1859739.0 (94%)] Data (t): 0.529 Batch (t): 1.586, 511.277/s, 85.2128/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.4890 (3.3437) Dstill: 2.6487 (2.9648) Loss: 6.1378 (6.3086)
2025-02-05,03:32:49 | INFO | Train Epoch: 3 [1756398/1859739.0 (94%)] Data (t): 0.529 Batch (t): 1.586, 511.277/s, 85.2128/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.4890 (3.3437) Dstill: 2.6487 (2.9648) Loss: 6.1378 (6.3086)
INFO:root:Train Epoch: 3 [1796298/1859739.0 (97%)] Data (t): 0.526 Batch (t): 1.569, 499.813/s, 83.3021/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.5457 (3.3481) Dstill: 2.5819 (2.9565) Loss: 6.1275 (6.3046)
2025-02-05,03:34:07 | INFO | Train Epoch: 3 [1796298/1859739.0 (97%)] Data (t): 0.526 Batch (t): 1.569, 499.813/s, 83.3021/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.5457 (3.3481) Dstill: 2.5819 (2.9565) Loss: 6.1275 (6.3046)
INFO:root:Train Epoch: 3 [1836198/1859739.0 (99%)] Data (t): 0.562 Batch (t): 1.595, 499.159/s, 83.1932/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4584 (3.3505) Dstill: 2.6477 (2.9499) Loss: 6.1061 (6.3004)
2025-02-05,03:35:27 | INFO | Train Epoch: 3 [1836198/1859739.0 (99%)] Data (t): 0.562 Batch (t): 1.595, 499.159/s, 83.1932/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4584 (3.3505) Dstill: 2.6477 (2.9499) Loss: 6.1061 (6.3004)
INFO:root:Train Epoch: 3 [1860138/1859739.0 (100%)] Data (t): 0.567 Batch (t): 1.575, 518.884/s, 86.4806/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.2090 (3.3475) Dstill: 3.2053 (2.9553) Loss: 6.4144 (6.3028)
2025-02-05,03:36:14 | INFO | Train Epoch: 3 [1860138/1859739.0 (100%)] Data (t): 0.567 Batch (t): 1.575, 518.884/s, 86.4806/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.2090 (3.3475) Dstill: 3.2053 (2.9553) Loss: 6.4144 (6.3028)
INFO:root:Train Epoch: 3 [  798/59975.0 (1%)] Data (t): 0.627 Batch (t): 1.517, 526.074/s, 87.6789/s/gpu LR: 0.000023 Logit Scale: 99.998 Contrastive_loss: 7.6739 (7.6739) Dstill: 9.1835 (9.1835) Loss: 16.857 (16.857)
2025-02-05,03:36:16 | INFO | Train Epoch: 3 [  798/59975.0 (1%)] Data (t): 0.627 Batch (t): 1.517, 526.074/s, 87.6789/s/gpu LR: 0.000023 Logit Scale: 99.998 Contrastive_loss: 7.6739 (7.6739) Dstill: 9.1835 (9.1835) Loss: 16.857 (16.857)
INFO:root:Train Epoch: 3 [40698/59975.0 (67%)] Data (t): 0.463 Batch (t): 1.387, 603.499/s, 100.583/s/gpu LR: 0.000028 Logit Scale: 99.991 Contrastive_loss: 6.9687 (7.3213) Dstill: 6.3318 (7.7576) Loss: 13.301 (15.079)
2025-02-05,03:37:25 | INFO | Train Epoch: 3 [40698/59975.0 (67%)] Data (t): 0.463 Batch (t): 1.387, 603.499/s, 100.583/s/gpu LR: 0.000028 Logit Scale: 99.991 Contrastive_loss: 6.9687 (7.3213) Dstill: 6.3318 (7.7576) Loss: 13.301 (15.079)
INFO:root:Train Epoch: 3 [60648/59975.0 (100%)] Data (t): 0.488 Batch (t): 1.439, 586.727/s, 97.7879/s/gpu LR: 0.000030 Logit Scale: 99.989 Contrastive_loss: 6.6883 (7.1103) Dstill: 5.9905 (7.1686) Loss: 12.679 (14.279)
2025-02-05,03:38:01 | INFO | Train Epoch: 3 [60648/59975.0 (100%)] Data (t): 0.488 Batch (t): 1.439, 586.727/s, 97.7879/s/gpu LR: 0.000030 Logit Scale: 99.989 Contrastive_loss: 6.6883 (7.1103) Dstill: 5.9905 (7.1686) Loss: 12.679 (14.279)
INFO:root:Train Epoch: 3 [  798/47810.0 (2%)] Data (t): 2.393 Batch (t): 3.364, 237.187/s, 39.5312/s/gpu LR: 0.000018 Logit Scale: 99.989 Contrastive_loss: 8.2300 (8.2300) Dstill: 12.212 (12.212) Loss: 20.442 (20.442)
2025-02-05,03:38:05 | INFO | Train Epoch: 3 [  798/47810.0 (2%)] Data (t): 2.393 Batch (t): 3.364, 237.187/s, 39.5312/s/gpu LR: 0.000018 Logit Scale: 99.989 Contrastive_loss: 8.2300 (8.2300) Dstill: 12.212 (12.212) Loss: 20.442 (20.442)
INFO:root:Train Epoch: 3 [40698/47810.0 (85%)] Data (t): 2.196 Batch (t): 3.147, 255.843/s, 42.6405/s/gpu LR: 0.000023 Logit Scale: 99.984 Contrastive_loss: 6.9818 (7.6059) Dstill: 7.2084 (9.7103) Loss: 14.190 (17.316)
2025-02-05,03:40:42 | INFO | Train Epoch: 3 [40698/47810.0 (85%)] Data (t): 2.196 Batch (t): 3.147, 255.843/s, 42.6405/s/gpu LR: 0.000023 Logit Scale: 99.984 Contrastive_loss: 6.9818 (7.6059) Dstill: 7.2084 (9.7103) Loss: 14.190 (17.316)
INFO:root:Train Epoch: 3 [47880/47810.0 (100%)] Data (t): 2.228 Batch (t): 3.171, 253.976/s, 42.3294/s/gpu LR: 0.000024 Logit Scale: 99.984 Contrastive_loss: 6.6167 (7.2762) Dstill: 7.0907 (8.8371) Loss: 13.707 (16.113)
2025-02-05,03:41:11 | INFO | Train Epoch: 3 [47880/47810.0 (100%)] Data (t): 2.228 Batch (t): 3.171, 253.976/s, 42.3294/s/gpu LR: 0.000024 Logit Scale: 99.984 Contrastive_loss: 6.6167 (7.2762) Dstill: 7.0907 (8.8371) Loss: 13.707 (16.113)
INFO:root:Train Epoch: 3 [ 798/7500.0 (10%)] Data (t): 35.167 Batch (t): 42.150, 18.9326/s, 3.15543/s/gpu LR: 0.000003 Logit Scale: 99.984 Contrastive_loss: 8.7261 (8.7261) Dstill: 13.561 (13.561) Loss: 22.287 (22.287)
2025-02-05,03:41:53 | INFO | Train Epoch: 3 [ 798/7500.0 (10%)] Data (t): 35.167 Batch (t): 42.150, 18.9326/s, 3.15543/s/gpu LR: 0.000003 Logit Scale: 99.984 Contrastive_loss: 8.7261 (8.7261) Dstill: 13.561 (13.561) Loss: 22.287 (22.287)
INFO:root:Train Epoch: 3 [7980/7500.0 (100%)] Data (t): 34.581 Batch (t): 42.213, 19.1477/s, 3.19129/s/gpu LR: 0.000004 Logit Scale: 99.982 Contrastive_loss: 8.1347 (8.4304) Dstill: 11.986 (12.774) Loss: 20.121 (21.204)
2025-02-05,03:48:13 | INFO | Train Epoch: 3 [7980/7500.0 (100%)] Data (t): 34.581 Batch (t): 42.213, 19.1477/s, 3.19129/s/gpu LR: 0.000004 Logit Scale: 99.982 Contrastive_loss: 8.1347 (8.4304) Dstill: 11.986 (12.774) Loss: 20.121 (21.204)
INFO:root:Start epoch 4
2025-02-05,03:48:13 | INFO | Start epoch 4
INFO:root:Train Epoch: 4 [    798/1859739.0 (0%)] Data (t): 0.665 Batch (t): 1.550, 514.827/s, 85.8046/s/gpu LR: 0.000099 Logit Scale: 99.982 Contrastive_loss: 4.9564 (4.9564) Dstill: 6.2808 (6.2808) Loss: 11.237 (11.237)
2025-02-05,03:48:15 | INFO | Train Epoch: 4 [    798/1859739.0 (0%)] Data (t): 0.665 Batch (t): 1.550, 514.827/s, 85.8046/s/gpu LR: 0.000099 Logit Scale: 99.982 Contrastive_loss: 4.9564 (4.9564) Dstill: 6.2808 (6.2808) Loss: 11.237 (11.237)
INFO:root:Train Epoch: 4 [  40698/1859739.0 (2%)] Data (t): 0.470 Batch (t): 1.412, 529.452/s, 88.2419/s/gpu LR: 0.000099 Logit Scale: 99.949 Contrastive_loss: 3.1738 (4.0651) Dstill: 3.3490 (4.8149) Loss: 6.5228 (8.8800)
2025-02-05,03:49:26 | INFO | Train Epoch: 4 [  40698/1859739.0 (2%)] Data (t): 0.470 Batch (t): 1.412, 529.452/s, 88.2419/s/gpu LR: 0.000099 Logit Scale: 99.949 Contrastive_loss: 3.1738 (4.0651) Dstill: 3.3490 (4.8149) Loss: 6.5228 (8.8800)
INFO:root:Train Epoch: 4 [  80598/1859739.0 (4%)] Data (t): 0.456 Batch (t): 1.412, 607.617/s, 101.270/s/gpu LR: 0.000099 Logit Scale: 99.961 Contrastive_loss: 3.2123 (3.7808) Dstill: 2.8792 (4.1697) Loss: 6.0915 (7.9505)
2025-02-05,03:50:36 | INFO | Train Epoch: 4 [  80598/1859739.0 (4%)] Data (t): 0.456 Batch (t): 1.412, 607.617/s, 101.270/s/gpu LR: 0.000099 Logit Scale: 99.961 Contrastive_loss: 3.2123 (3.7808) Dstill: 2.8792 (4.1697) Loss: 6.0915 (7.9505)
INFO:root:Train Epoch: 4 [ 120498/1859739.0 (6%)] Data (t): 0.475 Batch (t): 1.402, 563.434/s, 93.9057/s/gpu LR: 0.000099 Logit Scale: 99.975 Contrastive_loss: 3.0979 (3.6101) Dstill: 2.8201 (3.8323) Loss: 5.9180 (7.4424)
2025-02-05,03:51:46 | INFO | Train Epoch: 4 [ 120498/1859739.0 (6%)] Data (t): 0.475 Batch (t): 1.402, 563.434/s, 93.9057/s/gpu LR: 0.000099 Logit Scale: 99.975 Contrastive_loss: 3.0979 (3.6101) Dstill: 2.8201 (3.8323) Loss: 5.9180 (7.4424)
INFO:root:Train Epoch: 4 [ 160398/1859739.0 (9%)] Data (t): 0.509 Batch (t): 1.452, 500.938/s, 83.4896/s/gpu LR: 0.000099 Logit Scale: 99.988 Contrastive_loss: 2.9725 (3.4826) Dstill: 2.7953 (3.6249) Loss: 5.7678 (7.1075)
2025-02-05,03:52:59 | INFO | Train Epoch: 4 [ 160398/1859739.0 (9%)] Data (t): 0.509 Batch (t): 1.452, 500.938/s, 83.4896/s/gpu LR: 0.000099 Logit Scale: 99.988 Contrastive_loss: 2.9725 (3.4826) Dstill: 2.7953 (3.6249) Loss: 5.7678 (7.1075)
INFO:root:Train Epoch: 4 [ 200298/1859739.0 (11%)] Data (t): 0.582 Batch (t): 1.515, 538.175/s, 89.6958/s/gpu LR: 0.000099 Logit Scale: 99.987 Contrastive_loss: 3.5038 (3.4861) Dstill: 2.6320 (3.4594) Loss: 6.1358 (6.9455)
2025-02-05,03:54:15 | INFO | Train Epoch: 4 [ 200298/1859739.0 (11%)] Data (t): 0.582 Batch (t): 1.515, 538.175/s, 89.6958/s/gpu LR: 0.000099 Logit Scale: 99.987 Contrastive_loss: 3.5038 (3.4861) Dstill: 2.6320 (3.4594) Loss: 6.1358 (6.9455)
INFO:root:Train Epoch: 4 [ 240198/1859739.0 (13%)] Data (t): 0.578 Batch (t): 1.520, 585.158/s, 97.5264/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4314 (3.4783) Dstill: 2.7494 (3.3580) Loss: 6.1808 (6.8363)
2025-02-05,03:55:31 | INFO | Train Epoch: 4 [ 240198/1859739.0 (13%)] Data (t): 0.578 Batch (t): 1.520, 585.158/s, 97.5264/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.4314 (3.4783) Dstill: 2.7494 (3.3580) Loss: 6.1808 (6.8363)
INFO:root:Train Epoch: 4 [ 280098/1859739.0 (15%)] Data (t): 0.611 Batch (t): 1.536, 576.621/s, 96.1035/s/gpu LR: 0.000099 Logit Scale: 99.993 Contrastive_loss: 3.0234 (3.4214) Dstill: 2.7575 (3.2829) Loss: 5.7809 (6.7044)
2025-02-05,03:56:47 | INFO | Train Epoch: 4 [ 280098/1859739.0 (15%)] Data (t): 0.611 Batch (t): 1.536, 576.621/s, 96.1035/s/gpu LR: 0.000099 Logit Scale: 99.993 Contrastive_loss: 3.0234 (3.4214) Dstill: 2.7575 (3.2829) Loss: 5.7809 (6.7044)
INFO:root:Train Epoch: 4 [ 319998/1859739.0 (17%)] Data (t): 0.629 Batch (t): 1.556, 517.327/s, 86.2211/s/gpu LR: 0.000099 Logit Scale: 99.981 Contrastive_loss: 3.1252 (3.3885) Dstill: 2.7900 (3.2282) Loss: 5.9152 (6.6167)
2025-02-05,03:58:05 | INFO | Train Epoch: 4 [ 319998/1859739.0 (17%)] Data (t): 0.629 Batch (t): 1.556, 517.327/s, 86.2211/s/gpu LR: 0.000099 Logit Scale: 99.981 Contrastive_loss: 3.1252 (3.3885) Dstill: 2.7900 (3.2282) Loss: 5.9152 (6.6167)
INFO:root:Train Epoch: 4 [ 359898/1859739.0 (19%)] Data (t): 0.572 Batch (t): 1.519, 526.557/s, 87.7595/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2436 (3.3740) Dstill: 2.7219 (3.1775) Loss: 5.9655 (6.5516)
2025-02-05,03:59:21 | INFO | Train Epoch: 4 [ 359898/1859739.0 (19%)] Data (t): 0.572 Batch (t): 1.519, 526.557/s, 87.7595/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2436 (3.3740) Dstill: 2.7219 (3.1775) Loss: 5.9655 (6.5516)
INFO:root:Train Epoch: 4 [ 399798/1859739.0 (21%)] Data (t): 0.591 Batch (t): 1.537, 501.258/s, 83.5430/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.2319 (3.3611) Dstill: 2.9425 (3.1562) Loss: 6.1744 (6.5173)
2025-02-05,04:00:38 | INFO | Train Epoch: 4 [ 399798/1859739.0 (21%)] Data (t): 0.591 Batch (t): 1.537, 501.258/s, 83.5430/s/gpu LR: 0.000099 Logit Scale: 99.997 Contrastive_loss: 3.2319 (3.3611) Dstill: 2.9425 (3.1562) Loss: 6.1744 (6.5173)
INFO:root:Train Epoch: 4 [ 439698/1859739.0 (24%)] Data (t): 0.581 Batch (t): 1.528, 486.940/s, 81.1566/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2063 (3.3482) Dstill: 2.9658 (3.1403) Loss: 6.1721 (6.4885)
2025-02-05,04:01:55 | INFO | Train Epoch: 4 [ 439698/1859739.0 (24%)] Data (t): 0.581 Batch (t): 1.528, 486.940/s, 81.1566/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2063 (3.3482) Dstill: 2.9658 (3.1403) Loss: 6.1721 (6.4885)
INFO:root:Train Epoch: 4 [ 479598/1859739.0 (26%)] Data (t): 0.589 Batch (t): 1.550, 533.432/s, 88.9053/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.1344 (3.3318) Dstill: 2.7228 (3.1082) Loss: 5.8572 (6.4399)
2025-02-05,04:03:12 | INFO | Train Epoch: 4 [ 479598/1859739.0 (26%)] Data (t): 0.589 Batch (t): 1.550, 533.432/s, 88.9053/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.1344 (3.3318) Dstill: 2.7228 (3.1082) Loss: 5.8572 (6.4399)
INFO:root:Train Epoch: 4 [ 519498/1859739.0 (28%)] Data (t): 0.582 Batch (t): 1.527, 505.936/s, 84.3227/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1939 (3.3219) Dstill: 2.6102 (3.0726) Loss: 5.8041 (6.3945)
2025-02-05,04:04:28 | INFO | Train Epoch: 4 [ 519498/1859739.0 (28%)] Data (t): 0.582 Batch (t): 1.527, 505.936/s, 84.3227/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.1939 (3.3219) Dstill: 2.6102 (3.0726) Loss: 5.8041 (6.3945)
INFO:root:Train Epoch: 4 [ 559398/1859739.0 (30%)] Data (t): 0.576 Batch (t): 1.528, 546.895/s, 91.1492/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3052 (3.3208) Dstill: 2.7038 (3.0480) Loss: 6.0090 (6.3688)
2025-02-05,04:05:45 | INFO | Train Epoch: 4 [ 559398/1859739.0 (30%)] Data (t): 0.576 Batch (t): 1.528, 546.895/s, 91.1492/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3052 (3.3208) Dstill: 2.7038 (3.0480) Loss: 6.0090 (6.3688)
INFO:root:Train Epoch: 4 [ 599298/1859739.0 (32%)] Data (t): 0.615 Batch (t): 1.568, 484.336/s, 80.7227/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2119 (3.3140) Dstill: 2.7172 (3.0273) Loss: 5.9291 (6.3413)
2025-02-05,04:07:03 | INFO | Train Epoch: 4 [ 599298/1859739.0 (32%)] Data (t): 0.615 Batch (t): 1.568, 484.336/s, 80.7227/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2119 (3.3140) Dstill: 2.7172 (3.0273) Loss: 5.9291 (6.3413)
INFO:root:Train Epoch: 4 [ 639198/1859739.0 (34%)] Data (t): 0.607 Batch (t): 1.551, 479.731/s, 79.9551/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2569 (3.3106) Dstill: 2.7946 (3.0137) Loss: 6.0515 (6.3243)
2025-02-05,04:08:21 | INFO | Train Epoch: 4 [ 639198/1859739.0 (34%)] Data (t): 0.607 Batch (t): 1.551, 479.731/s, 79.9551/s/gpu LR: 0.000099 Logit Scale: 99.995 Contrastive_loss: 3.2569 (3.3106) Dstill: 2.7946 (3.0137) Loss: 6.0515 (6.3243)
INFO:root:Train Epoch: 4 [ 679098/1859739.0 (37%)] Data (t): 0.594 Batch (t): 1.549, 482.674/s, 80.4457/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.0634 (3.2969) Dstill: 2.6486 (2.9934) Loss: 5.7119 (6.2903)
2025-02-05,04:09:38 | INFO | Train Epoch: 4 [ 679098/1859739.0 (37%)] Data (t): 0.594 Batch (t): 1.549, 482.674/s, 80.4457/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.0634 (3.2969) Dstill: 2.6486 (2.9934) Loss: 5.7119 (6.2903)
INFO:root:Train Epoch: 4 [ 718998/1859739.0 (39%)] Data (t): 0.595 Batch (t): 1.570, 456.785/s, 76.1308/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2720 (3.2956) Dstill: 2.6630 (2.9760) Loss: 5.9350 (6.2716)
2025-02-05,04:10:57 | INFO | Train Epoch: 4 [ 718998/1859739.0 (39%)] Data (t): 0.595 Batch (t): 1.570, 456.785/s, 76.1308/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.2720 (3.2956) Dstill: 2.6630 (2.9760) Loss: 5.9350 (6.2716)
INFO:root:Train Epoch: 4 [ 758898/1859739.0 (41%)] Data (t): 0.574 Batch (t): 1.559, 542.121/s, 90.3535/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.3852 (3.3001) Dstill: 2.7251 (2.9634) Loss: 6.1104 (6.2635)
2025-02-05,04:12:15 | INFO | Train Epoch: 4 [ 758898/1859739.0 (41%)] Data (t): 0.574 Batch (t): 1.559, 542.121/s, 90.3535/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.3852 (3.3001) Dstill: 2.7251 (2.9634) Loss: 6.1104 (6.2635)
INFO:root:Train Epoch: 4 [ 798798/1859739.0 (43%)] Data (t): 0.601 Batch (t): 1.563, 504.221/s, 84.0368/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 2.9272 (3.2823) Dstill: 2.7015 (2.9510) Loss: 5.6286 (6.2333)
2025-02-05,04:13:33 | INFO | Train Epoch: 4 [ 798798/1859739.0 (43%)] Data (t): 0.601 Batch (t): 1.563, 504.221/s, 84.0368/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 2.9272 (3.2823) Dstill: 2.7015 (2.9510) Loss: 5.6286 (6.2333)
INFO:root:Train Epoch: 4 [ 838698/1859739.0 (45%)] Data (t): 0.589 Batch (t): 1.536, 535.518/s, 89.2529/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.3542 (3.2856) Dstill: 2.7217 (2.9405) Loss: 6.0758 (6.2261)
2025-02-05,04:14:50 | INFO | Train Epoch: 4 [ 838698/1859739.0 (45%)] Data (t): 0.589 Batch (t): 1.536, 535.518/s, 89.2529/s/gpu LR: 0.000099 Logit Scale: 99.999 Contrastive_loss: 3.3542 (3.2856) Dstill: 2.7217 (2.9405) Loss: 6.0758 (6.2261)
INFO:root:Train Epoch: 4 [ 878598/1859739.0 (47%)] Data (t): 0.599 Batch (t): 1.576, 537.001/s, 89.5001/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 2.9464 (3.2708) Dstill: 2.6516 (2.9280) Loss: 5.5980 (6.1988)
2025-02-05,04:16:08 | INFO | Train Epoch: 4 [ 878598/1859739.0 (47%)] Data (t): 0.599 Batch (t): 1.576, 537.001/s, 89.5001/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 2.9464 (3.2708) Dstill: 2.6516 (2.9280) Loss: 5.5980 (6.1988)
INFO:root:Train Epoch: 4 [ 918498/1859739.0 (49%)] Data (t): 0.617 Batch (t): 1.548, 550.714/s, 91.7857/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.2978 (3.2720) Dstill: 2.5471 (2.9121) Loss: 5.8449 (6.1841)
2025-02-05,04:17:26 | INFO | Train Epoch: 4 [ 918498/1859739.0 (49%)] Data (t): 0.617 Batch (t): 1.548, 550.714/s, 91.7857/s/gpu LR: 0.000099 Logit Scale: 99.996 Contrastive_loss: 3.2978 (3.2720) Dstill: 2.5471 (2.9121) Loss: 5.8449 (6.1841)
INFO:root:Train Epoch: 4 [ 958398/1859739.0 (52%)] Data (t): 0.606 Batch (t): 1.540, 494.756/s, 82.4593/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3104 (3.2735) Dstill: 2.6833 (2.9030) Loss: 5.9937 (6.1765)
2025-02-05,04:18:43 | INFO | Train Epoch: 4 [ 958398/1859739.0 (52%)] Data (t): 0.606 Batch (t): 1.540, 494.756/s, 82.4593/s/gpu LR: 0.000099 Logit Scale: 100.000 Contrastive_loss: 3.3104 (3.2735) Dstill: 2.6833 (2.9030) Loss: 5.9937 (6.1765)
INFO:root:Train Epoch: 4 [ 998298/1859739.0 (54%)] Data (t): 0.617 Batch (t): 1.560, 511.462/s, 85.2436/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.2881 (3.2741) Dstill: 2.6935 (2.8949) Loss: 5.9816 (6.1690)
2025-02-05,04:20:01 | INFO | Train Epoch: 4 [ 998298/1859739.0 (54%)] Data (t): 0.617 Batch (t): 1.560, 511.462/s, 85.2436/s/gpu LR: 0.000099 Logit Scale: 99.998 Contrastive_loss: 3.2881 (3.2741) Dstill: 2.6935 (2.8949) Loss: 5.9816 (6.1690)
INFO:root:Train Epoch: 4 [1038198/1859739.0 (56%)] Data (t): 0.595 Batch (t): 1.557, 531.129/s, 88.5215/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9743 (3.2630) Dstill: 2.6222 (2.8848) Loss: 5.5965 (6.1478)
2025-02-05,04:21:19 | INFO | Train Epoch: 4 [1038198/1859739.0 (56%)] Data (t): 0.595 Batch (t): 1.557, 531.129/s, 88.5215/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9743 (3.2630) Dstill: 2.6222 (2.8848) Loss: 5.5965 (6.1478)
INFO:root:Train Epoch: 4 [1078098/1859739.0 (58%)] Data (t): 0.616 Batch (t): 1.545, 542.642/s, 90.4404/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9558 (3.2520) Dstill: 2.7104 (2.8786) Loss: 5.6662 (6.1306)
2025-02-05,04:22:36 | INFO | Train Epoch: 4 [1078098/1859739.0 (58%)] Data (t): 0.616 Batch (t): 1.545, 542.642/s, 90.4404/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9558 (3.2520) Dstill: 2.7104 (2.8786) Loss: 5.6662 (6.1306)
INFO:root:Train Epoch: 4 [1117998/1859739.0 (60%)] Data (t): 0.613 Batch (t): 1.546, 511.188/s, 85.1981/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.3061 (3.2539) Dstill: 2.6622 (2.8711) Loss: 5.9682 (6.1250)
2025-02-05,04:23:53 | INFO | Train Epoch: 4 [1117998/1859739.0 (60%)] Data (t): 0.613 Batch (t): 1.546, 511.188/s, 85.1981/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.3061 (3.2539) Dstill: 2.6622 (2.8711) Loss: 5.9682 (6.1250)
INFO:root:Train Epoch: 4 [1157898/1859739.0 (62%)] Data (t): 0.630 Batch (t): 1.590, 542.164/s, 90.3606/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1454 (3.2502) Dstill: 2.7043 (2.8655) Loss: 5.8497 (6.1158)
2025-02-05,04:25:13 | INFO | Train Epoch: 4 [1157898/1859739.0 (62%)] Data (t): 0.630 Batch (t): 1.590, 542.164/s, 90.3606/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1454 (3.2502) Dstill: 2.7043 (2.8655) Loss: 5.8497 (6.1158)
INFO:root:Train Epoch: 4 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.544, 544.247/s, 90.7078/s/gpu LR: 0.000098 Logit Scale: 99.986 Contrastive_loss: 3.5924 (3.2613) Dstill: 2.6508 (2.8586) Loss: 6.2432 (6.1199)
2025-02-05,04:26:30 | INFO | Train Epoch: 4 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.544, 544.247/s, 90.7078/s/gpu LR: 0.000098 Logit Scale: 99.986 Contrastive_loss: 3.5924 (3.2613) Dstill: 2.6508 (2.8586) Loss: 6.2432 (6.1199)
INFO:root:Train Epoch: 4 [1237698/1859739.0 (67%)] Data (t): 0.608 Batch (t): 1.538, 517.769/s, 86.2949/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2024 (3.2594) Dstill: 2.6153 (2.8510) Loss: 5.8178 (6.1105)
2025-02-05,04:27:47 | INFO | Train Epoch: 4 [1237698/1859739.0 (67%)] Data (t): 0.608 Batch (t): 1.538, 517.769/s, 86.2949/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2024 (3.2594) Dstill: 2.6153 (2.8510) Loss: 5.8178 (6.1105)
INFO:root:Train Epoch: 4 [1277598/1859739.0 (69%)] Data (t): 0.487 Batch (t): 1.384, 550.045/s, 91.6741/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2854 (3.2602) Dstill: 2.6922 (2.8462) Loss: 5.9776 (6.1064)
2025-02-05,04:28:56 | INFO | Train Epoch: 4 [1277598/1859739.0 (69%)] Data (t): 0.487 Batch (t): 1.384, 550.045/s, 91.6741/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2854 (3.2602) Dstill: 2.6922 (2.8462) Loss: 5.9776 (6.1064)
INFO:root:Train Epoch: 4 [1317498/1859739.0 (71%)] Data (t): 0.524 Batch (t): 1.419, 562.614/s, 93.7690/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9873 (3.2522) Dstill: 2.6615 (2.8408) Loss: 5.6488 (6.0930)
2025-02-05,04:30:07 | INFO | Train Epoch: 4 [1317498/1859739.0 (71%)] Data (t): 0.524 Batch (t): 1.419, 562.614/s, 93.7690/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9873 (3.2522) Dstill: 2.6615 (2.8408) Loss: 5.6488 (6.0930)
INFO:root:Train Epoch: 4 [1357398/1859739.0 (73%)] Data (t): 0.562 Batch (t): 1.453, 557.624/s, 92.9374/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1492 (3.2493) Dstill: 2.6896 (2.8365) Loss: 5.8389 (6.0857)
2025-02-05,04:31:20 | INFO | Train Epoch: 4 [1357398/1859739.0 (73%)] Data (t): 0.562 Batch (t): 1.453, 557.624/s, 92.9374/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1492 (3.2493) Dstill: 2.6896 (2.8365) Loss: 5.8389 (6.0857)
INFO:root:Train Epoch: 4 [1397298/1859739.0 (75%)] Data (t): 0.631 Batch (t): 1.591, 496.319/s, 82.7199/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.2478 (3.2492) Dstill: 2.5917 (2.8297) Loss: 5.8395 (6.0789)
2025-02-05,04:32:39 | INFO | Train Epoch: 4 [1397298/1859739.0 (75%)] Data (t): 0.631 Batch (t): 1.591, 496.319/s, 82.7199/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.2478 (3.2492) Dstill: 2.5917 (2.8297) Loss: 5.8395 (6.0789)
INFO:root:Train Epoch: 4 [1437198/1859739.0 (77%)] Data (t): 0.640 Batch (t): 1.600, 489.673/s, 81.6121/s/gpu LR: 0.000098 Logit Scale: 99.997 Contrastive_loss: 3.1476 (3.2465) Dstill: 2.6346 (2.8244) Loss: 5.7822 (6.0709)
2025-02-05,04:33:59 | INFO | Train Epoch: 4 [1437198/1859739.0 (77%)] Data (t): 0.640 Batch (t): 1.600, 489.673/s, 81.6121/s/gpu LR: 0.000098 Logit Scale: 99.997 Contrastive_loss: 3.1476 (3.2465) Dstill: 2.6346 (2.8244) Loss: 5.7822 (6.0709)
INFO:root:Train Epoch: 4 [1477098/1859739.0 (79%)] Data (t): 0.627 Batch (t): 1.588, 496.244/s, 82.7074/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9790 (3.2394) Dstill: 2.6226 (2.8191) Loss: 5.6016 (6.0585)
2025-02-05,04:35:19 | INFO | Train Epoch: 4 [1477098/1859739.0 (79%)] Data (t): 0.627 Batch (t): 1.588, 496.244/s, 82.7074/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9790 (3.2394) Dstill: 2.6226 (2.8191) Loss: 5.6016 (6.0585)
INFO:root:Train Epoch: 4 [1516998/1859739.0 (82%)] Data (t): 0.611 Batch (t): 1.553, 571.858/s, 95.3097/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0699 (3.2351) Dstill: 2.6122 (2.8138) Loss: 5.6821 (6.0489)
2025-02-05,04:36:36 | INFO | Train Epoch: 4 [1516998/1859739.0 (82%)] Data (t): 0.611 Batch (t): 1.553, 571.858/s, 95.3097/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0699 (3.2351) Dstill: 2.6122 (2.8138) Loss: 5.6821 (6.0489)
INFO:root:Train Epoch: 4 [1556898/1859739.0 (84%)] Data (t): 0.612 Batch (t): 1.573, 538.937/s, 89.8228/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0495 (3.2304) Dstill: 2.6067 (2.8086) Loss: 5.6562 (6.0390)
2025-02-05,04:37:55 | INFO | Train Epoch: 4 [1556898/1859739.0 (84%)] Data (t): 0.612 Batch (t): 1.573, 538.937/s, 89.8228/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0495 (3.2304) Dstill: 2.6067 (2.8086) Loss: 5.6562 (6.0390)
INFO:root:Train Epoch: 4 [1596798/1859739.0 (86%)] Data (t): 0.626 Batch (t): 1.595, 512.711/s, 85.4518/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.1527 (3.2285) Dstill: 2.5795 (2.8030) Loss: 5.7322 (6.0316)
2025-02-05,04:39:15 | INFO | Train Epoch: 4 [1596798/1859739.0 (86%)] Data (t): 0.626 Batch (t): 1.595, 512.711/s, 85.4518/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.1527 (3.2285) Dstill: 2.5795 (2.8030) Loss: 5.7322 (6.0316)
INFO:root:Train Epoch: 4 [1636698/1859739.0 (88%)] Data (t): 0.638 Batch (t): 1.570, 485.233/s, 80.8722/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.3188 (3.2307) Dstill: 2.5753 (2.7976) Loss: 5.8942 (6.0283)
2025-02-05,04:40:33 | INFO | Train Epoch: 4 [1636698/1859739.0 (88%)] Data (t): 0.638 Batch (t): 1.570, 485.233/s, 80.8722/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.3188 (3.2307) Dstill: 2.5753 (2.7976) Loss: 5.8942 (6.0283)
INFO:root:Train Epoch: 4 [1676598/1859739.0 (90%)] Data (t): 0.611 Batch (t): 1.605, 497.544/s, 82.9239/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.4000 (3.2346) Dstill: 2.7120 (2.7956) Loss: 6.1120 (6.0302)
2025-02-05,04:41:53 | INFO | Train Epoch: 4 [1676598/1859739.0 (90%)] Data (t): 0.611 Batch (t): 1.605, 497.544/s, 82.9239/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.4000 (3.2346) Dstill: 2.7120 (2.7956) Loss: 6.1120 (6.0302)
INFO:root:Train Epoch: 4 [1716498/1859739.0 (92%)] Data (t): 0.592 Batch (t): 1.625, 525.742/s, 87.6237/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2907 (3.2359) Dstill: 2.5810 (2.7907) Loss: 5.8717 (6.0266)
2025-02-05,04:43:15 | INFO | Train Epoch: 4 [1716498/1859739.0 (92%)] Data (t): 0.592 Batch (t): 1.625, 525.742/s, 87.6237/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2907 (3.2359) Dstill: 2.5810 (2.7907) Loss: 5.8717 (6.0266)
INFO:root:Train Epoch: 4 [1756398/1859739.0 (94%)] Data (t): 0.590 Batch (t): 1.610, 474.853/s, 79.1422/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.3565 (3.2386) Dstill: 2.5665 (2.7857) Loss: 5.9230 (6.0243)
2025-02-05,04:44:35 | INFO | Train Epoch: 4 [1756398/1859739.0 (94%)] Data (t): 0.590 Batch (t): 1.610, 474.853/s, 79.1422/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.3565 (3.2386) Dstill: 2.5665 (2.7857) Loss: 5.9230 (6.0243)
INFO:root:Train Epoch: 4 [1796298/1859739.0 (97%)] Data (t): 0.567 Batch (t): 1.600, 506.850/s, 84.4749/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.4813 (3.2439) Dstill: 2.6603 (2.7830) Loss: 6.1416 (6.0269)
2025-02-05,04:45:55 | INFO | Train Epoch: 4 [1796298/1859739.0 (97%)] Data (t): 0.567 Batch (t): 1.600, 506.850/s, 84.4749/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.4813 (3.2439) Dstill: 2.6603 (2.7830) Loss: 6.1416 (6.0269)
INFO:root:Train Epoch: 4 [1836198/1859739.0 (99%)] Data (t): 0.607 Batch (t): 1.629, 501.398/s, 83.5663/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2856 (3.2448) Dstill: 2.7438 (2.7822) Loss: 6.0294 (6.0269)
2025-02-05,04:47:16 | INFO | Train Epoch: 4 [1836198/1859739.0 (99%)] Data (t): 0.607 Batch (t): 1.629, 501.398/s, 83.5663/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2856 (3.2448) Dstill: 2.7438 (2.7822) Loss: 6.0294 (6.0269)
INFO:root:Train Epoch: 4 [1860138/1859739.0 (100%)] Data (t): 0.603 Batch (t): 1.611, 512.280/s, 85.3800/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0664 (3.2410) Dstill: 2.8963 (2.7846) Loss: 5.9627 (6.0256)
2025-02-05,04:48:05 | INFO | Train Epoch: 4 [1860138/1859739.0 (100%)] Data (t): 0.603 Batch (t): 1.611, 512.280/s, 85.3800/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0664 (3.2410) Dstill: 2.8963 (2.7846) Loss: 5.9627 (6.0256)
INFO:root:Train Epoch: 4 [  798/59975.0 (1%)] Data (t): 0.579 Batch (t): 1.512, 527.613/s, 87.9354/s/gpu LR: 0.000031 Logit Scale: 100.000 Contrastive_loss: 7.7207 (7.7207) Dstill: 9.2119 (9.2119) Loss: 16.933 (16.933)
2025-02-05,04:48:06 | INFO | Train Epoch: 4 [  798/59975.0 (1%)] Data (t): 0.579 Batch (t): 1.512, 527.613/s, 87.9354/s/gpu LR: 0.000031 Logit Scale: 100.000 Contrastive_loss: 7.7207 (7.7207) Dstill: 9.2119 (9.2119) Loss: 16.933 (16.933)
INFO:root:Train Epoch: 4 [40698/59975.0 (67%)] Data (t): 0.500 Batch (t): 1.417, 591.663/s, 98.6105/s/gpu LR: 0.000036 Logit Scale: 99.991 Contrastive_loss: 6.9571 (7.3389) Dstill: 6.1596 (7.6857) Loss: 13.117 (15.025)
2025-02-05,04:49:17 | INFO | Train Epoch: 4 [40698/59975.0 (67%)] Data (t): 0.500 Batch (t): 1.417, 591.663/s, 98.6105/s/gpu LR: 0.000036 Logit Scale: 99.991 Contrastive_loss: 6.9571 (7.3389) Dstill: 6.1596 (7.6857) Loss: 13.117 (15.025)
INFO:root:Train Epoch: 4 [60648/59975.0 (100%)] Data (t): 0.471 Batch (t): 1.429, 606.862/s, 101.144/s/gpu LR: 0.000038 Logit Scale: 99.988 Contrastive_loss: 6.7418 (7.1399) Dstill: 5.9110 (7.0941) Loss: 12.653 (14.234)
2025-02-05,04:49:53 | INFO | Train Epoch: 4 [60648/59975.0 (100%)] Data (t): 0.471 Batch (t): 1.429, 606.862/s, 101.144/s/gpu LR: 0.000038 Logit Scale: 99.988 Contrastive_loss: 6.7418 (7.1399) Dstill: 5.9110 (7.0941) Loss: 12.653 (14.234)
INFO:root:Train Epoch: 4 [  798/47810.0 (2%)] Data (t): 2.462 Batch (t): 3.362, 237.385/s, 39.5642/s/gpu LR: 0.000024 Logit Scale: 99.988 Contrastive_loss: 7.8361 (7.8361) Dstill: 11.289 (11.289) Loss: 19.125 (19.125)
2025-02-05,04:49:56 | INFO | Train Epoch: 4 [  798/47810.0 (2%)] Data (t): 2.462 Batch (t): 3.362, 237.385/s, 39.5642/s/gpu LR: 0.000024 Logit Scale: 99.988 Contrastive_loss: 7.8361 (7.8361) Dstill: 11.289 (11.289) Loss: 19.125 (19.125)
INFO:root:Train Epoch: 4 [40698/47810.0 (85%)] Data (t): 2.228 Batch (t): 3.176, 247.189/s, 41.1981/s/gpu LR: 0.000029 Logit Scale: 99.983 Contrastive_loss: 6.8834 (7.3597) Dstill: 6.7158 (9.0023) Loss: 13.599 (16.362)
2025-02-05,04:52:35 | INFO | Train Epoch: 4 [40698/47810.0 (85%)] Data (t): 2.228 Batch (t): 3.176, 247.189/s, 41.1981/s/gpu LR: 0.000029 Logit Scale: 99.983 Contrastive_loss: 6.8834 (7.3597) Dstill: 6.7158 (9.0023) Loss: 13.599 (16.362)
INFO:root:Train Epoch: 4 [47880/47810.0 (100%)] Data (t): 2.253 Batch (t): 3.178, 250.814/s, 41.8024/s/gpu LR: 0.000030 Logit Scale: 99.983 Contrastive_loss: 6.4643 (7.0613) Dstill: 6.5979 (8.2008) Loss: 13.062 (15.262)
2025-02-05,04:53:04 | INFO | Train Epoch: 4 [47880/47810.0 (100%)] Data (t): 2.253 Batch (t): 3.178, 250.814/s, 41.8024/s/gpu LR: 0.000030 Logit Scale: 99.983 Contrastive_loss: 6.4643 (7.0613) Dstill: 6.5979 (8.2008) Loss: 13.062 (15.262)
INFO:root:Train Epoch: 4 [ 798/7500.0 (10%)] Data (t): 38.251 Batch (t): 46.078, 17.3186/s, 2.88644/s/gpu LR: 0.000004 Logit Scale: 99.983 Contrastive_loss: 8.0196 (8.0196) Dstill: 11.168 (11.168) Loss: 19.187 (19.187)
2025-02-05,04:53:50 | INFO | Train Epoch: 4 [ 798/7500.0 (10%)] Data (t): 38.251 Batch (t): 46.078, 17.3186/s, 2.88644/s/gpu LR: 0.000004 Logit Scale: 99.983 Contrastive_loss: 8.0196 (8.0196) Dstill: 11.168 (11.168) Loss: 19.187 (19.187)
INFO:root:Train Epoch: 4 [7980/7500.0 (100%)] Data (t): 34.141 Batch (t): 41.899, 18.2982/s, 3.04971/s/gpu LR: 0.000005 Logit Scale: 99.981 Contrastive_loss: 7.8088 (7.9142) Dstill: 9.8775 (10.523) Loss: 17.686 (18.437)
2025-02-05,05:00:07 | INFO | Train Epoch: 4 [7980/7500.0 (100%)] Data (t): 34.141 Batch (t): 41.899, 18.2982/s, 3.04971/s/gpu LR: 0.000005 Logit Scale: 99.981 Contrastive_loss: 7.8088 (7.9142) Dstill: 9.8775 (10.523) Loss: 17.686 (18.437)
INFO:root:Start epoch 5
2025-02-05,05:00:12 | INFO | Start epoch 5
INFO:root:Train Epoch: 5 [    798/1859739.0 (0%)] Data (t): 0.302 Batch (t): 1.212, 658.256/s, 109.709/s/gpu LR: 0.000098 Logit Scale: 99.981 Contrastive_loss: 4.6514 (4.6514) Dstill: 5.6515 (5.6515) Loss: 10.303 (10.303)
2025-02-05,05:00:13 | INFO | Train Epoch: 5 [    798/1859739.0 (0%)] Data (t): 0.302 Batch (t): 1.212, 658.256/s, 109.709/s/gpu LR: 0.000098 Logit Scale: 99.981 Contrastive_loss: 4.6514 (4.6514) Dstill: 5.6515 (5.6515) Loss: 10.303 (10.303)
INFO:root:Train Epoch: 5 [  40698/1859739.0 (2%)] Data (t): 0.466 Batch (t): 1.405, 542.432/s, 90.4053/s/gpu LR: 0.000098 Logit Scale: 99.943 Contrastive_loss: 3.0533 (3.8524) Dstill: 3.1101 (4.3808) Loss: 6.1634 (8.2332)
2025-02-05,05:01:24 | INFO | Train Epoch: 5 [  40698/1859739.0 (2%)] Data (t): 0.466 Batch (t): 1.405, 542.432/s, 90.4053/s/gpu LR: 0.000098 Logit Scale: 99.943 Contrastive_loss: 3.0533 (3.8524) Dstill: 3.1101 (4.3808) Loss: 6.1634 (8.2332)
INFO:root:Train Epoch: 5 [  80598/1859739.0 (4%)] Data (t): 0.454 Batch (t): 1.405, 577.709/s, 96.2848/s/gpu LR: 0.000098 Logit Scale: 99.961 Contrastive_loss: 3.1662 (3.6236) Dstill: 2.8610 (3.8742) Loss: 6.0272 (7.4978)
2025-02-05,05:02:34 | INFO | Train Epoch: 5 [  80598/1859739.0 (4%)] Data (t): 0.454 Batch (t): 1.405, 577.709/s, 96.2848/s/gpu LR: 0.000098 Logit Scale: 99.961 Contrastive_loss: 3.1662 (3.6236) Dstill: 2.8610 (3.8742) Loss: 6.0272 (7.4978)
INFO:root:Train Epoch: 5 [ 120498/1859739.0 (6%)] Data (t): 0.472 Batch (t): 1.402, 625.350/s, 104.225/s/gpu LR: 0.000098 Logit Scale: 99.982 Contrastive_loss: 2.9958 (3.4667) Dstill: 2.7794 (3.6005) Loss: 5.7752 (7.0672)
2025-02-05,05:03:44 | INFO | Train Epoch: 5 [ 120498/1859739.0 (6%)] Data (t): 0.472 Batch (t): 1.402, 625.350/s, 104.225/s/gpu LR: 0.000098 Logit Scale: 99.982 Contrastive_loss: 2.9958 (3.4667) Dstill: 2.7794 (3.6005) Loss: 5.7752 (7.0672)
INFO:root:Train Epoch: 5 [ 160398/1859739.0 (9%)] Data (t): 0.509 Batch (t): 1.443, 535.700/s, 89.2833/s/gpu LR: 0.000098 Logit Scale: 99.995 Contrastive_loss: 2.9143 (3.3562) Dstill: 2.8047 (3.4413) Loss: 5.7190 (6.7975)
2025-02-05,05:04:56 | INFO | Train Epoch: 5 [ 160398/1859739.0 (9%)] Data (t): 0.509 Batch (t): 1.443, 535.700/s, 89.2833/s/gpu LR: 0.000098 Logit Scale: 99.995 Contrastive_loss: 2.9143 (3.3562) Dstill: 2.8047 (3.4413) Loss: 5.7190 (6.7975)
INFO:root:Train Epoch: 5 [ 200298/1859739.0 (11%)] Data (t): 0.572 Batch (t): 1.495, 546.990/s, 91.1650/s/gpu LR: 0.000098 Logit Scale: 99.992 Contrastive_loss: 3.4413 (3.3704) Dstill: 2.6100 (3.3028) Loss: 6.0513 (6.6732)
2025-02-05,05:06:11 | INFO | Train Epoch: 5 [ 200298/1859739.0 (11%)] Data (t): 0.572 Batch (t): 1.495, 546.990/s, 91.1650/s/gpu LR: 0.000098 Logit Scale: 99.992 Contrastive_loss: 3.4413 (3.3704) Dstill: 2.6100 (3.3028) Loss: 6.0513 (6.6732)
INFO:root:Train Epoch: 5 [ 240198/1859739.0 (13%)] Data (t): 0.571 Batch (t): 1.508, 569.106/s, 94.8510/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3231 (3.3636) Dstill: 2.7271 (3.2205) Loss: 6.0503 (6.5842)
2025-02-05,05:07:26 | INFO | Train Epoch: 5 [ 240198/1859739.0 (13%)] Data (t): 0.571 Batch (t): 1.508, 569.106/s, 94.8510/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3231 (3.3636) Dstill: 2.7271 (3.2205) Loss: 6.0503 (6.5842)
INFO:root:Train Epoch: 5 [ 280098/1859739.0 (15%)] Data (t): 0.584 Batch (t): 1.531, 486.513/s, 81.0855/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 2.9356 (3.3101) Dstill: 2.6502 (3.1492) Loss: 5.5858 (6.4594)
2025-02-05,05:08:43 | INFO | Train Epoch: 5 [ 280098/1859739.0 (15%)] Data (t): 0.584 Batch (t): 1.531, 486.513/s, 81.0855/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 2.9356 (3.3101) Dstill: 2.6502 (3.1492) Loss: 5.5858 (6.4594)
INFO:root:Train Epoch: 5 [ 319998/1859739.0 (17%)] Data (t): 0.624 Batch (t): 1.545, 513.214/s, 85.5357/s/gpu LR: 0.000098 Logit Scale: 99.986 Contrastive_loss: 3.0116 (3.2770) Dstill: 2.7452 (3.1043) Loss: 5.7568 (6.3813)
2025-02-05,05:10:00 | INFO | Train Epoch: 5 [ 319998/1859739.0 (17%)] Data (t): 0.624 Batch (t): 1.545, 513.214/s, 85.5357/s/gpu LR: 0.000098 Logit Scale: 99.986 Contrastive_loss: 3.0116 (3.2770) Dstill: 2.7452 (3.1043) Loss: 5.7568 (6.3813)
INFO:root:Train Epoch: 5 [ 359898/1859739.0 (19%)] Data (t): 0.576 Batch (t): 1.528, 539.518/s, 89.9197/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1844 (3.2677) Dstill: 2.6389 (3.0578) Loss: 5.8233 (6.3255)
2025-02-05,05:11:17 | INFO | Train Epoch: 5 [ 359898/1859739.0 (19%)] Data (t): 0.576 Batch (t): 1.528, 539.518/s, 89.9197/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1844 (3.2677) Dstill: 2.6389 (3.0578) Loss: 5.8233 (6.3255)
INFO:root:Train Epoch: 5 [ 399798/1859739.0 (21%)] Data (t): 0.606 Batch (t): 1.541, 491.476/s, 81.9127/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.2095 (3.2624) Dstill: 2.6605 (3.0217) Loss: 5.8700 (6.2841)
2025-02-05,05:12:34 | INFO | Train Epoch: 5 [ 399798/1859739.0 (21%)] Data (t): 0.606 Batch (t): 1.541, 491.476/s, 81.9127/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.2095 (3.2624) Dstill: 2.6605 (3.0217) Loss: 5.8700 (6.2841)
INFO:root:Train Epoch: 5 [ 439698/1859739.0 (24%)] Data (t): 0.597 Batch (t): 1.545, 457.874/s, 76.3123/s/gpu LR: 0.000098 Logit Scale: 99.995 Contrastive_loss: 3.1443 (3.2526) Dstill: 2.8836 (3.0102) Loss: 6.0279 (6.2628)
2025-02-05,05:13:51 | INFO | Train Epoch: 5 [ 439698/1859739.0 (24%)] Data (t): 0.597 Batch (t): 1.545, 457.874/s, 76.3123/s/gpu LR: 0.000098 Logit Scale: 99.995 Contrastive_loss: 3.1443 (3.2526) Dstill: 2.8836 (3.0102) Loss: 6.0279 (6.2628)
INFO:root:Train Epoch: 5 [ 479598/1859739.0 (26%)] Data (t): 0.606 Batch (t): 1.553, 567.164/s, 94.5273/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.0845 (3.2397) Dstill: 2.5966 (2.9784) Loss: 5.6810 (6.2180)
2025-02-05,05:15:09 | INFO | Train Epoch: 5 [ 479598/1859739.0 (26%)] Data (t): 0.606 Batch (t): 1.553, 567.164/s, 94.5273/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.0845 (3.2397) Dstill: 2.5966 (2.9784) Loss: 5.6810 (6.2180)
INFO:root:Train Epoch: 5 [ 519498/1859739.0 (28%)] Data (t): 0.571 Batch (t): 1.508, 505.789/s, 84.2981/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0558 (3.2265) Dstill: 2.5879 (2.9505) Loss: 5.6437 (6.1770)
2025-02-05,05:16:24 | INFO | Train Epoch: 5 [ 519498/1859739.0 (28%)] Data (t): 0.571 Batch (t): 1.508, 505.789/s, 84.2981/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0558 (3.2265) Dstill: 2.5879 (2.9505) Loss: 5.6437 (6.1770)
INFO:root:Train Epoch: 5 [ 559398/1859739.0 (30%)] Data (t): 0.567 Batch (t): 1.530, 535.715/s, 89.2858/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3040 (3.2317) Dstill: 2.6780 (2.9323) Loss: 5.9820 (6.1640)
2025-02-05,05:17:40 | INFO | Train Epoch: 5 [ 559398/1859739.0 (30%)] Data (t): 0.567 Batch (t): 1.530, 535.715/s, 89.2858/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3040 (3.2317) Dstill: 2.6780 (2.9323) Loss: 5.9820 (6.1640)
INFO:root:Train Epoch: 5 [ 599298/1859739.0 (32%)] Data (t): 0.630 Batch (t): 1.589, 468.944/s, 78.1574/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1521 (3.2267) Dstill: 2.6585 (2.9152) Loss: 5.8106 (6.1419)
2025-02-05,05:19:00 | INFO | Train Epoch: 5 [ 599298/1859739.0 (32%)] Data (t): 0.630 Batch (t): 1.589, 468.944/s, 78.1574/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1521 (3.2267) Dstill: 2.6585 (2.9152) Loss: 5.8106 (6.1419)
INFO:root:Train Epoch: 5 [ 639198/1859739.0 (34%)] Data (t): 0.606 Batch (t): 1.548, 487.448/s, 81.2414/s/gpu LR: 0.000098 Logit Scale: 99.994 Contrastive_loss: 3.2984 (3.2309) Dstill: 2.8385 (2.9107) Loss: 6.1370 (6.1416)
2025-02-05,05:20:17 | INFO | Train Epoch: 5 [ 639198/1859739.0 (34%)] Data (t): 0.606 Batch (t): 1.548, 487.448/s, 81.2414/s/gpu LR: 0.000098 Logit Scale: 99.994 Contrastive_loss: 3.2984 (3.2309) Dstill: 2.8385 (2.9107) Loss: 6.1370 (6.1416)
INFO:root:Train Epoch: 5 [ 679098/1859739.0 (37%)] Data (t): 0.593 Batch (t): 1.542, 480.361/s, 80.0602/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 2.9973 (3.2179) Dstill: 2.6060 (2.8938) Loss: 5.6033 (6.1117)
2025-02-05,05:21:34 | INFO | Train Epoch: 5 [ 679098/1859739.0 (37%)] Data (t): 0.593 Batch (t): 1.542, 480.361/s, 80.0602/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 2.9973 (3.2179) Dstill: 2.6060 (2.8938) Loss: 5.6033 (6.1117)
INFO:root:Train Epoch: 5 [ 718998/1859739.0 (39%)] Data (t): 0.609 Batch (t): 1.566, 461.885/s, 76.9808/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1465 (3.2142) Dstill: 2.5842 (2.8775) Loss: 5.7307 (6.0916)
2025-02-05,05:22:53 | INFO | Train Epoch: 5 [ 718998/1859739.0 (39%)] Data (t): 0.609 Batch (t): 1.566, 461.885/s, 76.9808/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1465 (3.2142) Dstill: 2.5842 (2.8775) Loss: 5.7307 (6.0916)
INFO:root:Train Epoch: 5 [ 758898/1859739.0 (41%)] Data (t): 0.601 Batch (t): 1.573, 541.255/s, 90.2092/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3798 (3.2225) Dstill: 2.7255 (2.8699) Loss: 6.1052 (6.0923)
2025-02-05,05:24:11 | INFO | Train Epoch: 5 [ 758898/1859739.0 (41%)] Data (t): 0.601 Batch (t): 1.573, 541.255/s, 90.2092/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 3.3798 (3.2225) Dstill: 2.7255 (2.8699) Loss: 6.1052 (6.0923)
INFO:root:Train Epoch: 5 [ 798798/1859739.0 (43%)] Data (t): 0.601 Batch (t): 1.553, 534.329/s, 89.0548/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.8192 (3.2033) Dstill: 2.5544 (2.8548) Loss: 5.3736 (6.0581)
2025-02-05,05:25:29 | INFO | Train Epoch: 5 [ 798798/1859739.0 (43%)] Data (t): 0.601 Batch (t): 1.553, 534.329/s, 89.0548/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.8192 (3.2033) Dstill: 2.5544 (2.8548) Loss: 5.3736 (6.0581)
INFO:root:Train Epoch: 5 [ 838698/1859739.0 (45%)] Data (t): 0.587 Batch (t): 1.532, 518.499/s, 86.4164/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1985 (3.2030) Dstill: 2.7244 (2.8489) Loss: 5.9228 (6.0520)
2025-02-05,05:26:46 | INFO | Train Epoch: 5 [ 838698/1859739.0 (45%)] Data (t): 0.587 Batch (t): 1.532, 518.499/s, 86.4164/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1985 (3.2030) Dstill: 2.7244 (2.8489) Loss: 5.9228 (6.0520)
INFO:root:Train Epoch: 5 [ 878598/1859739.0 (47%)] Data (t): 0.611 Batch (t): 1.567, 522.105/s, 87.0175/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.8789 (3.1890) Dstill: 2.6327 (2.8395) Loss: 5.5117 (6.0285)
2025-02-05,05:28:04 | INFO | Train Epoch: 5 [ 878598/1859739.0 (47%)] Data (t): 0.611 Batch (t): 1.567, 522.105/s, 87.0175/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.8789 (3.1890) Dstill: 2.6327 (2.8395) Loss: 5.5117 (6.0285)
INFO:root:Train Epoch: 5 [ 918498/1859739.0 (49%)] Data (t): 0.596 Batch (t): 1.530, 535.021/s, 89.1702/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.1482 (3.1873) Dstill: 2.5737 (2.8284) Loss: 5.7218 (6.0157)
2025-02-05,05:29:20 | INFO | Train Epoch: 5 [ 918498/1859739.0 (49%)] Data (t): 0.596 Batch (t): 1.530, 535.021/s, 89.1702/s/gpu LR: 0.000098 Logit Scale: 99.996 Contrastive_loss: 3.1482 (3.1873) Dstill: 2.5737 (2.8284) Loss: 5.7218 (6.0157)
INFO:root:Train Epoch: 5 [ 958398/1859739.0 (52%)] Data (t): 0.608 Batch (t): 1.544, 554.700/s, 92.4499/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1969 (3.1876) Dstill: 2.6845 (2.8227) Loss: 5.8814 (6.0103)
2025-02-05,05:30:38 | INFO | Train Epoch: 5 [ 958398/1859739.0 (52%)] Data (t): 0.608 Batch (t): 1.544, 554.700/s, 92.4499/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1969 (3.1876) Dstill: 2.6845 (2.8227) Loss: 5.8814 (6.0103)
INFO:root:Train Epoch: 5 [ 998298/1859739.0 (54%)] Data (t): 0.612 Batch (t): 1.557, 501.240/s, 83.5399/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.3184 (3.1927) Dstill: 2.7465 (2.8197) Loss: 6.0650 (6.0124)
2025-02-05,05:31:55 | INFO | Train Epoch: 5 [ 998298/1859739.0 (54%)] Data (t): 0.612 Batch (t): 1.557, 501.240/s, 83.5399/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.3184 (3.1927) Dstill: 2.7465 (2.8197) Loss: 6.0650 (6.0124)
INFO:root:Train Epoch: 5 [1038198/1859739.0 (56%)] Data (t): 0.598 Batch (t): 1.551, 512.872/s, 85.4786/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9170 (3.1825) Dstill: 2.6726 (2.8143) Loss: 5.5896 (5.9968)
2025-02-05,05:33:13 | INFO | Train Epoch: 5 [1038198/1859739.0 (56%)] Data (t): 0.598 Batch (t): 1.551, 512.872/s, 85.4786/s/gpu LR: 0.000098 Logit Scale: 99.999 Contrastive_loss: 2.9170 (3.1825) Dstill: 2.6726 (2.8143) Loss: 5.5896 (5.9968)
INFO:root:Train Epoch: 5 [1078098/1859739.0 (58%)] Data (t): 0.627 Batch (t): 1.545, 536.100/s, 89.3500/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9557 (3.1744) Dstill: 2.6374 (2.8080) Loss: 5.5931 (5.9823)
2025-02-05,05:34:30 | INFO | Train Epoch: 5 [1078098/1859739.0 (58%)] Data (t): 0.627 Batch (t): 1.545, 536.100/s, 89.3500/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 2.9557 (3.1744) Dstill: 2.6374 (2.8080) Loss: 5.5931 (5.9823)
INFO:root:Train Epoch: 5 [1117998/1859739.0 (60%)] Data (t): 0.621 Batch (t): 1.554, 490.879/s, 81.8132/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2267 (3.1762) Dstill: 2.6985 (2.8042) Loss: 5.9252 (5.9804)
2025-02-05,05:35:48 | INFO | Train Epoch: 5 [1117998/1859739.0 (60%)] Data (t): 0.621 Batch (t): 1.554, 490.879/s, 81.8132/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.2267 (3.1762) Dstill: 2.6985 (2.8042) Loss: 5.9252 (5.9804)
INFO:root:Train Epoch: 5 [1157898/1859739.0 (62%)] Data (t): 0.613 Batch (t): 1.589, 520.282/s, 86.7137/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0023 (3.1704) Dstill: 2.5375 (2.7953) Loss: 5.5398 (5.9657)
2025-02-05,05:37:07 | INFO | Train Epoch: 5 [1157898/1859739.0 (62%)] Data (t): 0.613 Batch (t): 1.589, 520.282/s, 86.7137/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0023 (3.1704) Dstill: 2.5375 (2.7953) Loss: 5.5398 (5.9657)
INFO:root:Train Epoch: 5 [1197798/1859739.0 (64%)] Data (t): 0.618 Batch (t): 1.546, 529.898/s, 88.3163/s/gpu LR: 0.000098 Logit Scale: 99.989 Contrastive_loss: 3.5209 (3.1817) Dstill: 2.6542 (2.7908) Loss: 6.1751 (5.9724)
2025-02-05,05:38:25 | INFO | Train Epoch: 5 [1197798/1859739.0 (64%)] Data (t): 0.618 Batch (t): 1.546, 529.898/s, 88.3163/s/gpu LR: 0.000098 Logit Scale: 99.989 Contrastive_loss: 3.5209 (3.1817) Dstill: 2.6542 (2.7908) Loss: 6.1751 (5.9724)
INFO:root:Train Epoch: 5 [1237698/1859739.0 (67%)] Data (t): 0.616 Batch (t): 1.542, 507.405/s, 84.5675/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1512 (3.1807) Dstill: 2.5291 (2.7826) Loss: 5.6803 (5.9633)
2025-02-05,05:39:42 | INFO | Train Epoch: 5 [1237698/1859739.0 (67%)] Data (t): 0.616 Batch (t): 1.542, 507.405/s, 84.5675/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.1512 (3.1807) Dstill: 2.5291 (2.7826) Loss: 5.6803 (5.9633)
INFO:root:Train Epoch: 5 [1277598/1859739.0 (69%)] Data (t): 0.630 Batch (t): 1.568, 532.748/s, 88.7914/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.3069 (3.1846) Dstill: 2.7426 (2.7814) Loss: 6.0495 (5.9659)
2025-02-05,05:41:00 | INFO | Train Epoch: 5 [1277598/1859739.0 (69%)] Data (t): 0.630 Batch (t): 1.568, 532.748/s, 88.7914/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.3069 (3.1846) Dstill: 2.7426 (2.7814) Loss: 6.0495 (5.9659)
INFO:root:Train Epoch: 5 [1317498/1859739.0 (71%)] Data (t): 0.443 Batch (t): 1.367, 600.210/s, 100.035/s/gpu LR: 0.000098 Logit Scale: 99.997 Contrastive_loss: 2.9255 (3.1769) Dstill: 2.6314 (2.7770) Loss: 5.5569 (5.9539)
2025-02-05,05:42:09 | INFO | Train Epoch: 5 [1317498/1859739.0 (71%)] Data (t): 0.443 Batch (t): 1.367, 600.210/s, 100.035/s/gpu LR: 0.000098 Logit Scale: 99.997 Contrastive_loss: 2.9255 (3.1769) Dstill: 2.6314 (2.7770) Loss: 5.5569 (5.9539)
INFO:root:Train Epoch: 5 [1357398/1859739.0 (73%)] Data (t): 0.401 Batch (t): 1.310, 628.108/s, 104.685/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0759 (3.1740) Dstill: 2.6931 (2.7746) Loss: 5.7690 (5.9486)
2025-02-05,05:43:14 | INFO | Train Epoch: 5 [1357398/1859739.0 (73%)] Data (t): 0.401 Batch (t): 1.310, 628.108/s, 104.685/s/gpu LR: 0.000098 Logit Scale: 100.000 Contrastive_loss: 3.0759 (3.1740) Dstill: 2.6931 (2.7746) Loss: 5.7690 (5.9486)
INFO:root:Train Epoch: 5 [1397298/1859739.0 (75%)] Data (t): 0.413 Batch (t): 1.329, 566.564/s, 94.4274/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.1349 (3.1730) Dstill: 2.6063 (2.7699) Loss: 5.7413 (5.9428)
2025-02-05,05:44:21 | INFO | Train Epoch: 5 [1397298/1859739.0 (75%)] Data (t): 0.413 Batch (t): 1.329, 566.564/s, 94.4274/s/gpu LR: 0.000098 Logit Scale: 99.998 Contrastive_loss: 3.1349 (3.1730) Dstill: 2.6063 (2.7699) Loss: 5.7413 (5.9428)
INFO:root:Train Epoch: 5 [1437198/1859739.0 (77%)] Data (t): 0.666 Batch (t): 1.587, 482.534/s, 80.4223/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.1152 (3.1714) Dstill: 2.6577 (2.7669) Loss: 5.7729 (5.9383)
2025-02-05,05:45:40 | INFO | Train Epoch: 5 [1437198/1859739.0 (77%)] Data (t): 0.666 Batch (t): 1.587, 482.534/s, 80.4223/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.1152 (3.1714) Dstill: 2.6577 (2.7669) Loss: 5.7729 (5.9383)
INFO:root:Train Epoch: 5 [1477098/1859739.0 (79%)] Data (t): 0.620 Batch (t): 1.587, 501.466/s, 83.5776/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9369 (3.1652) Dstill: 2.5735 (2.7618) Loss: 5.5104 (5.9270)
2025-02-05,05:46:59 | INFO | Train Epoch: 5 [1477098/1859739.0 (79%)] Data (t): 0.620 Batch (t): 1.587, 501.466/s, 83.5776/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9369 (3.1652) Dstill: 2.5735 (2.7618) Loss: 5.5104 (5.9270)
INFO:root:Train Epoch: 5 [1516998/1859739.0 (82%)] Data (t): 0.610 Batch (t): 1.561, 532.398/s, 88.7331/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0210 (3.1615) Dstill: 2.5876 (2.7573) Loss: 5.6087 (5.9188)
2025-02-05,05:48:17 | INFO | Train Epoch: 5 [1516998/1859739.0 (82%)] Data (t): 0.610 Batch (t): 1.561, 532.398/s, 88.7331/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0210 (3.1615) Dstill: 2.5876 (2.7573) Loss: 5.6087 (5.9188)
INFO:root:Train Epoch: 5 [1556898/1859739.0 (84%)] Data (t): 0.616 Batch (t): 1.582, 523.814/s, 87.3023/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9668 (3.1567) Dstill: 2.6525 (2.7547) Loss: 5.6193 (5.9113)
2025-02-05,05:49:36 | INFO | Train Epoch: 5 [1556898/1859739.0 (84%)] Data (t): 0.616 Batch (t): 1.582, 523.814/s, 87.3023/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9668 (3.1567) Dstill: 2.6525 (2.7547) Loss: 5.6193 (5.9113)
INFO:root:Train Epoch: 5 [1596798/1859739.0 (86%)] Data (t): 0.619 Batch (t): 1.576, 502.307/s, 83.7178/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1283 (3.1560) Dstill: 2.5046 (2.7486) Loss: 5.6329 (5.9046)
2025-02-05,05:50:55 | INFO | Train Epoch: 5 [1596798/1859739.0 (86%)] Data (t): 0.619 Batch (t): 1.576, 502.307/s, 83.7178/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1283 (3.1560) Dstill: 2.5046 (2.7486) Loss: 5.6329 (5.9046)
INFO:root:Train Epoch: 5 [1636698/1859739.0 (88%)] Data (t): 0.628 Batch (t): 1.558, 484.252/s, 80.7087/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.3815 (3.1613) Dstill: 2.5859 (2.7447) Loss: 5.9673 (5.9060)
2025-02-05,05:52:13 | INFO | Train Epoch: 5 [1636698/1859739.0 (88%)] Data (t): 0.628 Batch (t): 1.558, 484.252/s, 80.7087/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.3815 (3.1613) Dstill: 2.5859 (2.7447) Loss: 5.9673 (5.9060)
INFO:root:Train Epoch: 5 [1676598/1859739.0 (90%)] Data (t): 0.607 Batch (t): 1.595, 506.559/s, 84.4265/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.4265 (3.1675) Dstill: 2.5633 (2.7405) Loss: 5.9898 (5.9080)
2025-02-05,05:53:33 | INFO | Train Epoch: 5 [1676598/1859739.0 (90%)] Data (t): 0.607 Batch (t): 1.595, 506.559/s, 84.4265/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.4265 (3.1675) Dstill: 2.5633 (2.7405) Loss: 5.9898 (5.9080)
INFO:root:Train Epoch: 5 [1716498/1859739.0 (92%)] Data (t): 0.592 Batch (t): 1.627, 482.590/s, 80.4317/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2345 (3.1690) Dstill: 2.5676 (2.7366) Loss: 5.8021 (5.9056)
2025-02-05,05:54:54 | INFO | Train Epoch: 5 [1716498/1859739.0 (92%)] Data (t): 0.592 Batch (t): 1.627, 482.590/s, 80.4317/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2345 (3.1690) Dstill: 2.5676 (2.7366) Loss: 5.8021 (5.9056)
INFO:root:Train Epoch: 5 [1756398/1859739.0 (94%)] Data (t): 0.596 Batch (t): 1.613, 486.877/s, 81.1462/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.3108 (3.1722) Dstill: 2.6197 (2.7340) Loss: 5.9306 (5.9061)
2025-02-05,05:56:15 | INFO | Train Epoch: 5 [1756398/1859739.0 (94%)] Data (t): 0.596 Batch (t): 1.613, 486.877/s, 81.1462/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.3108 (3.1722) Dstill: 2.6197 (2.7340) Loss: 5.9306 (5.9061)
INFO:root:Train Epoch: 5 [1796298/1859739.0 (97%)] Data (t): 0.561 Batch (t): 1.598, 494.760/s, 82.4600/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.4333 (3.1779) Dstill: 2.5518 (2.7300) Loss: 5.9851 (5.9079)
2025-02-05,05:57:35 | INFO | Train Epoch: 5 [1796298/1859739.0 (97%)] Data (t): 0.561 Batch (t): 1.598, 494.760/s, 82.4600/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.4333 (3.1779) Dstill: 2.5518 (2.7300) Loss: 5.9851 (5.9079)
INFO:root:Train Epoch: 5 [1836198/1859739.0 (99%)] Data (t): 0.610 Batch (t): 1.631, 477.234/s, 79.5390/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.3135 (3.1807) Dstill: 2.9286 (2.7342) Loss: 6.2420 (5.9150)
2025-02-05,05:58:56 | INFO | Train Epoch: 5 [1836198/1859739.0 (99%)] Data (t): 0.610 Batch (t): 1.631, 477.234/s, 79.5390/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.3135 (3.1807) Dstill: 2.9286 (2.7342) Loss: 6.2420 (5.9150)
INFO:root:Train Epoch: 5 [1860138/1859739.0 (100%)] Data (t): 0.609 Batch (t): 1.615, 510.521/s, 85.0869/s/gpu LR: 0.000097 Logit Scale: 99.997 Contrastive_loss: 3.0403 (3.1778) Dstill: 2.7762 (2.7351) Loss: 5.8165 (5.9129)
2025-02-05,05:59:45 | INFO | Train Epoch: 5 [1860138/1859739.0 (100%)] Data (t): 0.609 Batch (t): 1.615, 510.521/s, 85.0869/s/gpu LR: 0.000097 Logit Scale: 99.997 Contrastive_loss: 3.0403 (3.1778) Dstill: 2.7762 (2.7351) Loss: 5.8165 (5.9129)
INFO:root:Train Epoch: 5 [  798/59975.0 (1%)] Data (t): 0.576 Batch (t): 1.503, 530.772/s, 88.4620/s/gpu LR: 0.000038 Logit Scale: 99.997 Contrastive_loss: 7.6182 (7.6182) Dstill: 9.1342 (9.1342) Loss: 16.752 (16.752)
2025-02-05,05:59:46 | INFO | Train Epoch: 5 [  798/59975.0 (1%)] Data (t): 0.576 Batch (t): 1.503, 530.772/s, 88.4620/s/gpu LR: 0.000038 Logit Scale: 99.997 Contrastive_loss: 7.6182 (7.6182) Dstill: 9.1342 (9.1342) Loss: 16.752 (16.752)
INFO:root:Train Epoch: 5 [40698/59975.0 (67%)] Data (t): 0.469 Batch (t): 1.384, 565.311/s, 94.2185/s/gpu LR: 0.000043 Logit Scale: 99.988 Contrastive_loss: 6.7893 (7.2037) Dstill: 6.0044 (7.5693) Loss: 12.794 (14.773)
2025-02-05,06:00:55 | INFO | Train Epoch: 5 [40698/59975.0 (67%)] Data (t): 0.469 Batch (t): 1.384, 565.311/s, 94.2185/s/gpu LR: 0.000043 Logit Scale: 99.988 Contrastive_loss: 6.7893 (7.2037) Dstill: 6.0044 (7.5693) Loss: 12.794 (14.773)
INFO:root:Train Epoch: 5 [60648/59975.0 (100%)] Data (t): 0.466 Batch (t): 1.367, 582.021/s, 97.0036/s/gpu LR: 0.000046 Logit Scale: 99.985 Contrastive_loss: 6.5416 (6.9830) Dstill: 5.7838 (6.9741) Loss: 12.325 (13.957)
2025-02-05,06:01:30 | INFO | Train Epoch: 5 [60648/59975.0 (100%)] Data (t): 0.466 Batch (t): 1.367, 582.021/s, 97.0036/s/gpu LR: 0.000046 Logit Scale: 99.985 Contrastive_loss: 6.5416 (6.9830) Dstill: 5.7838 (6.9741) Loss: 12.325 (13.957)
INFO:root:Train Epoch: 5 [  798/47810.0 (2%)] Data (t): 2.523 Batch (t): 3.429, 232.741/s, 38.7902/s/gpu LR: 0.000030 Logit Scale: 99.985 Contrastive_loss: 7.6551 (7.6551) Dstill: 11.164 (11.164) Loss: 18.820 (18.820)
2025-02-05,06:01:33 | INFO | Train Epoch: 5 [  798/47810.0 (2%)] Data (t): 2.523 Batch (t): 3.429, 232.741/s, 38.7902/s/gpu LR: 0.000030 Logit Scale: 99.985 Contrastive_loss: 7.6551 (7.6551) Dstill: 11.164 (11.164) Loss: 18.820 (18.820)
INFO:root:Train Epoch: 5 [40698/47810.0 (85%)] Data (t): 2.240 Batch (t): 3.170, 243.761/s, 40.6269/s/gpu LR: 0.000035 Logit Scale: 99.980 Contrastive_loss: 6.8010 (7.2281) Dstill: 6.0720 (8.6182) Loss: 12.873 (15.846)
2025-02-05,06:04:12 | INFO | Train Epoch: 5 [40698/47810.0 (85%)] Data (t): 2.240 Batch (t): 3.170, 243.761/s, 40.6269/s/gpu LR: 0.000035 Logit Scale: 99.980 Contrastive_loss: 6.8010 (7.2281) Dstill: 6.0720 (8.6182) Loss: 12.873 (15.846)
INFO:root:Train Epoch: 5 [47880/47810.0 (100%)] Data (t): 2.332 Batch (t): 3.248, 254.532/s, 42.4220/s/gpu LR: 0.000036 Logit Scale: 99.979 Contrastive_loss: 6.3880 (6.9480) Dstill: 5.9271 (7.7212) Loss: 12.315 (14.669)
2025-02-05,06:04:41 | INFO | Train Epoch: 5 [47880/47810.0 (100%)] Data (t): 2.332 Batch (t): 3.248, 254.532/s, 42.4220/s/gpu LR: 0.000036 Logit Scale: 99.979 Contrastive_loss: 6.3880 (6.9480) Dstill: 5.9271 (7.7212) Loss: 12.315 (14.669)
INFO:root:Train Epoch: 5 [ 798/7500.0 (10%)] Data (t): 35.675 Batch (t): 44.560, 17.9085/s, 2.98475/s/gpu LR: 0.000005 Logit Scale: 99.980 Contrastive_loss: 45.421 (45.421) Dstill: 10.438 (10.438) Loss: 55.859 (55.859)
2025-02-05,06:05:25 | INFO | Train Epoch: 5 [ 798/7500.0 (10%)] Data (t): 35.675 Batch (t): 44.560, 17.9085/s, 2.98475/s/gpu LR: 0.000005 Logit Scale: 99.980 Contrastive_loss: 45.421 (45.421) Dstill: 10.438 (10.438) Loss: 55.859 (55.859)
INFO:root:Train Epoch: 5 [7980/7500.0 (100%)] Data (t): 33.195 Batch (t): 42.537, 19.0961/s, 3.18269/s/gpu LR: 0.000006 Logit Scale: 99.977 Contrastive_loss: 8.5279 (26.975) Dstill: 10.887 (10.662) Loss: 19.414 (37.637)
2025-02-05,06:11:48 | INFO | Train Epoch: 5 [7980/7500.0 (100%)] Data (t): 33.195 Batch (t): 42.537, 19.0961/s, 3.18269/s/gpu LR: 0.000006 Logit Scale: 99.977 Contrastive_loss: 8.5279 (26.975) Dstill: 10.887 (10.662) Loss: 19.414 (37.637)
INFO:root:Start epoch 6
2025-02-05,06:11:49 | INFO | Start epoch 6
INFO:root:Train Epoch: 6 [    798/1859739.0 (0%)] Data (t): 0.552 Batch (t): 1.460, 546.590/s, 91.0984/s/gpu LR: 0.000097 Logit Scale: 99.976 Contrastive_loss: 4.6906 (4.6906) Dstill: 5.1566 (5.1566) Loss: 9.8471 (9.8471)
2025-02-05,06:11:51 | INFO | Train Epoch: 6 [    798/1859739.0 (0%)] Data (t): 0.552 Batch (t): 1.460, 546.590/s, 91.0984/s/gpu LR: 0.000097 Logit Scale: 99.976 Contrastive_loss: 4.6906 (4.6906) Dstill: 5.1566 (5.1566) Loss: 9.8471 (9.8471)
INFO:root:Train Epoch: 6 [  40698/1859739.0 (2%)] Data (t): 0.459 Batch (t): 1.422, 535.617/s, 89.2696/s/gpu LR: 0.000097 Logit Scale: 99.922 Contrastive_loss: 3.0249 (3.8577) Dstill: 4.1416 (4.6491) Loss: 7.1665 (8.5068)
2025-02-05,06:13:02 | INFO | Train Epoch: 6 [  40698/1859739.0 (2%)] Data (t): 0.459 Batch (t): 1.422, 535.617/s, 89.2696/s/gpu LR: 0.000097 Logit Scale: 99.922 Contrastive_loss: 3.0249 (3.8577) Dstill: 4.1416 (4.6491) Loss: 7.1665 (8.5068)
INFO:root:Train Epoch: 6 [  80598/1859739.0 (4%)] Data (t): 0.450 Batch (t): 1.410, 605.392/s, 100.899/s/gpu LR: 0.000097 Logit Scale: 99.923 Contrastive_loss: 3.1674 (3.6276) Dstill: 3.4771 (4.2584) Loss: 6.6444 (7.8860)
2025-02-05,06:14:12 | INFO | Train Epoch: 6 [  80598/1859739.0 (4%)] Data (t): 0.450 Batch (t): 1.410, 605.392/s, 100.899/s/gpu LR: 0.000097 Logit Scale: 99.923 Contrastive_loss: 3.1674 (3.6276) Dstill: 3.4771 (4.2584) Loss: 6.6444 (7.8860)
INFO:root:Train Epoch: 6 [ 120498/1859739.0 (6%)] Data (t): 0.461 Batch (t): 1.400, 627.786/s, 104.631/s/gpu LR: 0.000097 Logit Scale: 99.925 Contrastive_loss: 2.8552 (3.4345) Dstill: 2.8675 (3.9107) Loss: 5.7228 (7.3452)
2025-02-05,06:15:22 | INFO | Train Epoch: 6 [ 120498/1859739.0 (6%)] Data (t): 0.461 Batch (t): 1.400, 627.786/s, 104.631/s/gpu LR: 0.000097 Logit Scale: 99.925 Contrastive_loss: 2.8552 (3.4345) Dstill: 2.8675 (3.9107) Loss: 5.7228 (7.3452)
INFO:root:Train Epoch: 6 [ 160398/1859739.0 (9%)] Data (t): 0.503 Batch (t): 1.457, 502.399/s, 83.7332/s/gpu LR: 0.000097 Logit Scale: 99.930 Contrastive_loss: 2.8113 (3.3099) Dstill: 2.8214 (3.6928) Loss: 5.6327 (7.0027)
2025-02-05,06:16:35 | INFO | Train Epoch: 6 [ 160398/1859739.0 (9%)] Data (t): 0.503 Batch (t): 1.457, 502.399/s, 83.7332/s/gpu LR: 0.000097 Logit Scale: 99.930 Contrastive_loss: 2.8113 (3.3099) Dstill: 2.8214 (3.6928) Loss: 5.6327 (7.0027)
INFO:root:Train Epoch: 6 [ 200298/1859739.0 (11%)] Data (t): 0.565 Batch (t): 1.510, 545.534/s, 90.9223/s/gpu LR: 0.000097 Logit Scale: 99.933 Contrastive_loss: 3.3924 (3.3236) Dstill: 2.5714 (3.5059) Loss: 5.9637 (6.8295)
2025-02-05,06:17:51 | INFO | Train Epoch: 6 [ 200298/1859739.0 (11%)] Data (t): 0.565 Batch (t): 1.510, 545.534/s, 90.9223/s/gpu LR: 0.000097 Logit Scale: 99.933 Contrastive_loss: 3.3924 (3.3236) Dstill: 2.5714 (3.5059) Loss: 5.9637 (6.8295)
INFO:root:Train Epoch: 6 [ 240198/1859739.0 (13%)] Data (t): 0.565 Batch (t): 1.511, 559.389/s, 93.2315/s/gpu LR: 0.000097 Logit Scale: 99.941 Contrastive_loss: 3.2374 (3.3113) Dstill: 2.6459 (3.3831) Loss: 5.8833 (6.6944)
2025-02-05,06:19:06 | INFO | Train Epoch: 6 [ 240198/1859739.0 (13%)] Data (t): 0.565 Batch (t): 1.511, 559.389/s, 93.2315/s/gpu LR: 0.000097 Logit Scale: 99.941 Contrastive_loss: 3.2374 (3.3113) Dstill: 2.6459 (3.3831) Loss: 5.8833 (6.6944)
INFO:root:Train Epoch: 6 [ 280098/1859739.0 (15%)] Data (t): 0.582 Batch (t): 1.532, 501.630/s, 83.6051/s/gpu LR: 0.000097 Logit Scale: 99.954 Contrastive_loss: 2.8986 (3.2597) Dstill: 2.6161 (3.2872) Loss: 5.5147 (6.5469)
2025-02-05,06:20:23 | INFO | Train Epoch: 6 [ 280098/1859739.0 (15%)] Data (t): 0.582 Batch (t): 1.532, 501.630/s, 83.6051/s/gpu LR: 0.000097 Logit Scale: 99.954 Contrastive_loss: 2.8986 (3.2597) Dstill: 2.6161 (3.2872) Loss: 5.5147 (6.5469)
INFO:root:Train Epoch: 6 [ 319998/1859739.0 (17%)] Data (t): 0.621 Batch (t): 1.543, 491.315/s, 81.8859/s/gpu LR: 0.000097 Logit Scale: 99.963 Contrastive_loss: 2.9173 (3.2217) Dstill: 2.8576 (3.2395) Loss: 5.7749 (6.4611)
2025-02-05,06:21:40 | INFO | Train Epoch: 6 [ 319998/1859739.0 (17%)] Data (t): 0.621 Batch (t): 1.543, 491.315/s, 81.8859/s/gpu LR: 0.000097 Logit Scale: 99.963 Contrastive_loss: 2.9173 (3.2217) Dstill: 2.8576 (3.2395) Loss: 5.7749 (6.4611)
INFO:root:Train Epoch: 6 [ 359898/1859739.0 (19%)] Data (t): 0.550 Batch (t): 1.511, 537.187/s, 89.5311/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.1607 (3.2156) Dstill: 2.6585 (3.1814) Loss: 5.8192 (6.3969)
2025-02-05,06:22:55 | INFO | Train Epoch: 6 [ 359898/1859739.0 (19%)] Data (t): 0.550 Batch (t): 1.511, 537.187/s, 89.5311/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.1607 (3.2156) Dstill: 2.6585 (3.1814) Loss: 5.8192 (6.3969)
INFO:root:Train Epoch: 6 [ 399798/1859739.0 (21%)] Data (t): 0.593 Batch (t): 1.536, 485.975/s, 80.9959/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.0875 (3.2039) Dstill: 2.6784 (3.1356) Loss: 5.7658 (6.3396)
2025-02-05,06:24:12 | INFO | Train Epoch: 6 [ 399798/1859739.0 (21%)] Data (t): 0.593 Batch (t): 1.536, 485.975/s, 80.9959/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.0875 (3.2039) Dstill: 2.6784 (3.1356) Loss: 5.7658 (6.3396)
INFO:root:Train Epoch: 6 [ 439698/1859739.0 (24%)] Data (t): 0.599 Batch (t): 1.549, 480.488/s, 80.0814/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.0518 (3.1912) Dstill: 2.7712 (3.1053) Loss: 5.8230 (6.2965)
2025-02-05,06:25:30 | INFO | Train Epoch: 6 [ 439698/1859739.0 (24%)] Data (t): 0.599 Batch (t): 1.549, 480.488/s, 80.0814/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 3.0518 (3.1912) Dstill: 2.7712 (3.1053) Loss: 5.8230 (6.2965)
INFO:root:Train Epoch: 6 [ 479598/1859739.0 (26%)] Data (t): 0.592 Batch (t): 1.541, 544.090/s, 90.6816/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9921 (3.1759) Dstill: 2.7139 (3.0752) Loss: 5.7060 (6.2511)
2025-02-05,06:26:47 | INFO | Train Epoch: 6 [ 479598/1859739.0 (26%)] Data (t): 0.592 Batch (t): 1.541, 544.090/s, 90.6816/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9921 (3.1759) Dstill: 2.7139 (3.0752) Loss: 5.7060 (6.2511)
INFO:root:Train Epoch: 6 [ 519498/1859739.0 (28%)] Data (t): 0.578 Batch (t): 1.510, 488.067/s, 81.3445/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0996 (3.1705) Dstill: 2.6174 (3.0425) Loss: 5.7171 (6.2129)
2025-02-05,06:28:02 | INFO | Train Epoch: 6 [ 519498/1859739.0 (28%)] Data (t): 0.578 Batch (t): 1.510, 488.067/s, 81.3445/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0996 (3.1705) Dstill: 2.6174 (3.0425) Loss: 5.7171 (6.2129)
INFO:root:Train Epoch: 6 [ 559398/1859739.0 (30%)] Data (t): 0.556 Batch (t): 1.525, 539.962/s, 89.9937/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.2210 (3.1738) Dstill: 2.6280 (3.0148) Loss: 5.8490 (6.1887)
2025-02-05,06:29:18 | INFO | Train Epoch: 6 [ 559398/1859739.0 (30%)] Data (t): 0.556 Batch (t): 1.525, 539.962/s, 89.9937/s/gpu LR: 0.000097 Logit Scale: 99.999 Contrastive_loss: 3.2210 (3.1738) Dstill: 2.6280 (3.0148) Loss: 5.8490 (6.1887)
INFO:root:Train Epoch: 6 [ 599298/1859739.0 (32%)] Data (t): 0.617 Batch (t): 1.583, 487.395/s, 81.2325/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1294 (3.1711) Dstill: 2.7272 (2.9969) Loss: 5.8566 (6.1679)
2025-02-05,06:30:38 | INFO | Train Epoch: 6 [ 599298/1859739.0 (32%)] Data (t): 0.617 Batch (t): 1.583, 487.395/s, 81.2325/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1294 (3.1711) Dstill: 2.7272 (2.9969) Loss: 5.8566 (6.1679)
INFO:root:Train Epoch: 6 [ 639198/1859739.0 (34%)] Data (t): 0.590 Batch (t): 1.535, 503.168/s, 83.8614/s/gpu LR: 0.000097 Logit Scale: 99.995 Contrastive_loss: 3.1394 (3.1692) Dstill: 2.6448 (2.9761) Loss: 5.7841 (6.1453)
2025-02-05,06:31:54 | INFO | Train Epoch: 6 [ 639198/1859739.0 (34%)] Data (t): 0.590 Batch (t): 1.535, 503.168/s, 83.8614/s/gpu LR: 0.000097 Logit Scale: 99.995 Contrastive_loss: 3.1394 (3.1692) Dstill: 2.6448 (2.9761) Loss: 5.7841 (6.1453)
INFO:root:Train Epoch: 6 [ 679098/1859739.0 (37%)] Data (t): 0.577 Batch (t): 1.531, 481.302/s, 80.2169/s/gpu LR: 0.000097 Logit Scale: 99.997 Contrastive_loss: 2.9615 (3.1577) Dstill: 2.5869 (2.9545) Loss: 5.5484 (6.1122)
2025-02-05,06:33:11 | INFO | Train Epoch: 6 [ 679098/1859739.0 (37%)] Data (t): 0.577 Batch (t): 1.531, 481.302/s, 80.2169/s/gpu LR: 0.000097 Logit Scale: 99.997 Contrastive_loss: 2.9615 (3.1577) Dstill: 2.5869 (2.9545) Loss: 5.5484 (6.1122)
INFO:root:Train Epoch: 6 [ 718998/1859739.0 (39%)] Data (t): 0.598 Batch (t): 1.572, 471.102/s, 78.5170/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0800 (3.1536) Dstill: 2.7915 (2.9459) Loss: 5.8715 (6.0995)
2025-02-05,06:34:30 | INFO | Train Epoch: 6 [ 718998/1859739.0 (39%)] Data (t): 0.598 Batch (t): 1.572, 471.102/s, 78.5170/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.0800 (3.1536) Dstill: 2.7915 (2.9459) Loss: 5.8715 (6.0995)
INFO:root:Train Epoch: 6 [ 758898/1859739.0 (41%)] Data (t): 0.586 Batch (t): 1.575, 521.235/s, 86.8726/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.2170 (3.1568) Dstill: 2.6745 (2.9324) Loss: 5.8915 (6.0891)
2025-02-05,06:35:48 | INFO | Train Epoch: 6 [ 758898/1859739.0 (41%)] Data (t): 0.586 Batch (t): 1.575, 521.235/s, 86.8726/s/gpu LR: 0.000097 Logit Scale: 99.998 Contrastive_loss: 3.2170 (3.1568) Dstill: 2.6745 (2.9324) Loss: 5.8915 (6.0891)
INFO:root:Train Epoch: 6 [ 798798/1859739.0 (43%)] Data (t): 0.591 Batch (t): 1.554, 528.383/s, 88.0638/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9063 (3.1448) Dstill: 2.5271 (2.9131) Loss: 5.4334 (6.0579)
2025-02-05,06:37:06 | INFO | Train Epoch: 6 [ 798798/1859739.0 (43%)] Data (t): 0.591 Batch (t): 1.554, 528.383/s, 88.0638/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9063 (3.1448) Dstill: 2.5271 (2.9131) Loss: 5.4334 (6.0579)
INFO:root:Train Epoch: 6 [ 838698/1859739.0 (45%)] Data (t): 0.572 Batch (t): 1.547, 514.153/s, 85.6922/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2754 (3.1508) Dstill: 2.7720 (2.9067) Loss: 6.0475 (6.0574)
2025-02-05,06:38:23 | INFO | Train Epoch: 6 [ 838698/1859739.0 (45%)] Data (t): 0.572 Batch (t): 1.547, 514.153/s, 85.6922/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2754 (3.1508) Dstill: 2.7720 (2.9067) Loss: 6.0475 (6.0574)
INFO:root:Train Epoch: 6 [ 878598/1859739.0 (47%)] Data (t): 0.602 Batch (t): 1.589, 508.507/s, 84.7512/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.7646 (3.1340) Dstill: 2.6063 (2.8936) Loss: 5.3709 (6.0276)
2025-02-05,06:39:43 | INFO | Train Epoch: 6 [ 878598/1859739.0 (47%)] Data (t): 0.602 Batch (t): 1.589, 508.507/s, 84.7512/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.7646 (3.1340) Dstill: 2.6063 (2.8936) Loss: 5.3709 (6.0276)
INFO:root:Train Epoch: 6 [ 918498/1859739.0 (49%)] Data (t): 0.585 Batch (t): 1.548, 549.893/s, 91.6489/s/gpu LR: 0.000097 Logit Scale: 99.995 Contrastive_loss: 3.1106 (3.1330) Dstill: 2.5084 (2.8775) Loss: 5.6190 (6.0105)
2025-02-05,06:41:00 | INFO | Train Epoch: 6 [ 918498/1859739.0 (49%)] Data (t): 0.585 Batch (t): 1.548, 549.893/s, 91.6489/s/gpu LR: 0.000097 Logit Scale: 99.995 Contrastive_loss: 3.1106 (3.1330) Dstill: 2.5084 (2.8775) Loss: 5.6190 (6.0105)
INFO:root:Train Epoch: 6 [ 958398/1859739.0 (52%)] Data (t): 0.574 Batch (t): 1.544, 486.520/s, 81.0866/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1907 (3.1353) Dstill: 2.6585 (2.8688) Loss: 5.8492 (6.0041)
2025-02-05,06:42:17 | INFO | Train Epoch: 6 [ 958398/1859739.0 (52%)] Data (t): 0.574 Batch (t): 1.544, 486.520/s, 81.0866/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1907 (3.1353) Dstill: 2.6585 (2.8688) Loss: 5.8492 (6.0041)
INFO:root:Train Epoch: 6 [ 998298/1859739.0 (54%)] Data (t): 0.605 Batch (t): 1.559, 484.732/s, 80.7886/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2359 (3.1392) Dstill: 2.6690 (2.8611) Loss: 5.9049 (6.0003)
2025-02-05,06:43:35 | INFO | Train Epoch: 6 [ 998298/1859739.0 (54%)] Data (t): 0.605 Batch (t): 1.559, 484.732/s, 80.7886/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2359 (3.1392) Dstill: 2.6690 (2.8611) Loss: 5.9049 (6.0003)
INFO:root:Train Epoch: 6 [1038198/1859739.0 (56%)] Data (t): 0.587 Batch (t): 1.548, 514.030/s, 85.6716/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.8254 (3.1276) Dstill: 2.9625 (2.8649) Loss: 5.7878 (5.9924)
2025-02-05,06:44:53 | INFO | Train Epoch: 6 [1038198/1859739.0 (56%)] Data (t): 0.587 Batch (t): 1.548, 514.030/s, 85.6716/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.8254 (3.1276) Dstill: 2.9625 (2.8649) Loss: 5.7878 (5.9924)
INFO:root:Train Epoch: 6 [1078098/1859739.0 (58%)] Data (t): 0.608 Batch (t): 1.543, 563.738/s, 93.9563/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.8573 (3.1179) Dstill: 2.7207 (2.8597) Loss: 5.5780 (5.9776)
2025-02-05,06:46:10 | INFO | Train Epoch: 6 [1078098/1859739.0 (58%)] Data (t): 0.608 Batch (t): 1.543, 563.738/s, 93.9563/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.8573 (3.1179) Dstill: 2.7207 (2.8597) Loss: 5.5780 (5.9776)
INFO:root:Train Epoch: 6 [1117998/1859739.0 (60%)] Data (t): 0.603 Batch (t): 1.550, 481.283/s, 80.2139/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1505 (3.1190) Dstill: 3.1426 (2.8695) Loss: 6.2931 (5.9885)
2025-02-05,06:47:27 | INFO | Train Epoch: 6 [1117998/1859739.0 (60%)] Data (t): 0.603 Batch (t): 1.550, 481.283/s, 80.2139/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1505 (3.1190) Dstill: 3.1426 (2.8695) Loss: 6.2931 (5.9885)
INFO:root:Train Epoch: 6 [1157898/1859739.0 (62%)] Data (t): 0.610 Batch (t): 1.587, 529.112/s, 88.1853/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9875 (3.1146) Dstill: 2.5713 (2.8595) Loss: 5.5587 (5.9742)
2025-02-05,06:48:47 | INFO | Train Epoch: 6 [1157898/1859739.0 (62%)] Data (t): 0.610 Batch (t): 1.587, 529.112/s, 88.1853/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9875 (3.1146) Dstill: 2.5713 (2.8595) Loss: 5.5587 (5.9742)
INFO:root:Train Epoch: 6 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.543, 549.009/s, 91.5016/s/gpu LR: 0.000097 Logit Scale: 99.988 Contrastive_loss: 3.3923 (3.1236) Dstill: 2.6286 (2.8521) Loss: 6.0209 (5.9757)
2025-02-05,06:50:04 | INFO | Train Epoch: 6 [1197798/1859739.0 (64%)] Data (t): 0.613 Batch (t): 1.543, 549.009/s, 91.5016/s/gpu LR: 0.000097 Logit Scale: 99.988 Contrastive_loss: 3.3923 (3.1236) Dstill: 2.6286 (2.8521) Loss: 6.0209 (5.9757)
INFO:root:Train Epoch: 6 [1237698/1859739.0 (67%)] Data (t): 0.607 Batch (t): 1.541, 526.164/s, 87.6940/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1442 (3.1242) Dstill: 2.7520 (2.8489) Loss: 5.8962 (5.9732)
2025-02-05,06:51:21 | INFO | Train Epoch: 6 [1237698/1859739.0 (67%)] Data (t): 0.607 Batch (t): 1.541, 526.164/s, 87.6940/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.1442 (3.1242) Dstill: 2.7520 (2.8489) Loss: 5.8962 (5.9732)
INFO:root:Train Epoch: 6 [1277598/1859739.0 (69%)] Data (t): 0.605 Batch (t): 1.547, 517.650/s, 86.2751/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2652 (3.1285) Dstill: 2.6545 (2.8431) Loss: 5.9197 (5.9716)
2025-02-05,06:52:38 | INFO | Train Epoch: 6 [1277598/1859739.0 (69%)] Data (t): 0.605 Batch (t): 1.547, 517.650/s, 86.2751/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 3.2652 (3.1285) Dstill: 2.6545 (2.8431) Loss: 5.9197 (5.9716)
INFO:root:Train Epoch: 6 [1317498/1859739.0 (71%)] Data (t): 0.618 Batch (t): 1.584, 491.232/s, 81.8719/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 2.8702 (3.1209) Dstill: 2.6610 (2.8377) Loss: 5.5312 (5.9586)
2025-02-05,06:53:57 | INFO | Train Epoch: 6 [1317498/1859739.0 (71%)] Data (t): 0.618 Batch (t): 1.584, 491.232/s, 81.8719/s/gpu LR: 0.000097 Logit Scale: 99.996 Contrastive_loss: 2.8702 (3.1209) Dstill: 2.6610 (2.8377) Loss: 5.5312 (5.9586)
INFO:root:Train Epoch: 6 [1357398/1859739.0 (73%)] Data (t): 0.634 Batch (t): 1.549, 517.294/s, 86.2157/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9850 (3.1170) Dstill: 2.7191 (2.8343) Loss: 5.7040 (5.9513)
2025-02-05,06:55:15 | INFO | Train Epoch: 6 [1357398/1859739.0 (73%)] Data (t): 0.634 Batch (t): 1.549, 517.294/s, 86.2157/s/gpu LR: 0.000097 Logit Scale: 100.000 Contrastive_loss: 2.9850 (3.1170) Dstill: 2.7191 (2.8343) Loss: 5.7040 (5.9513)
INFO:root:Train Epoch: 6 [1397298/1859739.0 (75%)] Data (t): 0.599 Batch (t): 1.545, 522.425/s, 87.0709/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 3.0459 (3.1151) Dstill: 2.5932 (2.8276) Loss: 5.6392 (5.9427)
2025-02-05,06:56:32 | INFO | Train Epoch: 6 [1397298/1859739.0 (75%)] Data (t): 0.599 Batch (t): 1.545, 522.425/s, 87.0709/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 3.0459 (3.1151) Dstill: 2.5932 (2.8276) Loss: 5.6392 (5.9427)
INFO:root:Train Epoch: 6 [1437198/1859739.0 (77%)] Data (t): 0.636 Batch (t): 1.580, 476.903/s, 79.4839/s/gpu LR: 0.000096 Logit Scale: 99.997 Contrastive_loss: 3.0754 (3.1140) Dstill: 2.6936 (2.8240) Loss: 5.7690 (5.9380)
2025-02-05,06:57:51 | INFO | Train Epoch: 6 [1437198/1859739.0 (77%)] Data (t): 0.636 Batch (t): 1.580, 476.903/s, 79.4839/s/gpu LR: 0.000096 Logit Scale: 99.997 Contrastive_loss: 3.0754 (3.1140) Dstill: 2.6936 (2.8240) Loss: 5.7690 (5.9380)
INFO:root:Train Epoch: 6 [1477098/1859739.0 (79%)] Data (t): 0.644 Batch (t): 1.601, 506.341/s, 84.3901/s/gpu LR: 0.000096 Logit Scale: 99.999 Contrastive_loss: 2.8516 (3.1071) Dstill: 2.6116 (2.8184) Loss: 5.4633 (5.9255)
2025-02-05,06:59:11 | INFO | Train Epoch: 6 [1477098/1859739.0 (79%)] Data (t): 0.644 Batch (t): 1.601, 506.341/s, 84.3901/s/gpu LR: 0.000096 Logit Scale: 99.999 Contrastive_loss: 2.8516 (3.1071) Dstill: 2.6116 (2.8184) Loss: 5.4633 (5.9255)
INFO:root:Train Epoch: 6 [1516998/1859739.0 (82%)] Data (t): 0.618 Batch (t): 1.580, 557.142/s, 92.8570/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 2.8878 (3.1015) Dstill: 2.5671 (2.8120) Loss: 5.4549 (5.9134)
2025-02-05,07:00:30 | INFO | Train Epoch: 6 [1516998/1859739.0 (82%)] Data (t): 0.618 Batch (t): 1.580, 557.142/s, 92.8570/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 2.8878 (3.1015) Dstill: 2.5671 (2.8120) Loss: 5.4549 (5.9134)
INFO:root:Train Epoch: 6 [1556898/1859739.0 (84%)] Data (t): 0.617 Batch (t): 1.582, 528.666/s, 88.1110/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 2.8750 (3.0958) Dstill: 2.6451 (2.8078) Loss: 5.5201 (5.9036)
2025-02-05,07:01:49 | INFO | Train Epoch: 6 [1556898/1859739.0 (84%)] Data (t): 0.617 Batch (t): 1.582, 528.666/s, 88.1110/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 2.8750 (3.0958) Dstill: 2.6451 (2.8078) Loss: 5.5201 (5.9036)
INFO:root:Train Epoch: 6 [1596798/1859739.0 (86%)] Data (t): 0.623 Batch (t): 1.576, 514.077/s, 85.6795/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 3.1160 (3.0963) Dstill: 2.5973 (2.8027) Loss: 5.7133 (5.8989)
2025-02-05,07:03:08 | INFO | Train Epoch: 6 [1596798/1859739.0 (86%)] Data (t): 0.623 Batch (t): 1.576, 514.077/s, 85.6795/s/gpu LR: 0.000096 Logit Scale: 100.000 Contrastive_loss: 3.1160 (3.0963) Dstill: 2.5973 (2.8027) Loss: 5.7133 (5.8989)
slurmstepd: error: *** JOB 10621 ON fwgegpu02 CANCELLED AT 2025-02-05T07:04:06 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 10621 STEPD TERMINATED ON fwgegpu02 AT 2025-02-05T07:06:36 DUE TO JOB NOT ENDING WITH SIGNALS ***
